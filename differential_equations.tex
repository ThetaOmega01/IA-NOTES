\documentclass[10pt]{article}
\pdfoutput=1 
\usepackage{NotesTeX,lipsum}

%\usepackage{showframe}

\title{\begin{center}{\Huge \textit{Differential Equations Notes}}\\{{\itshape Based on Lectures and "An Introduction to ODEs"}}\end{center}}
\author{$\theta\omega\theta$}
\affiliation{
Not in University of Cambridge\\
skipped some talks irrelevant to contents\\
}

\emailAdd{not telling you}

\begin{document}
	\maketitle
	\flushbottom
	\newpage
    \pagestyle{fancynotes}
    %main doc
    \part{Basic Calculus}
    \section{Differentiation}
    \subsection{Definitions and methods}
	\begin{definition}[Derivative]
        The derivative of a function $f(x)$ wrt its argument $x$ is the function
        \[
            \frac{\mathrm{d}f}{\mathrm{d}x} = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} 
        .\]
        We define higher derivatives recursively by 
        \[
            \frac{\mathrm{d}^nf}{\mathrm{d}x^n} = \frac{\mathrm{d}}{\mathrm{d}x}\left( \frac{\mathrm{d}^{n-1}f}{\mathrm{d}x^{n-1}}  \right)   
        .\]
    \end{definition}
    For the derivative to exist, we need
    \[
        \lim_{h \to 0-} \frac{f(x+h)-f(x)}{h} = \lim_{h \to 0+} \frac{f(x+h)-f(x)}{h} 
    .\]
    
    Rules for differentiation:
    \begin{enumerate}
        \item \textbf{Chain rule}: $ (f(g(x)))' = f'(g(x))g'(x) $.
        \item \textbf{Product rule}: $ (u\cdot v)' = u\cdot v'+u'\cdot v $.
        \item \textbf{Leibniz's rule}: generalisation of product rule.\sidenote{There are multiple ways to prove, e.g. by induction.}
        \[
            \frac{\mathrm{d}^n}{\mathrm{d}x^n}(u\cdot v) = \sum_{k=0}^{n}\binom{n}{k}u^{(k)}v^{(n-k)}
        .\]
    \end{enumerate}
    \subsection{Order of magnitude}
    The goal is to compare the sizes of functions, in the vicinity of specific points.
    \begin{definition}[Little and Big o]
        We say $ f(x) = o(g(x)) $ as $x\to x_0$ if $ \lim_{x \to x_0} \frac{f(x)}{g(x)} = 0 $.

        We say $ f(x) = O(g(x)) $ as $x\to x_0$ if $ \exists M, \delta>0, \left| x-x_0 \right| <\delta \Rightarrow \left| f(x) \right| \le M \left| g(x) \right| . $ The infinite case is defined similarly.
    \end{definition}

    To find the tangent line to $f$ at $x_0$, note that 
    \[
        \begin{aligned}
             & \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x=x_0} = \frac{f(x_0+h)-f(x_0)}{h}+ \frac{o(h)}{h} & \text{when $ h \to 0$} \\
             \Longrightarrow & f(x_0+h) = f(x_0)+\frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x=x_0} h + o(h)& \text{when $ h \to 0$} \\
        \end{aligned}
    \]
    \subsection{Taylor's Theorem and L'Hopital's Theorem}
    We want to approximate a function $f(x)$ with a polynomial of order $n$:
    \[
        f(x) = \underbrace{a_0+a_1x+\cdots+a_nx^n}_{P_n(x)}
    .\]
    Differentiating recursively we get 
    \begin{equation}\label{eq:taylor_series}
        P_n(x) = f(x_0)+(x-x_0)f'(x_0)+\cdots+\frac{(x-x_0)^n}{n!}f^{(n)}(x_0).
    \end{equation}
    Alternatively, we can write $ f(x) = P_n(x)+E_n $, where $E_n$ is called the \textit{remainder/error}.

    By generalisation of $ f(x+h) = f(x) + hf'(x)+o(h), h\to 0 $, we get 
    \begin{equation}\label{eq:taylor_series_with_remainder}
        f(x+h) = f(x)+hf'(x)+\frac{h^2}{2}f'(x)+\cdots+\frac{h^n}{n!}f^{(n)}(x)+o(h^n).
    \end{equation}

    By refining the range of $o(h^n)$ we get
    \begin{theorem}[Taylor]\label{thm:taylor_theorem}
        If the first $n+1$ derivatives of $f(x)$ exist, then 
        \[
            f(x+h) = f(x)+hf'(x)+\frac{h^2}{2}f'(x)+\cdots+\frac{h^n}{n!}f^{(n)}(x)+O(h^{n+1}).
        .\]
    \end{theorem}

    Using this we can prove 
    \begin{theorem}[L'Hopital]\label{thm:L'Hopital}
        Let $f$ and $g$ be differentiable at $x=x_0$ and
        \[
            \lim_{x \to x_0} f(x)=f(x_0)=0, \quad \lim_{x \to x_0} g(x)=g(x_0)=0
        .\]
    \end{theorem}
    \begin{proof}(Not rigorous)
        As $x\to x_0$, 
        \[
            \begin{aligned}
                 \frac{f(x)}{g(x)} &= \frac{f(x_0)+(x-x_0)f'(x_0)+o(x-x_0)}{g(x_0)+(x-x_0)g'(x_0)+o(x-x_0)}\\
                 &= \frac{(x-x_0)f'(x_0)+o(x-x_0)}{(x-x_0)g'(x_0)+o(x-x_0)}\\
                 &\to \frac{f'(x_0)}{g'(x_0)}.
            \end{aligned}
        \]
    \end{proof}
    Note that it can be applied recursively.
    \section{Integration}
    \subsection{Definition}
    All functions mentions are assumed to be well-hehaved.

    We evaluate the area under the curve of $f(x)$ by considering
    \[
        \sum_{n=0}^{N-1}f(x_n)\Delta x
    \]
    where $ \Delta x = \frac{b-a}{N} $ and $ x_n = a+n\Delta x. $
    \begin{theorem}[MVT]\label{thm:mean_value_theorem_for_integral}
        For a continuous function $f(x)$:
        \[
            \int_{x_n}^{x_{n+1}} f(x) \,\mathrm{d}x = f(x_c)(x_{n+1}-x_n) \quad \text{for some } x_c\in (x_n,x_{n+1})
        .\]
    \end{theorem}
    Estimate $ f(x_c) $ as follows:
    \[
        f(x_c) = f(x_n)+O(x_c-x_n) = f(x_n)+O(x_{n+1}-x_n)
    .\]\
    Hence
    \[
        \begin{aligned}
            \int_{x_n}^{x_{n+1}} f(x) \,\mathrm{d}x &= f(x_c)(x_{n+1}-x_n)\\
            &= [f(x_n)+O(x_{n+1}-x_n)](x_{n+1}-x_n)\\
            &= \Delta x f(x_n)+O(\Delta x^2).
        \end{aligned}
    \]
    Therefore the error $ \epsilon = O(\Delta x^2) $. It follows that
    \[
        \int_{a}^{b} f(x) \,\mathrm{d}x = \lim_{\Delta x \to 0} \left\{ \left[ \sum_{n=0}^{N-1}f(x_n)\Delta x \right] + O(N\Delta x^2)\right\}
    .\]
    Hence 
    \begin{definition}[Definite integral]
        $\displaystyle \int_{a}^{b} f(x) \,\mathrm{d}x = \lim_{N \to \infty} \sum_{n=0}^{N-1}f(x_n)\Delta x$ 
    \end{definition}
    \subsection{Fundamental Theorem of Calculus}
    \begin{theorem}[FTC]\label{thm:ftc}
        Let
        \[
            F(x) = \int_{a}^{x} f(t) \,\mathrm{d}t
        ,\]
        then
        \[
            \frac{\mathrm{d}F}{\mathrm{d}x} = f(x) 
        .\]
    \end{theorem}
    \begin{proof}
        From the definition of derivative:
        \[
            \begin{aligned}
                 \frac{\mathrm{d}F}{\mathrm{d}x} &= \lim_{h \to 0} \frac{1}{h} \left\{ \int_{a}^{x+h} f(t) \,\mathrm{d}t - \int_{a}^{x} f(t) \,\mathrm{d}t\right\}\\
                 &= \lim_{h \to 0} \frac{1}{h} \int_{x}^{x+h} f(t) \,\mathrm{d}t\\
                 &= \lim_{h \to 0} \frac{1}{h}\left( f(x)h+O(h^2) \right)\\
                 &= f(x).
            \end{aligned}
        \]
    \end{proof}
    \begin{corollary}\label{col:ftc}
            \[
                \frac{\mathrm{d}}{\mathrm{d}x}\int_{x}^{b} -f(t) \,\mathrm{d}t 
            .\]
            \[
                \frac{\mathrm{d}}{\mathrm{d}x} \int_{a}^{g(x)} f(t) \,\mathrm{d}t = \frac{\mathrm{d}}{\mathrm{d}x}F(g(x)) = \frac{\mathrm{d}F}{\mathrm{d}g} \frac{\mathrm{d}g}{\mathrm{d}x} = f(g(x))\frac{\mathrm{d}g}{\mathrm{d}x}   
            .\]
    \end{corollary}
    \begin{definition}[Indefinite integral]
        \[
            \int f(x) \,\mathrm{d}x = \int_{x_0}^{x} f(t) \,\mathrm{d}t
        .\]
    \end{definition}
    \subsection{Techniques of Integration}
    skipped
\end{document}