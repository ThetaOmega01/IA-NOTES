\documentclass[10pt]{article}
\pdfoutput=1 
\usepackage{NotesTeX,lipsum}
\usepackage{IEEEtrantools}
\usepackage{mhchem}

\def\d{{\mathrm d}}
\def\e{{\mathrm e}}
\def\g{{\mathrm g}}
\def\h{{\mathrm h}}
\def\f{{\mathrm f}}
\def\p{{\mathrm p}}
\def\s{{\mathrm s}}
\def\t{{\mathrm t}}
\def\i{{\mathrm i}}

\def\A{{\mathrm A}}
\def\B{{\mathrm B}}
\def\E{{\mathrm E}}
\def\F{{\mathrm F}}
\def\G{{\mathrm G}}
\def\H{{\mathrm H}}
\def\P{{\mathrm P}}


\def\bb{\mathbf b}
\def \bc{\mathbf c}
\def\bx {\mathbf x}
\def\bn {\mathbf n}
\def\le{\leqslant}
\def\ge{\geqslant}
\def\arcosh{{\rm arcosh}\,}

\newcommand{\bluecomment}[1]{{\color{blue}#1}}
%\renewcommand{\comment}[1]{}
\newcommand{\redcomment}[1]{{\color{red}#1}}
\newcommand{\tm}{\times}
%\usepackage{showframe}

\title{\begin{center}{\Huge \textit{Differential Equations Notes}}\\{{\itshape Based on Lectures and "An Introduction to ODEs"}}\end{center}}
\author{$\theta\omega\theta$}
\affiliation{
Not in University of Cambridge\\
skipped some talks irrelevant to contents\\
}

\emailAdd{not telling you}

\begin{document}
	\maketitle
	\flushbottom
	\newpage
    \pagestyle{fancynotes}
    %main doc
    \part{Basic Calculus}
    \section{Differentiation}
    \subsection{Definitions and methods}
	\begin{definition}[Derivative]
        The derivative of a function $f(x)$ wrt its argument $x$ is the function
        \[
            \frac{\mathrm{d}f}{\mathrm{d}x} = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} 
        .\]
        We define higher derivatives recursively by 
        \[
            \frac{\mathrm{d}^nf}{\mathrm{d}x^n} = \frac{\mathrm{d}}{\mathrm{d}x}\left( \frac{\mathrm{d}^{n-1}f}{\mathrm{d}x^{n-1}}  \right)   
        .\]
    \end{definition}
    For the derivative to exist, we need
    \[
        \lim_{h \to 0-} \frac{f(x+h)-f(x)}{h} = \lim_{h \to 0+} \frac{f(x+h)-f(x)}{h} 
    .\]
    
    Rules for differentiation:
    \begin{enumerate}
        \item \textbf{Chain rule}: $ (f(g(x)))' = f'(g(x))g'(x) $.
        \item \textbf{Product rule}: $ (u\cdot v)' = u\cdot v'+u'\cdot v $.
        \item \textbf{Leibniz's rule}: generalisation of product rule.\sidenote{There are multiple ways to prove, e.g. by induction.}
        \[
            \frac{\mathrm{d}^n}{\mathrm{d}x^n}(u\cdot v) = \sum_{k=0}^{n}\binom{n}{k}u^{(k)}v^{(n-k)}
        .\]
    \end{enumerate}
    \subsection{Order of magnitude}
    The goal is to compare the sizes of functions, in the vicinity of specific points.
    \begin{definition}[Little and Big o]
        We say $ f(x) = o(g(x)) $ as $x\to x_0$ if $ \lim_{x \to x_0} \frac{f(x)}{g(x)} = 0 $.

        We say $ f(x) = O(g(x)) $ as $x\to x_0$ if $ \exists M, \delta>0, \left| x-x_0 \right| <\delta \Rightarrow \left| f(x) \right| \le M \left| g(x) \right| . $ The infinite case is defined similarly.
    \end{definition}

    To find the tangent line to $f$ at $x_0$, note that 
    \[
        \begin{aligned}
             & \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x=x_0} = \frac{f(x_0+h)-f(x_0)}{h}+ \frac{o(h)}{h} & \text{when $ h \to 0$} \\
             \Longrightarrow & f(x_0+h) = f(x_0)+\frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x=x_0} h + o(h)& \text{when $ h \to 0$} \\
        \end{aligned}
    \]
    \subsection{Taylor's Theorem and L'Hopital's Theorem}
    We want to approximate a function $f(x)$ with a polynomial of order $n$:
    \[
        f(x) = \underbrace{a_0+a_1x+\cdots+a_nx^n}_{P_n(x)}
    .\]
    Differentiating recursively we get 
    \begin{equation}\label{eq:taylor_series}
        P_n(x) = f(x_0)+(x-x_0)f'(x_0)+\cdots+\frac{(x-x_0)^n}{n!}f^{(n)}(x_0).
    \end{equation}
    Alternatively, we can write $ f(x) = P_n(x)+E_n $, where $E_n$ is called the \textit{remainder/error}.

    By generalisation of $ f(x+h) = f(x) + hf'(x)+o(h), h\to 0 $, we get 
    \begin{equation}\label{eq:taylor_series_with_remainder}
        f(x+h) = f(x)+hf'(x)+\frac{h^2}{2}f'(x)+\cdots+\frac{h^n}{n!}f^{(n)}(x)+o(h^n).
    \end{equation}

    By refining the range of $o(h^n)$ we get
    \begin{theorem}[Taylor]\label{thm:taylor_theorem}
        If the first $n+1$ derivatives of $f(x)$ exist, then 
        \[
            f(x+h) = f(x)+hf'(x)+\frac{h^2}{2}f'(x)+\cdots+\frac{h^n}{n!}f^{(n)}(x)+O(h^{n+1}).
        .\]
    \end{theorem}

    Using this we can prove 
    \begin{theorem}[L'Hopital]\label{thm:L'Hopital}
        Let $f$ and $g$ be differentiable at $x=x_0$ and
        \[
            \lim_{x \to x_0} f(x)=f(x_0)=0, \quad \lim_{x \to x_0} g(x)=g(x_0)=0
        .\]
    \end{theorem}
    \begin{proof}(Not rigorous)
        As $x\to x_0$, 
        \[
            \begin{aligned}
                 \frac{f(x)}{g(x)} &= \frac{f(x_0)+(x-x_0)f'(x_0)+o(x-x_0)}{g(x_0)+(x-x_0)g'(x_0)+o(x-x_0)}\\
                 &= \frac{(x-x_0)f'(x_0)+o(x-x_0)}{(x-x_0)g'(x_0)+o(x-x_0)}\\
                 &\to \frac{f'(x_0)}{g'(x_0)}.
            \end{aligned}
        \]
    \end{proof}
    Note that it can be applied recursively.
    \section{Integration}
    \subsection{Definition}
    All functions mentions are assumed to be well-hehaved.

    We evaluate the area under the curve of $f(x)$ by considering
    \[
        \sum_{n=0}^{N-1}f(x_n)\Delta x
    \]
    where $ \Delta x = \frac{b-a}{N} $ and $ x_n = a+n\Delta x. $
    \begin{theorem}[MVT]\label{thm:mean_value_theorem_for_integral}
        For a continuous function $f(x)$:
        \[
            \int_{x_n}^{x_{n+1}} f(x) \,\mathrm{d}x = f(x_c)(x_{n+1}-x_n) \quad \text{for some } x_c\in (x_n,x_{n+1})
        .\]
    \end{theorem}
    Estimate $ f(x_c) $ as follows:
    \[
        f(x_c) = f(x_n)+O(x_c-x_n) = f(x_n)+O(x_{n+1}-x_n)
    .\]\
    Hence
    \[
        \begin{aligned}
            \int_{x_n}^{x_{n+1}} f(x) \,\mathrm{d}x &= f(x_c)(x_{n+1}-x_n)\\
            &= [f(x_n)+O(x_{n+1}-x_n)](x_{n+1}-x_n)\\
            &= \Delta x f(x_n)+O(\Delta x^2).
        \end{aligned}
    \]
    Therefore the error $ \epsilon = O(\Delta x^2) $. It follows that
    \[
        \int_{a}^{b} f(x) \,\mathrm{d}x = \lim_{\Delta x \to 0} \left\{ \left[ \sum_{n=0}^{N-1}f(x_n)\Delta x \right] + O(N\Delta x^2)\right\}
    .\]
    Hence 
    \begin{definition}[Definite integral]
        $\displaystyle \int_{a}^{b} f(x) \,\mathrm{d}x = \lim_{N \to \infty} \sum_{n=0}^{N-1}f(x_n)\Delta x$ 
    \end{definition}
    \subsection{Fundamental Theorem of Calculus}
    \begin{theorem}[FTC]\label{thm:ftc}
        Let
        \[
            F(x) = \int_{a}^{x} f(t) \,\mathrm{d}t
        ,\]
        then
        \[
            \frac{\mathrm{d}F}{\mathrm{d}x} = f(x) 
        .\]
    \end{theorem}
    \begin{proof}
        From the definition of derivative:
        \[
            \begin{aligned}
                 \frac{\mathrm{d}F}{\mathrm{d}x} &= \lim_{h \to 0} \frac{1}{h} \left\{ \int_{a}^{x+h} f(t) \,\mathrm{d}t - \int_{a}^{x} f(t) \,\mathrm{d}t\right\}\\
                 &= \lim_{h \to 0} \frac{1}{h} \int_{x}^{x+h} f(t) \,\mathrm{d}t\\
                 &= \lim_{h \to 0} \frac{1}{h}\left( f(x)h+O(h^2) \right)\\
                 &= f(x).
            \end{aligned}
        \]
    \end{proof}
    \begin{corollary}\label{col:ftc}
            \[
                \frac{\mathrm{d}}{\mathrm{d}x}\int_{x}^{b} -f(t) \,\mathrm{d}t 
            .\]
            \[
                \frac{\mathrm{d}}{\mathrm{d}x} \int_{a}^{g(x)} f(t) \,\mathrm{d}t = \frac{\mathrm{d}}{\mathrm{d}x}F(g(x)) = \frac{\mathrm{d}F}{\mathrm{d}g} \frac{\mathrm{d}g}{\mathrm{d}x} = f(g(x))\frac{\mathrm{d}g}{\mathrm{d}x}   
            .\]
    \end{corollary}
    \begin{definition}[Indefinite integral]
        \[
            \int f(x) \,\mathrm{d}x = \int_{x_0}^{x} f(t) \,\mathrm{d}t
        .\]
    \end{definition}
    \subsection{Techniques of Integration}
    skipped
    \section{Introduction to multivariable functions}\marginnote{Lecture 5.}
        \subsection{Partial derivative}
        \begin{definition}
            The \textit{partial derivative} of $ f(x,y) $ wrt $x$ is 
            \begin{equation}
                \frac{\partial f}{\partial x}\Big|_y = \lim_{\delta x \to 0} \frac{f(x+\delta x,y)-f(x,y)}{\delta x}.
            \end{equation}
            Similarly 
            \[
                \frac{\partial f}{\partial y}\Big|_x = \lim_{\delta y \to 0} \frac{f(x,y+\delta y)-f(x,y)}{\delta y}.
            \]
            We can take them in any order to form \textit{cross derivatives}.
        \end{definition}
        Note that 
        \begin{equation}
            \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial }{\partial x}\left( \frac{\partial f}{\partial y}  \right)=\frac{\partial }{\partial y}\left( \frac{\partial f}{\partial x}  \right) .
        \end{equation}
        \subsection{Multivariable chain rule}
        \begin{theorem}\label{thm:mvchainrule}
            For well-behaved functions, we have 
            \begin{equation}
                \d f=\frac{\partial f}{\partial x} \d x+\frac{\partial f}{\partial y} \d y
            \end{equation}
        \end{theorem}
        \begin{proof}
            Note that 
            \[
                \begin{aligned}
                     \delta f &= f(x+\delta x, y+\delta y)-f(x+\delta x, y)+f(x+\delta x, y)-f(x,y)\\
                     &= f(x+\delta x,y)+\delta y \frac{\partial f}{\partial y}(x+\delta x, y)+o(\delta y)-f(x+\delta x,y)\\
                     &+ f(x,y)+\delta x \frac{\partial f}{\partial x}(x,y)+o(\delta x)-f(x,y) \\
                     &= \delta y \frac{\partial f}{\partial y}(x+\delta x, y)+\delta x \frac{\partial f}{\partial x}(x,y)+o(\delta x)+o(\delta y)\\
                     &= \delta y \left( \frac{\partial f}{\partial y}(x,y)+\delta x \frac{\partial }{\partial x}\left( \frac{\partial f}{\partial y}(x,y)  \right)+o(\delta x)   \right)+\delta x \frac{\partial f}{\partial x}(x,y)+o(\delta x)+o(\delta y)\\
                     &= \delta y \frac{\partial f}{\partial y}(x,y)+\delta x \frac{\partial f}{\partial x}(x,y)+\delta x \delta y \frac{\partial }{\partial x}\left( \frac{\partial f}{\partial y}(x,y)  \right)+o(\delta x)+o(\delta y)+o(\delta x \delta y).
                \end{aligned}
            \]
            Taking limit gives the result.
        \end{proof}
        \begin{remark}
            For $ f(x(t),y(t)) $, we have
            \begin{equation}
                \frac{\d f}{\d t}=\lim _{\delta x, \delta y, \delta t\to 0}\left[\frac{\partial f}{\partial x} \frac{\d x}{\d t}+\frac{\partial f}{\partial y} \frac{\d y}{\d t}\right] = \frac{\partial f}{\partial x} \frac{\d x}{\d t}+\frac{\partial f}{\partial y} \frac{\d y}{\d t}.
            \end{equation}
            And integral form:
            \begin{equation}
                \int \d f=\int \frac{\partial f}{\partial x} \d x+\int \frac{\partial f}{\partial y} \d y
            \end{equation}
            In this case we need to specify the \textit{path} of integral as there might be some priority issues.
        \end{remark}
        \subsection{Applications of multivariable chain rule}
        \subsubsection{Change of variables}
        It is often useful to write a DE in a different coordinate system before solving it. Need to transform the derivatives into the new coordinate system. 
        \begin{example}
            Change from cartesian coordinates to polar coordinates: $ x=r\cos \theta, y=r\sin \theta $. Firstly, write
            \[
                f=f(x(r,\theta),y(r,\theta))
            .\]
            We have 
            \[
                \frac{\partial f}{\partial r} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial r}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial r}=\frac{\partial f}{\partial x}\cos \theta+\frac{\partial f}{\partial y}\sin \theta
                \marginnote{By regarding $ \frac{\partial f}{\partial r}$ as $\frac{\mathrm{d}f}{\mathrm{d}r} $ with $\theta$ fixed, we get this result.}     
            .\]
            Similar for other partial derivatives.
        \end{example}
        \subsubsection{Implicit Differentiation}
        Consider $ f(x,y,z)=c, c\in \mathbb{R} $. $f$ describes a surface in 3d space. $ f(x,y,z)=c $ implicitly defines $ x(y,z),y(x,z),z(x,y) $. However, we can find $ \frac{\partial z}{\partial x}  $ here using implicit differentiation.

        Consider $ f(x,y,z(x,y))=c $.
        \[
            \d f = \frac{\partial f}{\partial x}\d x+ \frac{\partial f}{\partial y}\d y+ \frac{\partial f}{\partial z}\d z   
        .\]
        Finding the partial derivative for $x$: 
        \[
            \begin{aligned}
                && \frac{\partial f}{\partial x}\Big|_y &= \frac{\partial f}{\partial x}\Big|_{yz}\frac{\partial x}{\partial x}\Big|_y+\frac{\partial f}{\partial y}\Big|_{xz}\frac{\partial y}{\partial x}\Big|_y+\frac{\partial f}{\partial z}\Big|_{xy}\frac{\partial z}{\partial x}\Big|_y\marginnote{Notice the subscripts are very important since they discribes different functions}\\
                && &= \frac{\partial f}{\partial x}\Big|_{yz}+\frac{\partial f}{\partial y}\Big|_{xz}\frac{\partial y}{\partial x}\Big|_y+\frac{\partial f}{\partial z}\Big|_{xy}\frac{\partial z}{\partial x}\Big|_y.\\
                &\Longleftrightarrow &\frac{\partial f}{\partial x}\Big|_y&=\frac{\partial f}{\partial x}\Big|_{yz}+\frac{\partial f}{\partial z}\Big|_{xy}\frac{\partial z}{\partial x}\Big|_y  \marginnote{Since $ \frac{\partial y}{\partial x}\Big|_y=0  $}  \\    
                &\Longleftrightarrow &0 &= \frac{\partial f}{\partial x}\Big|_{yz}+\frac{\partial f}{\partial z}\Big|_{xy}\frac{\partial z}{\partial x}\Big|_y \marginnote{Since $f=c$ along the surface $z(x,y)$.} \\
                &\Longleftrightarrow && \boxed{\frac{\partial z}{\partial x}\Big|_y=-\frac{\partial f/ \partial x|_{yz}}{\partial f/\partial z|_{xy}} }    
            \end{aligned}
        \]
        Note that $ \frac{\partial f}{\partial x}\Big|_{yz} \neq 0 $ in general.
        \begin{remark}
            Reciprocal rule still holds as long as the same variable(s) are held fixed. e.g.
            \[
                \frac{\partial r}{\partial x}\Big|_y = \frac{1}{\frac{\partial x}{\partial r}\Big|_y }\quad\text{but}\quad  \frac{\partial r}{\partial x}\Big|_y \neq \frac{1}{\frac{\partial x}{\partial r}\Big|_{\theta} }
            .\]
        \end{remark}
        \subsubsection{Differentiation of an integral wrt its parameters}
        Consider a family of functions $ f(x;\alpha) $, where $ \alpha $ is the parameter. Define
        \[
            I(\alpha) = \int_{a(\alpha)}^{b(\alpha)} f(x;\alpha) \,\mathrm{d}x
        .\]
        \begin{IEEEeqnarray*}{rCl}
            \frac{\mathrm{d}I}{\mathrm{d}\alpha}  & = & \lim_{\delta \alpha \to 0} \frac{I(\alpha+\delta \alpha)-I(\alpha)}{\delta \alpha}\marginnote{Draw a graph to understand the steps.}
        \\
            & = &\lim_{\delta \alpha \to 0} \frac{1}{\delta \alpha}\left[ \int_{a(\alpha+\delta \alpha)}^{b(\alpha+\delta \alpha)} f(x;\alpha+\delta \alpha) \,\mathrm{d}x-\int_{a(\alpha)}^{b(\alpha)} f(x;\alpha) \,\mathrm{d}x \right]
        \\
            & = &\lim_{\delta \alpha \to 0} \frac{1}{\delta \alpha}\left[ \int_{a(\alpha)}^{b(\alpha)}f(x;\alpha+\delta \alpha)- f(x;\alpha) \,\mathrm{d}x -\int_{a(\alpha)}^{a(\alpha+\delta \alpha)} f(x;\alpha+\delta \alpha) \,\mathrm{d}x+\int_{b(\alpha)}^{b(\alpha+\delta \alpha)} f(x;\alpha+\delta \alpha) \,\mathrm{d}x \right]
        \\ 
            & = &\int_{a(\alpha)}^{b(\alpha)}\frac{\partial f}{\partial \alpha}  \,\mathrm{d}x - f(a;\alpha) \lim_{\delta \alpha \to 0} \frac{a(\alpha+\delta \alpha)-a(\alpha)}{\delta \alpha}+ f(b;\alpha)\lim_{\delta \alpha \to 0}\frac{b(\alpha+\delta \alpha)-b(\alpha)}{\delta \alpha}.\marginnote{When $ \delta \alpha $ is very small, we can approximate the latter two integrals with the area of the rectangle of hight $ f(a;\alpha) $ and width $ a(\alpha+\delta \alpha)-a(\alpha) $.}
        \end{IEEEeqnarray*}
        Hence,
        \[
            \boxed{\frac{\mathrm{d}I}{\mathrm{d}\alpha} = \frac{\mathrm{d}}{\mathrm{d}\alpha}\int_{a(\alpha)}^{b(\alpha)} f(x;\alpha) \,\mathrm{d}x =  \int_{a(\alpha)}^{b(\alpha)}\frac{\partial f}{\partial \alpha}  \,\mathrm{d}x+f(b;\alpha)\frac{\mathrm{d}b}{\mathrm{d}\alpha}-f(a;\alpha) \frac{\mathrm{d}a}{\mathrm{d}\alpha} }
        .\]

    \part{First order linear ODEs}
    \section{Terminology}
    \begin{definition}
        An \textit{ordinary differential equation} is a differential equation involving a function of one variable. A \textit{partial differential equation} is (a) differential equation(s) involving a function of more than one variable.

        \textit{n}th order DE: the highest order of derivative is $n$.

        Linear: dependent variable appears linearly.
    \end{definition}
    \section{Prelude: Exponential functions}
    Consider $f=a^x, a>0$, we have
    \[
        \begin{aligned}
             \frac{\mathrm{d}f}{\mathrm{d}x} &= \lim_{h \to 0} \frac{a^{x+h}-a^x}{h}=a^x \lim_{h \to 0} \frac{a^h-1}{h}\\
            &= \lambda a^x.
        \end{aligned}
    \]
    Hence
    \begin{definition}
        Define $ \exp(x)=e^x $ as the solution to the DE 
        \[
            \frac{\mathrm{d}f}{\mathrm{d}x}=f(x), \quad f(0)=1 
        .\]
        Therefore $e$ is the value of $a$ such that $\lambda=1$. i.e.,
        \[
            \lim_{h \to 0} \frac{e^h-1}{h}=1
        .\]
    \end{definition}
    Define $ \ln (x) $ as the inverse of $e^x$ such that $ e^{\ln(x)}=x $.

    Consider $ a^x=e^{\ln(a)x}$, so 
    \[
        \frac{\mathrm{d}f}{\mathrm{d}x}=(\ln a)a^x, \lambda=\ln a 
    .\]

    The exponential function is the \textit{eigenfunction} of the differential operator.

    The \textit{eigenfunction} of an operator is unchanged by the action of the operator, except for a multiplicative scaling by the eigenvalue.
    \section{Rules for linear ODEs}
    \begin{enumerate}[\bfseries 1.]
        \item Any linear homogeneous ODE with constant coefficients has solutions of form $ e^{\lambda x} $, the eigenfunction. By \textit{homogeneous} we mean that all terms involve the dependent variable or its derivatives.

        This means that $y=0$ is a trivial solution for all homogeneous ODEs.

        Constant coefficients imply that the independent variable does not appear explicitly in DE.
        \item For linear homogeneous ODEs, any constant multiple of a solution is also a solution.
        \item An $n$th order ODE has $n$ independent solutions.

        For constant coefficient ODEs, this rule follows from the fundamental theorem of algebra.
        \item An $n$th order ODE requires $n$ initial/boundary conditions.
    \end{enumerate}
    \section{Inhomogeneous(forced) first order ODEs with constant coefficients}
    \subsection{Constant forcing}
    \begin{example}
        Consider the equation 
        \[
            5y'-3y=10
        .\]
        Solution steps:
        \begin{enumerate}
            \item Write the general solution $ y=y_p+y_c $ where $y_p$ is a \textit{particular integral} and $y_c$ is a complementary function
            \item Find $ y_p $ by simply setting $y'=0$. In this case, $y=-10/3$.
            \item Insert general solution into DE:
            \[
              \begin{aligned}
                   &5(y_p+y_c)'-3(y_p+y_c)&=10\\
                   \Longleftrightarrow & 5y_c'+10-3y_c&=10\\
                    \Longleftrightarrow & 5y_c-3y_c'=0.
              \end{aligned}  
            \]
            Note that $ y_c $ is a solution to corresponding homogeneous equation.
            \item Solve for $y_c$. In this case, $y_c=Ae^{3x/5}$.
            \item Combine $y_p$ and $y_c$.
        \end{enumerate}
    \end{example}
    \subsection{Eigenfunction forcing}
    Example problem: In a sample of rock, isotope A decays to isotope B at a rate proportional to $a$, the number of nuclei of A. B decays to C at a rate proportional to $b$, the number of nuclei of $B$. Find $b(t)$.
    
    We have 
    \[
        \begin{aligned}
            &\frac{\mathrm{d}a}{\mathrm{d}t} = -k_a a \Longrightarrow  a = a_0 e^{-k_a t}\\
            &\frac{\mathrm{d}b}{\mathrm{d}t} = k_a a -k_b b, 
        \end{aligned}
    \]
    which means $ \dot{b}+k_b b=k_a a_0 e^{-k_a t} $. RHS is called a \textit{forcing term}, and it is an eigenfunction of differential operator.

    We \textit{guess} the form of the particular integral
    \[
        b_p = ce^{-k_a t}
    ,\]
    then the equation becomes 
    \[
        -k_a c+k_b c = k_a a_0 \Longleftrightarrow c=\frac{k_a}{k_b-k_a}a_0,\quad \text{for }k_b\neq k_a
    .\]
    Since the general solution for the DE is $b=b_p+b_c$,
    \[
        \dot{b_c}+k_b b_c = 0 \Longleftrightarrow b_c = De^{-k_b t}
    .\]
    Hence
    \[
        b=\frac{k_{a}}{k_{b}-k_{a}} a_{0} e^{-k_{a} t}+D e^{-k_{b} t}
    .\]
    If $b(0)=0$, $ D = -c $, then 
    \[
        b = \frac{k_a}{k_b-k_a}a_0\left( e^{-k_a t}-e^{-k_b t} \right)
    .\]
    Taking the ratio of $ b $ and $a$:
    \[
        \frac{b(t)}{a(t)} = \frac{k_a}{k_b-k_a}\left( 1-e^{(k_a-k_b)t} \right)
    .\]
    We can date the age without knowing $a_0$ is. This result allows rocks and other materials to be dated by measuring ratio of isotopes.
    \section{First order ODEs of non-constant coefficients}
    The general form is 
    \[
        a(x)y'+b(x)+y = c(x)
    .\]
    The standard form is 
    \[
        y'+ p(x)y=f(x)
    .\]
    Solved using \textit{integrating factors}, multiply by IF $\mu$:
    \[
        \mu y'+(\mu p)y=\mu f
    .\]
    If $ \mu p=\mu' $, LHS$= (\mu y)' $ by product rule. Hence we want $p = \mu'/\mu$.
    \[
        \int p \,\mathrm{d}x = \int \frac{\mu'}{\mu} \,\mathrm{d}x = \ln \mu \Longrightarrow\boxed{\mu = e^{\int p(x) \,\mathrm{d}x}}
    .\]
    Thus the DE becomes 
    \[
        (\mu y)' = \mu f \Longleftrightarrow y = \frac{1}{\mu}\int \mu f \,\mathrm{d}x
    .\]
    \section{Discrete equations}\marginnote{Lecture 8}
    A \textit{discrete equation} is an equation involving a function evaluated at a discrete set of points.
    \subsection{Numerical integration}
    Consider a discrete representation of $y(x)$, $ y(x_1),\dots,y(x_{n}) $. One approximation to $y'$ is 
    \[
        \frac{\mathrm{d}y}{\mathrm{d}x}\Big|_{x_n} \approx \frac{y_{n+1}-y_n}{h},\quad h=\frac{x_n}{n} 
    ,\]
    given that $x_i$ are uniformly distributed. This is called the \textit{Forward Euler} approximation, but it is not the best approximation of the derivative in most contexts.
    
    \begin{example}
        Consider $ 5y'-3y=0 $. We can approximate the equation by 
        \[
            5\frac{y_{n+1}-y_n}{h}-3y=0
        ,\]
        which is called a \textit{difference equation}, and deduce that 
        \[
            y_{n+1}=\left( 1+\frac{3h}{5} \right)y_n
        ,\]
        which is called a \textit{recurrence relation}.

        Apply recurrence relation repeatedly:
        \[
            y_n=\left( 1+\frac{3h}{5} \right)^{n}y_{0}=\left( 1+\frac{3x_n}{5n} \right)^{n}y_{0}
        .\]
        Euler's definition of $ e^x $ is 
        \[
            e^x=\lim_{n \to \infty} \left( 1+\frac{x}{n} \right)^n
        .\]
        It can be shown that this definition is equivalent to the previous definition. Hence 
        \[
            y(x)=\lim_{n \to \infty} y_n = y_0e^{3x/5}
        .\]
        \bluecomment{Note for finite $n$, $y_n<y(x)$.}
    \end{example}
    \subsection{Series solutions}
    A powerful way to solve ODEs is to seek solutions in the form of an infinite power series. 
    \[
        y(x)=\sum_{n=0}^{\infty}a_n x^n
    .\]
    Plug into DE and find a solution.
    \begin{example}
        Consider $ 5y'-3y=0 $. Let 
        \[
            y(x)=\sum_{n=0}^{\infty}a_n x^n, \quad y'=\sum_{n=1}^{\infty}na_n x^{n-1}
        .\]
        Multiply both sides by $x$:
        \[
            \begin{aligned}
                xy'&=\sum_{n=1}^{\infty}na_nx^n,\\
                xy&= \sum_{n=0}^{\infty}a_n x^{n+1}=\sum_{n=1}^{\infty}a_{n-1} x^{n}.
            \end{aligned}
        \]
        Then the DE becomes 
        \[
            \begin{aligned}
                &5\sum_{n=1}^{\infty}na_nx^n-3\sum_{n=1}^{\infty}a_{n-1} x^{n}=0\\
                \Longleftrightarrow & \sum_{n=1}^{\infty}x^n\left( 5na_n-3a_{n-1} \right)=0.
            \end{aligned}
        \]
        This holds for every $x\in \mathbb{R}$, so it holds if and only if
        \[
            \forall x\in \mathbb{R}, 5na_n-3a_{n-1}=0 \Longleftrightarrow a_n = \frac{3}{5n}a_{n-1} \Longleftrightarrow a_n=\left( \frac{3}{5} \right)^n \frac{a_0}{n!}
        .\]
        Hence
        \[
            y= a_0\sum_{n=0}^{\infty} \left( \frac{3}{5} \right)^n \frac{x^n}{n!} = a_0 e^{3x/5}
        .\]
        This converges for all $x$, so $ y(x)=a_0 e^{3x/5} $ is a solution.
    \end{example}
    \part{First order nonlinear ODEs}
    General form is 
    \begin{equation}
        Q(x,y)\frac{\mathrm{d}y}{\mathrm{d}x}+P(x,y)=0.
    \end{equation}
    \section{Separable equations}
    (10.1) is separable if and only if it can be written in the form 
    \[
        q(y)\d y=p(x)\d x,
    \]
    and we simply solve $x,y$ by integrating both sides.
    \section{Exact equations}
    (10.1) is an \textit{exact equation} if and only if 
    \[
        Q(x,y)\d y+P(x,y)\d x\tag{*}
    \]
    is an \textit{exact differential} of function $f(x,y)$. i.e., $ \d f= Q\d y+P\d x $. If this holds, then (10.1) implies that $ \d f=0 $ and $f(x,y)$ is constant. We can use multivariable chain rule to check.
    \[
        \d f=\frac{\partial f}{\partial x}\d x+ \frac{\partial f}{\partial y}\d y \Longrightarrow \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{\mathrm{d}y}{\mathrm{d}x}=0     
    .\]
    Comparing with (10.1), if $(*)$ is an exact differential, then $\exists f(x)$ such that 
    \[
        \frac{\partial f}{\partial x}=P(x,y),\quad \frac{\partial f}{\partial y}=Q(x,y)  \tag{**}
    .\]
    Hence 
    \[
        \begin{aligned}
             & \frac{\partial^2 f}{\partial y\partial x} =\frac{\partial P}{\partial y} \land  \frac{\partial^2 f}{\partial x\partial y} =\frac{\partial Q}{\partial x}\\
             \Longleftrightarrow & \boxed{\frac{\partial P}{\partial y}=\frac{\partial Q}{\partial x}}.
        \end{aligned}
    \]
    If it holds throughout a \textit{simply connected} domain $\mathcal{D}$, then $ P\d x+Q\d y $ is an exact differential of a single-valued function $f(x,y)$ in $D$. Henc we can use this to check exact equations.

    $f(x,y)$ can be found by integrating $(**)$.
    \begin{example}
        Consider 
        \[
            6y(y-x)\frac{\mathrm{d}y}{\mathrm{d}x}+(2x-3y^2)=0 
        .\]
        Here $P=2x-3y^2, Q=6y(y-x)$. We have 
        \[
            \frac{\partial P}{\partial y}=-6y= \frac{\partial Q}{\partial x}
        ,\]
        so it is an exact equation.
        Note that 
        \[
            \begin{aligned}
                 &\int \frac{\partial f}{\partial x}  \,\mathrm{d}x=x^2-3xy^2+h(y),\\
                 &\frac{\partial f}{\partial y}=(x^2-3xy^2+h(y))'_y= -6xy+h'(y)=6y(y-x),\\
                 \Longrightarrow & h'=6y^2 \Longrightarrow h=2y^3.
            \end{aligned}
        \]
        Hence $f(x,y)=x^2-3xy^2+2y^3+C$ and 
        \[
            x^2-3xy^2+2y^3=C
        \]
        is the general solution. 
    \end{example}
    \section{Isoclines and solution curves}\marginnote{Lecture 9.}
    Nonlinear equations are not guaranteed to have simple/closed form solutions. Nevertheless we can analyze the behaviour of the system without solving.

    Consider an ODE of the form 
    \[
        \frac{\mathrm{d}y}{\mathrm{d}t}=f(y,t) 
    .\]
    Each intial condition will give a different solution curve.
    \begin{example}
        Consider the equation 
        \[
            \frac{\mathrm{d}y}{\mathrm{d}t}=t(1-y^2)=f(y,t) \tag{$*$}
        .\]
        It is separable:
        \[
            \begin{aligned}
                 &\int \frac{\d y}{1-y^2} \,\mathrm{d}y=\int t \,\mathrm{d}t\\
                 \Longrightarrow &y= \frac{A-e^{-t^2}}{A+e^{-t^2}}.
            \end{aligned}
        \]
        This general solution produces a family of solution curves, parameterised by by $A$.
    \end{example}
    \begin{definition}[Isocline]
        An \textit{isocline} is the curve along which $ f=\dot{y}=C $, where $C$ is a constant.
    \end{definition}
    Procedure of drawing a curve: draw isoclines, inspect the slope of $y$, draw a vector field, and plot the lines.
    \begin{remark}
        Since $f(y,t)$ is single-valued, any two solution curves do not cross.
    \end{remark}
    \section{Fixed(equilibrium) points}
    \begin{definition}
        A fixed point is a point where
        \[
            \frac{\mathrm{d}y}{\mathrm{d}t}=f(y,t)=0 
        .\]
        A fixed point is called \textit{stable}(\textit{unstable}) if solution curves in a small neighbourhood of the fixed point converge(diverge) to(away) the fixed point.
    \end{definition}
    We can analyze the stability of fixed points using a \textit{perturbation} analysis.

    Let $y=a$ be a fixed point of $ \frac{\mathrm{d}y}{\mathrm{d}t}=f(y,t)  $, i.e. $ f(a,t)=0 $. Consider a small perturbation from the fixed point: $ y=a+\epsilon(t) $ We have 
    \[
        \begin{aligned}
            \frac{\mathrm{d}\epsilon}{\mathrm{d}t}&=\frac{\mathrm{d}(y-a)}{\mathrm{d}t}
            = \frac{\mathrm{d}y}{\mathrm{d}t}\\
            &=f(a+\epsilon,t)\\
            &= f(a,t)+\epsilon \frac{\partial f}{\partial y}(a,t)+O(\epsilon^2).
        \end{aligned}
    \]
    For small $ \epsilon $, we have 
    \[
        \frac{\mathrm{d}\epsilon}{\mathrm{d}t}\approx \epsilon \frac{\partial f}{\partial y}(a,t)  
    .\]
    Hence we've converted the non-linear ODE into a linear one wrt $ \epsilon $.

    If $ \lim_{t \to \infty} \epsilon=0 $, then $a$ is a stable fixed point. Conversely if $ \lim_{t \to \infty} \epsilon=\infty  $, then $a$ is an unstable fixed point. If $ f'_y(a,t)=0 $, then we need higher order terms in Taylor series.
    \begin{example}
        Consider $f(y,t)=t(1-y^2)$. The fixed points are $y=\pm 1$. $ f'_y=-2yt $. At $y=1$, we have 
        \[
            \dot{\epsilon}\approx -2\epsilon t \Rightarrow \epsilon=\epsilon_0 e^{-t^2}\to 0
        .\] 
        Hence $1$ is stable.
        
        At $y=-1$, $ \dot{\epsilon}=2t\epsilon \Rightarrow \epsilon=\epsilon_0e^{t^2}\to \infty $. Hence $-1$ is unstable.
    \end{example}
    \section{Autonomous DEs}
    \begin{definition}
        An \textit{autonomous DE} is a special case when $\dot{y}=f(y)$.
    \end{definition}
    In this case, near fixed points $ y=a$, we have $ \dot{\epsilon}=f'_y(a)\epsilon=\epsilon k $, where $k$ is constant. Hence $ \epsilon=\epsilon_0 e^{kt} $. Therefore for autonomous DEs we have 
    \[
      \text{if } \begin{cases}
      f'(a)<0 \Rightarrow &\text{stable F.P.}\\
       f'(a)>0 \Rightarrow &\text{unstable F.P.}\\
      \end{cases}   
    \]
    \section{Phase Portraits}\marginnote{Lecture 10}
    Another way to analyze solutions to a DE is using a geometrical representation of the solution called a \textit{phase portrait}.
    \begin{example}[Chemical kinetics]
        Consider the reaction
        \[
            \ce{NaOH + HCl -> NaCl + H_2O}
        \]
        with 
        \begin{center}
            \begin{tabular}{lccccccc}
              \toprule
              & NaOH & + & HCl & $\rightarrow$ & H$_2$O & + & NaCl \\
              \midrule
              Number of molecules & $a$ & & $b$ & & $c$ & & $c$ \\
              Initial number of molecules & $a_0$ & & $b_0$ & & $0$ & & $0$ \\
              \bottomrule
            \end{tabular}
          \end{center}
          A model of reaction rate is 
          \[
              \frac{\mathrm{d}c}{\mathrm{d}t}=\lambda ab 
          ,\]
          where $ \lambda $ is constant. Atoms are conserved: $ a=a_0-c, b=b_0-c $. Then the equation can be written as 
          \[
              \frac{\mathrm{d}c}{\mathrm{d}t}=\lambda(a_0-c)(b_0-c) 
          .\]
          This is an example of nonlinear first order ODE.

          Plot a 2D phase portrait for this DE. One way is to plot $ \frac{\mathrm{d}c}{\mathrm{d}t}  $ against $t$:
          \begin{center}
            \begin{tikzpicture}
              \draw [->] (-0.5, 0) -- (5, 0) node [right] {$c$};
              \draw [->] (0, -1) -- (0, 4) node [above] {$\dot c$};
              \draw [blue, semithick] (0.5, 3) parabola bend (2.5, -1) (4.5, 3);
              \node at (1.5, 0) [anchor = north east] {$a_0$};
              \node at (3.5, 0) [anchor = north west] {$b_0$};
            \end{tikzpicture}
          \end{center}
          We can analyze the behaviour using 1D phase portrait:
          \begin{center}
            \begin{tikzpicture}
              \draw [->-=0.5] (0, 0) -- (2, 0);
              \draw [->-=0.6] (4, 0) -- (2, 0);
              \draw [->-=0.5, ->] (4, 0) -- (6, 0) node [right] {$c$};
              \draw (2, 0.1) -- (2, -0.1) node [below] {$a_0$};
              \draw (4, 0.1) -- (4, -0.1) node [below] {$b_0$};
            \end{tikzpicture}
          \end{center}
          arrows are drawn by sign of $ \dot{c} $.
    \end{example}
    \begin{example}[Population dynamics]
        Let $ y(t) $ be population, $ \alpha y $ be birth rate and $ \beta y $ be death rate.
        \begin{enumerate}[(a)]
            \item Linear model
            \[
                \frac{\mathrm{d}y}{\mathrm{d}t}=\alpha y-\beta y \Rightarrow y=y_{0}e^{(\alpha-\beta)t} 
            .\]
            If $ \alpha>\beta $, then $ \lim_{t \to \infty} y=\infty  $.
            \item Nonlinear model 
            \[
                \frac{\mathrm{d}y}{\mathrm{d}t} =(\alpha-\beta)y-\gamma y^2
            .\]
            $ \gamma y^2 $ is dominant when $y$ is large. It models increased death rate at high population.

            Equivalently we have $  \dot{y}=ry(1-\frac{y}{\lambda}) $ where $ r=\alpha-\beta, \lambda=\frac{\alpha-\beta}{\gamma} $. $ \lambda $ is called the \textit{carrying capacity}.
            \begin{center}
                \begin{tikzpicture}[yscale = 1.5]
                  \draw [->] (-0.5, 0) -- (4.5, 0) node [right] {$y$};
                  \draw [->] (0, -1) -- (0, 2) node [above] {$\dot{y}$};
                  \node [anchor = north east] at (0, 0) {$O$};
                  \draw [mblue, semithick] (0, 0) parabola bend (1.75, 1.5) (4, -.9796);
                  \node [anchor = north east] at (3.5, 0) {$\lambda$};
              
                  \draw [->-=0.5] (0, -1.5) -- (3.5, -1.5);
                  \draw [-<-=0.2, ->] (3.5, -1.5) -- (4.5, -1.5) node [right] {$y$};
                  \draw (0, -1.4) -- (0, -1.6) node [below] {$O$};
                  \draw (3.5, -1.4) -- (3.5, -1.6) node [below] {$\lambda$};
                \end{tikzpicture}
              \end{center}
        \end{enumerate}
    \end{example}
    \section{Fixed points in discrete equations}
    \subsection{Definitions}
    Consider a first order discrete(difference) equation of the form 
    \[
        x_{n+1}=f(x_n)
    .\]
    Define the \textit{fixed point} as the value of $x_n$ where $ x_{n+1}=x_n $. That is, where
    \[
        f(x_n)=x_n
    .\]
    \subsection{Stability}
    Use perturbation analysis to study its stability. Let $ x_f $ be a stable point of $x_n$, and perturb by a small $ \epsilon $:
    \[
        \begin{aligned}
            f(x_f+\epsilon)&=f(x_f)+\epsilon \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f}+O(\epsilon^2) \\
            &= x_f+\epsilon \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f}+O(\epsilon^2)\\
            &\approx x_f+\epsilon \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f}.
        \end{aligned}
    \]
    Let $ x_n= x_f+\epsilon$, then 
    \[
        x_{n+1}\approx x_f+\epsilon \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f}.
    \]
    Therefore,
    \[
        x_f \text{ is} \begin{cases}
        \text{stable} &\text{if } \left| \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f} \right| <1\\
        \text{unstable} &\text{if }\left| \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f} \right| >1\\
        \end{cases} 
    \]
    \subsection{Example: Logistic map}
    Nonlinear discrete population model 
    \[
        \frac{x_{n+1}-x_n}{\Delta t}=\lambda x_n-\gamma x_{n}^2
    .\]
    Formally,
    \[
        x_{n+1}=(\lambda \Delta t+1) x_n-\gamma \Delta t x_n^2
    .\]
    A simpler version is 
    \[
        x_{n+1}=rx_n(1-x_n)=f(x_n)
    .\]
    This equation is the Logistic map.
    
    \textbf{Fixed points}: Let $ f(x_n)=x_n $. We have $ x_n(r-1-rx_n)=0 \Rightarrow x_n=0 \lor x_n=1-1/r $.

    \textbf{Stability}: Write $ f(x)=rx(1-x) $, then 
    \[
        \frac{\mathrm{d}f}{\mathrm{d}x}=r(1-2x) 
    .\]
    Hence for $x_n=0$ we have $ \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_0=r $, so if $ 0<r<1 $, it is stable. For $ r>1 $, it is unstable.

    For $ x_n=1-\frac{1}{r} $, $ \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_n}=2-r  $. Then for $ 0<r<1 $ it is unphysical since $x_n<0$. If $ 1<r<3 $, it is stable. For $ r>3 $ it is unstable.

    \part{Higher Order Linear ODEs}
    \section{Linear 2nd order ODEs with constant coefficients}\marginnote{Lecture 11.}
    The general form of a linear 2nd order ODE with constant coefficients is 
    \begin{equation}\label{eq:11.1}
        a \frac{\mathrm{d}^2y}{\mathrm{d}x^2}+b \frac{\mathrm{d}y}{\mathrm{d}x}+cy = f(x),
    \end{equation}
    $a,b,c$ are constants and $f(x)$ is called the \textit{forcing term}.

    Note that from the definition of derivative we have 
    \[
        \frac{\mathrm{d}}{\mathrm{d}x}(y_1+y_2)=\frac{\mathrm{d}y_1}{\mathrm{d}x}+\frac{\mathrm{d}y^2}{\mathrm{d}x},   
    \]
    and similarly
    \[
        \frac{\mathrm{d}^2}{\mathrm{d}x^2}(y_1+y_2)=\frac{\mathrm{d}^2y_1}{\mathrm{d}x^2}+\frac{\mathrm{d}^2y^2}{\mathrm{d}x^2}
    .\]

    \begin{definition}
        A \textit{linear differnetial operator} $ \mathcal{D} $ is built from a linear combination of derivatives. For example,
    \[
        \mathcal{D} = a\frac{\mathrm{d}^2}{\mathrm{d}x^2}+b \frac{\mathrm{d}}{\mathrm{d}x}+c.
    \]
    \end{definition}
    It follows that 
    \[
        \mathcal{D}(y_1+y_2)=\mathcal{D}(y_1)+\mathcal{D}(y_2)
    .\]

    We can exploit this by solving (\ref{eq:11.1}) in 3 steps
    \begin{enumerate}
        \item Find the complementary functions $y_1,y_2$ which satisfy the homogeneous equation
        \begin{equation}\label{eq:11.2}
            a \frac{\mathrm{d}^2y}{\mathrm{d}x^2}+b \frac{\mathrm{d}y}{\mathrm{d}x}+cy = 0.
        \end{equation}
        \item Find a particular integral $y_p$ which solves (\ref{eq:11.1}).
        \item If $y_1,y_2$ are linearly independent, then 
        \[y_1+y_p\text{ and }y_2+y_p\]
        are linearly independent solutions to (\ref{eq:11.1}).
    \end{enumerate}
    3. follows since $ \mathcal{D}(y_1)=\mathcal{D}(y_2)=0 $ and $ \mathcal{D}(y_p)=f(x) $, so that $ \mathcal{D}(y_1+y_2)=f(x) $.
    \section{Linear independence of functions}
    \begin{definition}
        A set of functions are \textit{linearly dependent} if 
        \begin{equation}\label{eq:11.3}
            \sum_{i=1}^{N} c_if_i=0
        \end{equation}
        for a set of $N$ functions, $c_i$ are constants, and at least one $c_i\neq 0$.

        If $f_i$ are not linearly dependent, then they are \textit{linearly independent}.
    \end{definition}
    Equivalently, if any function can be written as a linear combination of the others, say
    \[
        f_1=\alpha_2 f_2+\cdots+\alpha_N f_N
    .\]
    Then the functions are linearly dependent.
    \section{Eigenfunctions for 2nd order ODEs}
    Recall that $ e^{\lambda x} $ is the eigenfunction of $ \frac{\mathrm{d}}{\mathrm{d}x}  $. In fact, it is the eigenfunction of $ \frac{\mathrm{d}^n}{\mathrm{d}x^n}  $, for $ \frac{\mathrm{d}^n}{\mathrm{d}x^n} (e^{\lambda x})=\lambda^{n}e^{\lambda x} $. Hence it is the eigenfunction of any linear differential operator $ \mathcal{D} $. Then equation (\ref{eq:11.2}) can be written as
    \[
        \underbrace{\left( a\frac{\mathrm{d}^2}{\mathrm{d}x^2}+b \frac{\mathrm{d}}{\mathrm{d}x}+c \right)}_{\mathcal{D}}y=0
    .\]
    Therefore, solutions to \ref{eq:11.2} take the form
    \[
        y_c=Ae^{\lambda x}
    .\]
    Plugging in we get 
    \begin{equation}\label{eq:11.3}
        a\lambda^2+b\lambda+c=0.
    \end{equation}
    This is the \textit{characteristic} or \textit{auxiliary} equation.

    From the fundamental theorem of algebra, we have 2(possibly repeat) roots in $ \mathbb{C} $. Let $ \lambda_1, \lambda_2 $ be roots. 

    \begin{enumerate}[\textbf{case \arabic*}]
        \item $ \lambda_1\neq \lambda_2$. Then
        \[
            y_1=Ae^{\lambda_1 x},\quad y_2=Be^{\lambda_2 x}
        .\]
        In this case $ y_1,y_2 $ are linearly independent and \textit{complete}(they form a basis of solution space). That is, any solution $f(x)$ can be written as 
        \[
            f(x)=c_1y_1(x)+c_2y_2(x)
        .\]
        Therefore, the general form of $f_c$ is 
        \[
            y_c(x)=Ae^{\lambda_1 x}+Be^{\lambda_2 x}
        .\]
        \item $ \lambda_1=\lambda_2 $: degenerated. Here $ y_1,y_2 $ are linearly dependent and not complete.
        \begin{example}
            Take a look at
            \[
                y''-4y'-4y=0
            .\]
            Try $ y_c=e^{\lambda x} $, we get $ (\lambda-2)^2=0 $. This is degenerated and we need a new tool:
        \end{example}
        \textbf{Detuning}: Consider a slightly modified(detuned) equation 
        \[
           y''-4y'+(4-\epsilon^2)y=0 
        \]
        for $ \epsilon\ll 1 $. Try $ y_c=e^{\lambda x} $:
        \[
            \lambda^2-4\lambda+(4-\epsilon^2)=0 \Longleftrightarrow \lambda=2\pm \epsilon
        .\]
        So
        \[
            y_c=Ae^{(2+\epsilon)x}+Be^{(2-\epsilon)x}=e^{2x}(Ae^{\epsilon x}+Be^{-\epsilon x})
        .\]
        Expand this in Taylor series:
        \[
            y_c=e^{2x}\left( (A+B)+\epsilon x(A-B)+O(\epsilon^2) \right)
        .\]
        Take limit $ \epsilon\to 0 $:
        \[
            \lim_{\epsilon \to 0} y_c \approx e^{2x}\left( (A+B)+\epsilon x(A-B) \right)
        .\]
        Apply initial conditions $ y_c(0)=C, y_c'(0)=D $, we get $ C=A+B, D=2(A+B)+\epsilon(A-B)=2C+\epsilon(A-B) $. Hence 
        \[
            A+B=O(1),\, A-B=O\left(\frac{1}{\epsilon}\right) \text{ as }\epsilon\to 0
        .\]
        Let $ \alpha=A+B=O(1), \beta=\epsilon(A-B)=O(\epsilon)O(1/\epsilon)=O(1) $, we have 
        \[
            \lim_{\epsilon \to 0} y_c=e^{2x}(\alpha+\beta x)
        .\]
        \textit{General rule}: If $y_1$ is a degenerate complementary function for linearly ODE with constant coefficients, then $ y_2 $ is a linearly independent complementary function.
    \end{enumerate}
    \section{Homogenoeous 2nd order linear ODEs with non-constant coefficients}
    \marginnote{Lecture 12}
    The general form is 
    \begin{equation}\label{eq:2nd non-const}
        \frac{\mathrm{d}^2y}{\mathrm{d}x^2}+p(x)\frac{\mathrm{d}y}{\mathrm{d}x} +q(x)y=0.
    \end{equation}
    We can assume that the coefficient of $y''$ is $1$ since we can divide the coefficient otherwise.
    \subsection{Reduction of order}
    Given one solution to \ref{eq:2nd non-const}, $y_1(x)$, our objective is to find a second solution $y_2(x)$. The idea is to look for a solution of form

    \begin{equation}\label{eq:12.2}
        y_2(x)=v(x)y_1(x).
    \end{equation}

    Note that $ y_2'=v'y_1+vy_1', y_2''=v''y_1+2v'y_1'+vy_1'' $. If $y_2$ is also a solution, then 
    \[
        y_2''+p(x)y_2'+q(x)y_2=0
    .\]
    Use this in \ref{eq:12.2} and collect terms:
    \[
        \begin{aligned}
             &v(y_1''+py_1'+qy_1)+v'(2y_1'+py_1)+v''y_1=0\\
             \Longleftrightarrow &v'(2y_1'+py_1)+v''y_1=0.\marginnote{Since $y_1$ is a solution.}
        \end{aligned}
    \]
    Let $u=v'$, then $u'y_1+u(2y_1'+py_1)=0$, which is first order and separable. Then we can solve for $u(x)$ and integrate for $v(x)$.
    \section{Solution space}
    An $n$th order linear ODE of the form
    \[p(x)y^{(n)}+q(x)y^{(n-1)}+\cdots+r(x)y=f(x)\]
    can be used to write $y^{(n)}(x)$ in terms of $ y,y',\dots, y^{(n-1)} $.
    \begin{example}[Damped oscillator]
        \[m \ddot{y}=-ky-L \dot{y}.\]
        The acceleration is described by the disposition and velocity. The state of the system can be described by an $n$-dimensional \textit{solution vector}
        \begin{equation}\label{eq:12.3}
            \mathbf{Y}(x)=\begin{pmatrix}
                y(x)\\y'(x)\\\vdots\\y^{(n-1)}(x)
            \end{pmatrix}.
        \end{equation}
    \end{example}
    \begin{example}
        Consider $ y''+4y=0 $. This is an example of \textit{undamped oscillator}. Two solutions are $ y_1=\cos 2x, y_2=\sin 2x $. $ y_1'=-2 \sin 2x, y_2= 2 \cos 2x $. Hence 
        \[
            \mathbf{Y}_1(x)=\begin{pmatrix}
                y_1\\y_1'
            \end{pmatrix}=\begin{pmatrix}
                \cos 2x\\-2 \sin 2x
            \end{pmatrix}, \mathbf{Y}_2(x)=\begin{pmatrix}
                \sin 2x\\2 \cos 2x
            \end{pmatrix}
        .\]
        The phase portrait is 
        \begin{center}
            \begin{tikzpicture}
              \draw [->] (-3, 0) -- (3, 0) node [right] {$y$};
              \draw [->] (0, -2.5) -- (0, 2.5) node [above] {$y'$};
        
              \draw (0, 0) circle [x radius = 1.2, y radius = 1.6];
              \draw [color=red,->] (0, 0) -- (1, 0.8844) node [right] {$\mathbf{Y}_2(x)$};
              \draw [color=blue,->] (0, 0) -- (0.6633, -1.3333) node [right] {$\mathbf{Y}_1(x)$};
              \draw [->] (0, 0) -- (0, 1.6) node [right] {$\mathbf{Y}_2(0)$};
              \draw [->] (0, 0) -- (1.2, 0) node [right] {$\mathbf{Y}_1(0)$};
            \end{tikzpicture}
          \end{center}
          \marginnote{Note that they do span the space, since any solution is a linear combination of $y_1$ and $y_2$ and the two free initial conditions are completely determined by $y$ and $y'$, as described in solutoin vectors.}
          Since $\mathbf{Y}_1,\mathbf{Y}_2$ are linearly independent for all $x$, any point in solution space $(y,y')$ can be reached by a linear combination of $y_1$ and $y_2$ .
    \end{example}

    Solutions $y_1,\dots,y_n$ are linearly independent if their solution vectors $ \mathbf{Y}_1,\dots,\mathbf{Y}_n $ are linearly independent.

    $n$ linearly independent solution vectors form a basis for solution space of an $n$th order ODE.
    \section{Wronskian}
    \subsection{Initial/Boundary conditions}
    Consider ICs for a 2nd order homogeneous ODE: 
    \[
        y(0)=a, y'(0)=b
    .\]
    If the general solution is 
    \[
        y(x)=Ay_1(x)+By_2(x)
    ,\]
    then we have the following linear system 
    \[
        \left\{ \begin{aligned}
            &Ay_1(0)+By_2(0)=a\\
            &Ay_1'(0)+By_2'(0)=b
        \end{aligned}\right.  
    \]
    or
    \[
        \underbrace{\begin{pmatrix}
            y_1(0)&y_2(0)\\
            y_1'(0)&y_2'(0)
        \end{pmatrix}}_{=M}
        \begin{pmatrix}
            A\\B
        \end{pmatrix}
        =
        \begin{pmatrix}
            a\\b
        \end{pmatrix}
    .\]
    It has a unique solution for $A,B$ if 
    \[
        |M|\neq 0
    .\] 
    \newpage
    \subsection{Wronsikian}
    \begin{definition}
        The \textit{Wronsikian} $ W(x) $ is the determinant of the \textit{fundamental} matrix, formed by placing solution vectors $ \mathbf{Y}_i $ in the $i$th column.
        \[
            W(x)=
            \begin{vmatrix}
                \uparrow &\uparrow &&\uparrow \\
                \mathbf{Y}_1&\mathbf{Y}_2&\cdots & \mathbf{Y}_n\\
                \downarrow & \downarrow & &  \downarrow
            \end{vmatrix}=
            \begin{vmatrix}y_{1}&y_{2}&\cdots &y_{n}\\y_{1}'&y_{2}'&\cdots &y_{n}'\\\vdots &\vdots &\ddots &\vdots \\y_{1}^{{(n-1)}}&y_{2}^{{(n-1)}}&\cdots &y_{n}^{{(n-1)}}\end{vmatrix}
        .\]
    \end{definition}
    For a second order ODE, we have 
    \begin{equation}\label{eq:12.4}
        W(x)=\begin{vmatrix}
            y_1&y_2\\
            y_1'&y_2'
        \end{vmatrix}=y_1y_2'-y_2y_1'.
    \end{equation}
    Solution vectors are linearly independent if $W(x)\neq 0$.

    \begin{example}
        Back to $ y''+4y=0 $. We have 
        \[
            W(x)=\begin{vmatrix}
                \cos 2x& \sin 2x\\
                -2 \sin 2x& 2 \cos 2x
            \end{vmatrix}=2 \cos^2 2x+ 2 \sin^2 2x=2
        .\]
        Hence $ \mathbf{Y}_1,\mathbf{Y}_2 $ are linearly independent $ \forall x $.
    \end{example}
    \begin{remark}
        Reverse implication: if $ \mathbf{Y}_1,  \mathbf{Y}_2 $ are linearly dependent, then $ W(x)=0 $.

        Suppose that $ y(x) $ is a linear combination of $y_1(x),y_2(x)$. Then $\mathbf{Y}, \mathbf{Y}_1, \mathbf{Y}_2 $ are linearly dependent. Hence 
        \[
            W(x)=\begin{vmatrix}
                y&y_1&y_2\\
                y'&y_1'&y_2'\\
                y''&y_1''&y_2''
            \end{vmatrix}=0
        .\]
        For $ y_1= \cos 2x, y_2=\sin 2x $,
        \[
            \begin{aligned}
                &\begin{vmatrix}
                    y&\cos 2x&\sin 2x\\
                    y'&-2 \sin 2x&2 \cos 2x\\
                    y''&-4 \cos 2x&-4 \sin 2x
                \end{vmatrix}=0\\
                \Longrightarrow & 8y+2y''=0\\
                \Longrightarrow & y''+4y=0.
            \end{aligned}
        \]
        This allows us to find the original equations given the solutions.
    \end{remark}
    \begin{remark}
        Note that $W(x)=0$ does \textit{not} necessarily imply linear dependence.
    \end{remark}
    \section{Abel's Theorem}\marginnote{Lecture 13.}
    \subsection{Theorem}
    Consider a second-order homogeneous ODE 
    \begin{equation}\label{eq:13.1}
        y''+p(x)y'+q(x)=0.
    \end{equation}
    \begin{theorem}[Abel]\label{thm:Abel}
        If $p(x),q(x)$ are continuous on an interval $I$, then the Wronsikian $W(x)$ is either $ W(x)=0 $ or $ W(x)\neq 0 \forall x\in I $.
    \end{theorem}
    Sketch of proof: Let $y_1,y_2$ be solutions to equation \ref{eq:13.1}:
    \[
        \begin{aligned}
             &y_1''+p(x)y_1'+q(x)y_1=0\\
             &y_2''+p(x)y_2'+q(x)y_2=0.
        \end{aligned}
    \]
    Multiply the first by $y_2,$ the second by $y_1$:
    \[
        \begin{aligned}
            &y_2(y_1''+p(x)y_1'+q(x)y_1)=0\\
            &y_1(y_2''+p(x)y_2'+q(x)y_2)=0.
       \end{aligned}
    \]
    Subtract the first by the second:
    \[
        \begin{aligned}
            &(y_2y_1''-y_1y_2'')+p(x)(y_2y_1'-y_1y_2')=0\\
            \Longrightarrow & \frac{\mathrm{d}W}{\mathrm{d}x}+p(x)W(x)=0. 
        \end{aligned}
    \]
    Therefore,
    \[
        \begin{aligned}
            &\int_{W(x_0)}^{W(x)} \frac{1}{W} \,\mathrm{d}W = \int_{x_0}^x -p(\tilde{x}) \,\mathrm{d}\tilde{x}\\
            \Longrightarrow  & \boxed{W(x)=W(x_0)e^{-\int_{x_0}^{x} p(u) \,\mathrm{d}u}}\marginnote{Called the \textit{Abel's identity}.}
        \end{aligned}
    \]
    Since $p(x)$ is continuous on $I$, it is bounded and thus integrable. Therefore, $ e^{-\int_{x_0}^{x} p(u) \,\mathrm{d}u}\neq 0 $. It follows that if $ W(x_0)=0 $ then $ W(x)=0, \forall x $. If $ W(x_0)\neq 0 $ then $ W(x)\neq 0, \forall x $.
    \begin{corollary}\label{col:abel}
            If $p(x)=0$, then $ W=W(x_0) $.
    \end{corollary}
    Note that we can find $ W(x) $ without solving the equation itself.
    \begin{example}[Bessel's equation]
        Consider
        \[
            x^2y''+xy'+(x^2-n^2)y=0
        .\]
        There is no closed form of solution, but we can still find the Wronskian of this equation:
        \[
            \begin{aligned}
                &y''+\frac{1}{x}y'+\left( 1-\frac{n^2}{x^2} \right) y=0\\
                \Longrightarrow &W(x)=W(x_0) e^{-\int_{x_0}^{x} \frac{1}{u} \,\mathrm{d}u}\\
                &=W(x_0) e^{-(\ln x-\ln x_0)}\\
                &= W_0 e^{-\ln x}=\frac{W_0}{x}.
            \end{aligned}
        \]
    \end{example}
    \subsection{Application of Abel's theorem}
    We can find the second solution using the first solution for second order ODEs.
    \[
        y_1y_2'-y_2y_1'=W(x_0)e^{-\int_{x_0}^{x} p(u) \,\mathrm{d}u}
    .\]
    This is a first order ODE for $y_2$.
    \subsection{Generalisation to higher order ODEs}
    Note that any linear $n$th order ODE can be written with 
    \[
        \mathbf{Y}'+\mathbf{A}(x)\mathbf{Y}=\mathbf{0}
    .\]
    This will be shown later.

    It can be shown that 
    \[
        W'+\trace(\bfA)W=0
    \]
    and that 
    \[
        W'=W_0e^{-\int^{x} \trace(\mathbf{A}) \,\mathrm{d}u}
    \]
    and Abel's theorem holds.\marginnote{This is a really rough discussion, to be discussed later and ES3 Q7.}
    \section{Equidimensional equations}
    \subsection{Definition}
    \begin{definition}[Equidimensional equations]
        An ODE is \textit{equidimensional} if the differential operator is unaffected by a multiplicative scaling.
    \end{definition}
    For example, rescale $ x \to x' $ with $ x'=\alpha x $.

    The general form of 2nd order equidimensional equations is 
    \begin{equation}\label{eq:13.8}
        ax^2y''+bxy'+cy=f(x).
    \end{equation}
    \begin{remark}
        Note that $ \frac{\mathrm{d}}{\mathrm{d}x'}=\frac{1}{\alpha}\frac{\mathrm{d}}{\mathrm{d}x},$ and $ \frac{\mathrm{d}^2}{\mathrm{d}x'^2}=\frac{1}{\alpha^2}\frac{\mathrm{d}^2}{\mathrm{d}x^2}   $.
    \end{remark}
    Hence by scaling to $x'$, we get 
    \[
        ax'^2 \frac{\mathrm{d}^2y}{\mathrm{d}x'^2} +bx' \frac{\mathrm{d}y}{\mathrm{d}x'}+cy=f(x'/\alpha) 
    .\]
    Notice that LHS does not change, so the equation is equidimensional.
    \subsection{Methods to solve}
    \begin{enumerate}
        \item Note that $ y=x^k $ is an eigenfunction of the operator $ x \frac{\mathrm{d}}{\mathrm{d}x}  $. To solve \ref{eq:13.8}, try $ y=x^k $:
        \[
            ak(k-1)+bk+c=0
        .\]
        If the roots $ k_1\neq k_2 $, then the solutions are of the form
        \[
            y_c=Ax^{k_1}+Bx^{k_2}
        .\]
        \item Note that $ z=\ln x $ turns \ref{eq:13.8} into an equation with constant coefficients.
        \[
            a \frac{\mathrm{d}^2y}{\mathrm{d}z^2}+(b-a)\frac{\mathrm{d}y}{\mathrm{d}z}+cy = f(e^z) \marginnote{Verify this.}
        .\]
        Try $ y=e^{\lambda z} $ for complementary function:
        \[
            a \lambda^2+(b-a)\lambda+c=0
        .\]
        Let $ \lambda_1,\lambda_2 $ be the solutions. If $ \lambda_1\neq \lambda_2 $ then $ y_c=Ae^{\lambda_1 z}+Be^{\lambda_2 z}=Ax^{\lambda_1}+Be^{\lambda_2}$. If $ \lambda_2=\lambda_2(k-1=k_2) $, then by detuning we have 
        \[
            y_c=(A+B\ln x)x^{k_1}
        .\]
    \end{enumerate}
    \section{Forced(Inhomogeneous) 2nd order ODEs}
    This section discusses methods of finding particular integral $ y_p(x) $.
    \subsection{Guess work}
    \begin{center}
        \begin{tabular}{cc}
        \toprule
        Form of $f(x)$ & Form of $y_p$ \\ \midrule
        $ e^{mx} $ & $ Ae^{mx} $ \\
        $ \sin kx, \cos kx $ & $ A \sin kx + B \cos kx $ \\
        $ P_n(x) $, $n$th degree polynomial & $ a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0 $ \\ \bottomrule
        \end{tabular}
        \end{center}
        Steps:
        \begin{enumerate}
            \item Insert guess into ODE.
            \item Equate coefficients of functions
            \item Solve for unknown coefficients.
        \end{enumerate}
    \subsection{Variation of parameters}
    Method for finding particular integral given complementary functions $ y_1,y_2 $. We assume that $ y_1,y_2 $ are linearly independent, and have solution vectors 
    \[
        \mathbf{Y}_1=\begin{pmatrix}
            y_1\\y_1'
        \end{pmatrix},
        \bfY_2=\begin{pmatrix}
            y_2\\y_2'
        \end{pmatrix}
    .\]
    Suppose the solution vector for $ y_p $ satisfies
    \begin{equation}\label{eq:14.1}
        \bfY_p=\begin{pmatrix}
            y_p\\y_p'
        \end{pmatrix}
        =u(x)\bfY_1+v(x)\bfY_2.
    \end{equation}
    Try to find 2 equations for $u(x),v(x)$.

    Note that
    \[
        \ref{eq:14.1}  \Rightarrow  
        \begin{aligned}
            y_p&=uy_1+vy_2,\qquad (\text{a})\\
            y_p'&=uy_1'+vy_2'.\qquad (\text{b})    
        \end{aligned}
    \]
    Differentiating (a) gives 
    \[
        y_p'=u'y_1+uy_1'+v'y_2+vy_2'\tag{c}
    .\]
    Subtracting by (b) gives 
    \[
        \boxed{u'y_1+v'y_2=0}\tag{d}
    .\]
    We also have 
    \[
        y_p''=uy_1''+u'y_1'+v'y_2'+vy_2''\tag{e}
    .\]
    If $ y_p(x) $ satisfies $ y_p''+p(x)y_p'+q(x)y_p=f(x) $, then 
    \[
        (\text{e})+p(x)(\text{b})+q(x)(\text{a})=f(x)
    .\]
    We also know 
    \[
        y_1''+py_1'+qy_2=0\land y_2''+py_2+qy_2=0,
    \]
    so after simplification
    \[
       \boxed{ u'y_1'+v'y_2'=f(x)}\tag{f}
    .\]

    (d), (f) gives 
    \[
        \underbrace{\begin{pmatrix}
            y_1&y_2\\
            y_1'&y_2'
        \end{pmatrix}}_{\text{fundamental matrix}}
        \begin{pmatrix}
            u'\\v'
        \end{pmatrix}
        =
        \begin{pmatrix}
            0\\f
        \end{pmatrix}
        \Longrightarrow 
        \begin{pmatrix}
            u'\\v'
        \end{pmatrix}=
        \frac{1}{W(x)}\begin{pmatrix}
            y_2&-y_2\\
            -y_1'&y_1
        \end{pmatrix}
        \begin{pmatrix}
            0\\f
        \end{pmatrix}
        =\begin{pmatrix}
            -y_2f/W\\ y_1f/W
        \end{pmatrix}
    .\]
    Hence, 
    \[
        \boxed{y_p = y_2 \int^{x} \frac{y_1(t)f(t)}{W(t)} \,\mathrm{d}t-y_1 \int^{x} \frac{y_2(t)f(t)}{W(t)} \,\mathrm{d}t}
    .\]
    \section{Forced oscillating systems}
    Many physical systems have a restoring force and damping, e.g. friction.
    \begin{example}[Car suspension]
        Consider a car wheel with a spring and a damper at the top. The ground pushes the wheel with a force $ F(t) $ and the height of the centre of the wheel is $y(t)$, with $y(0)=0$. Newton's 2nd law tells us that 
        \begin{equation}\label{eq:14.2}
            M\ddot{y} = F(t)-\underbrace{ky}_{\text{spring}}-\underbrace{L \dot{y}}_{\text{damper}}.
        \end{equation}
        The standard form is 
        \[
            \ddot{y}+\frac{L}{M}\dot{y}+\frac{k}{M}y=\frac{F(t)}{M}
        .\]
        Let $ \tau= \sqrt{k/M}t $, then 
        \[
            y''+2Ky'+y=f(\tau)
        ,\]
        where $ y'=\frac{\mathrm{d}y}{\mathrm{d}\tau}, K=\frac{L}{2\sqrt{kM}}, f=\frac{F}{k}  $.
    \end{example}
    \subsection{Unforced response(free/natural response)}
    If $f=0$, we have 
    \[
        y''+2Ky'+y=0 \Longrightarrow \lambda^2+2K \lambda+1=0 \Longrightarrow \lambda = -K\pm \sqrt{K^2-1}
    .\]
    \subsubsection*{case 1: $ K<1 $, underdamped}
    In this case, $ \lambda_1,\lambda_2\in \mathbb{C} $, and 
    \[
        y=e^{-K\tau}\left( A \sin \left( \sqrt{1-K^2}\tau \right)+B \cos \left( \sqrt{1-K^2}\tau \right) \right)
    .\]
    \begin{center}
        \begin{tikzpicture}
          \draw [->] (0, 0) -- (6, 0) node [right] {$t,\tau$};
          \draw [->] (0, -2) -- (0, 2) node [above] {$y$};
      
          \draw [semithick, mblue, domain = 0:5.8, samples=200] plot (\x, {1.8 * exp (-0.5 * \x) * sin (300 * \x + 40)});
          \draw[dashed, purple, domain = 0:5.8, samples=200, mylabel=at 2.0 above right with {$e^{-K\tau}$}] plot (\x, {1.8 * exp (-0.5 * \x) });
          \draw [dashed, purple, domain = 0:5.8, samples=200] plot (\x, {-1.8 * exp (-0.5 * \x) });
        \end{tikzpicture}
      \end{center}
      This is a damped oscillator of $2\pi/\sqrt{1-K^2}$. Note that period $ \to \infty $ as $ K \to 1 $.
      \subsubsection*{Case 2: critically damped}
      In this case $ \lambda_1=\lambda_2=-K $, which we can use detuning to obtain
      \[
          y=(A+B\tau)e^{-K\tau}
      .\]
      \begin{center}
        \begin{tikzpicture}
          \draw [->] (0, 0) -- (6, 0) node [right] {$t, \tau$};
          \draw [->] (0, -2) -- (0, 2) node [above] {$y$};
      
          \draw [semithick, mblue, domain = 0:5.8, samples=100] plot (\x, {(1 + 3 * \x) * exp (-\x)});
        \end{tikzpicture}
      \end{center}
      \subsubsection*{Case 3: $K>1$, overdamped}
      $ \lambda_1,\lambda_2\in \mathbb{R} $ and $ <0 $.
      We have 
      \[
          y=Ae^{\lambda_1 \tau}+Be^{\lambda_2 \tau}
      .\]
      Wlog let $ \left| \lambda_1 \right| <|\lambda_2| $,
      \begin{center}
        \begin{tikzpicture}
          \draw [->] (0, 0) -- (6, 0) node [right] {$t,\tau$};
          \draw [->] (0, -2) -- (0, 2) node [above] {$y$};
            
          \draw [semithick, mblue, domain = 0:5.8, samples=100] plot (\x, {2 * exp (-0.3 * \x) - 1.2*exp(-2 * \x)});

          \draw [dashed, red, domain = 0:5.8, samples=100] plot (\x, {2 * exp (-0.3 * \x)});
          \draw [dashed, green, domain = 0:5.8, samples=100] plot (\x, { - 1.2*exp(-2 * \x)});
        \end{tikzpicture}
      \end{center}
      \begin{remark}
          Unforced response decays in all cases.
      \end{remark}
      \section{Damped oscillating systems: Forced response}\marginnote{Lecture 15}
      \subsection{Sinvsoided forcing}
      Consider 
      \begin{equation}\label{eq:15.1}
        \ddot{y}+\mu \dot{y}+ \omega_0^2y = \sin \omega t.
      \end{equation}
      Guess $ y_p=A \sin \omega t+B \cos \omega t $, and equate all coefficients of $ \sin \omega t $ and $ \cos \omega t $:
      \begin{IEEEeqnarray}{RrCl}
         \sin \omega t: & -A \omega^2-B \mu \omega+\omega_0^2 A & = & 1
      \\
        \cos \omega t: & -B \omega^2+A \mu \omega+ \omega_o^2 B & = &0 
      \end{IEEEeqnarray}
      Note that
      \begin{IEEEeqnarray*}{Rl}
          (27.3) \Longrightarrow & A = B\frac{\omega^2-\omega_0^2}{\mu \omega}
      \\
        (27.2) \Longrightarrow & A(\omega_0^2-\omega^2) = 1+B \mu \omega
        \\
        \Longrightarrow &A= \frac{\omega_0^2-\omega^2}{(\omega_0^2-\omega^2)^2+\mu^2 \omega^2},
        \\
        &B=\frac{-\mu \omega}{(\omega_0^2-\omega^2)^2+\mu^2 \omega^2}.
      \end{IEEEeqnarray*}
      Therefore, 
      \[
          y_p=\frac{1}{(\omega_0^2-\omega^2)^2+\mu^2 \omega^2}\left[ (\omega_0^2-\omega^2)\sin \omega t-\mu \omega \cos \omega t \right]
      .\]
      For underdamped $y_c$, the solution will tend to $y_p$ as $t$ grows.
      \begin{remark}[for damped systems]
          \begin{itemize}
              \item Complementary functions give transient(short time) response to intitial conditions.
              \item Particular integral gives long time response to forcing.
          \end{itemize}
      \end{remark}
      When $ \omega=\omega_0 $ and $ \mu\neq 0 $(damped), then 
      \[
          \lim_{\omega \to \omega_0}y_0 = \frac{-\cos \omega t}{\mu \omega} 
      ,\]
      which is a finite-amplitude oscillation. Note that the amplitude increases with decreasing $\mu$.
      \subsection{Resonance in undamped systems}
      A general form is
      \begin{equation}
        \ddot{y}+\omega_0^2 y = \sin \omega_0 t.
      \end{equation}
      We use detuning to solve this. Consider 
      \begin{equation}
        \ddot{y}+\omega_0^2 y = \sin \omega t, \omega\neq \omega_0.
      \end{equation}
      Guess $ y_p=C \sin \omega t $. We get 
      \[
          C(-\omega^2+\omega_0^2)=1 \Longrightarrow y_p = \frac{1}{\omega_0^2-\omega^2}\sin \omega t
      .\]
      Since LHS of (27.4) is linear, $ y=y_p+Ay_c $ is also a solution. Consider 
      \[
          y_p = \frac{1}{\omega_0^2-\omega^2}\sin \omega t + A \sin \omega_0 t
      .\]
      Pick $A=-\frac{1}{\omega_0^2-\omega^2}$, then 
      \[
          y_p = \frac{\sin \omega t-\sin \omega_0 t}{\omega_0^2-\omega^2} = \frac{2}{\omega_0^2-\omega^2}\left[ \cos \left( \frac{\omega+\omega_0}{2}t \right)\sin \left( \frac{\omega-\omega_0}{2}t \right) \right]
      .\] 
      Write $ \omega_0-\omega = \delta \omega $, we get 
      \[
          y_p = \frac{-2}{\delta \omega(\omega_0+\omega)}\left[ \cos \left( \omega_0-\frac{\delta \omega}{2}t \right) \sin \frac{\delta \omega}{2}t \right]
      .\]
      \begin{center}
        \begin{tikzpicture}
          \draw [->] (0, 0) -- (6.5, 0) node [right] {$t$};
          \draw [->] (0, -2) -- (0, 2) node [above] {$y_p$};
      
          \draw [semithick, mblue, domain=0:6,samples=600] plot(\x, {2 * cos(2000 * \x) * sin (90 * \x)});
          \draw [semithick, morange] (0, 0) sin (1, 2) cos (2, 0) sin (3, -2) cos (4, 0) sin (5, 2) cos (6, 0);
          \draw [semithick, mgreen] (0, 0) sin (1, -2) cos (2, 0) sin (3, 2) cos (4, 0) sin (5, -2) cos (6, 0);
      
          \draw [semithick, mblue] (7.5, 0.7) -- (8, 0.7) node [black, right] {$y_p$};
          \draw [semithick, morange] (7.5, 0) -- (8, 0) node [black, right] {$\sin\left(\frac{\delta \omega}{2} t\right)$};
          \draw [semithick, mgreen] (7.5, -0.7) -- (8, -0.7) node [black, right] {$-\sin\left(\frac{\delta \omega}{2} t\right)$};
      
          \draw [<->] (1, -2.5) -- (5, -2.5) node [pos=0.5,fill=white] {$O\left(\frac{1}{\delta \omega}\right)$};
        \end{tikzpicture}
      \end{center}
      Let $ \delta \omega\to 0 $, we get 
      \[
          \lim_{\delta \omega \to 0} \sin \left( \frac{\delta \omega}{2}t \right) \approx \frac{\delta \omega}{2}t
      ,\]
      and thus 
      \[
          \lim_{\delta \omega \to 0} y_p = -\frac{-t}{2\omega_0}\cos \omega_0 t
      .\]
      This represents a linear growth in amplitude. Note that $y_p$ takes form of complementary functions times an independent variable.
      \subsection{Impulses and point forces}
      Consider a system that experiences a sudden force. e.g., Consider a car driving over a speedbump. Suppose at time $T$ the car is above the bump, and at $T-\epsilon, T+\epsilon$ the car drives on and off the bump respectively. Consider $ \epsilon\to 0 $: force becomes a sudden impulse.
      \begin{center}
        \begin{tikzpicture}
          \draw [->] (-3, 0) -- (3.25, 0) node [right] {$t$};
          \draw [->, use as bounding box] (0, 0) node [below] {$T$} -- (0, 2.3) node [above] {force};
          \draw [semithick, morange, domain=-2.5:2.5, samples = 100] plot (\x, { 1.6 * exp( - 4 * \x * \x)});
        \end{tikzpicture}
      \end{center}
      Take $y$ as the disposition wrt $t$, the equation is 
      \begin{equation}
          M \ddot{y} = F(t)-ky-L \dot{y}
      \end{equation}
      Integrate this from $T-\epsilon$ to $T+\epsilon$:
      \[
          \begin{aligned}
            &\lim_{\epsilon \to 0} \int_{T-\epsilon}^{T+\epsilon} M \ddot{y} \,\mathrm{d}t = \lim_{\epsilon \to 0} \int_{T-\epsilon}^{T+\epsilon} F(t)-ky-L \dot{y} \,\mathrm{d}t\\
            \Longleftrightarrow & \lim_{\epsilon \to 0} M\left[ \dot{y} \right]_{T-\epsilon}^{T+\epsilon} = \lim_{\epsilon \to 0} \left\{ \int_{T-\epsilon}^{T+\epsilon} F(t) \,\mathrm{d}t - k \int_{T-\epsilon}^{T+\epsilon} y \,\mathrm{d}t - L\left[ y \right]_{T-\epsilon}^{T+\epsilon}\right\}\\
            \Longrightarrow & \lim_{\epsilon \to 0} M\left[ \dot{y} \right]_{T-\epsilon}^{T+\epsilon} = \lim_{\epsilon \to 0} \int_{T-\epsilon}^{T+\epsilon} F(t) \,\mathrm{d}t,
          \end{aligned}
      \]
      given that $y$ is finite and continuous.

      Define \textit{impulse} $I$ as 
      \[
          I =\lim_{\epsilon \to 0} \int_{T-\epsilon}^{T+\epsilon} F(t) \,\mathrm{d}t
      ,\]
      hence it becomes 
      \[
          I = \lim_{\epsilon \to 0} M\left[ \dot{y} \right]_{T-\epsilon}^{T+\epsilon}
      .\]
      Velocity $ \dot{y} $ experiences a sudden change(discontinuous) which depends on integral of force $ F(t) $.
\end{document}