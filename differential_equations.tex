\documentclass[10pt]{article}
\pdfoutput=1 
\usepackage{NotesTeX,lipsum}
\usepackage{IEEEtrantools}
\usepackage{mhchem}

\def\d{{\mathrm d}}
\def\e{{\mathrm e}}
\def\g{{\mathrm g}}
\def\h{{\mathrm h}}
\def\f{{\mathrm f}}
\def\p{{\mathrm p}}
\def\s{{\mathrm s}}
\def\t{{\mathrm t}}
\def\i{{\mathrm i}}

\def\A{{\mathrm A}}
\def\B{{\mathrm B}}
\def\E{{\mathrm E}}
\def\F{{\mathrm F}}
\def\G{{\mathrm G}}
\def\H{{\mathrm H}}
\def\P{{\mathrm P}}


\def\bb{\mathbf b}
\def \bc{\mathbf c}
\def\bx {\mathbf x}
\def\bn {\mathbf n}
\def\le{\leqslant}
\def\ge{\geqslant}
\def\arcosh{{\rm arcosh}\,}

\newcommand{\bluecomment}[1]{{\color{blue}#1}}
%\renewcommand{\comment}[1]{}
\newcommand{\redcomment}[1]{{\color{red}#1}}
\newcommand{\tm}{\times}
%\usepackage{showframe}

\title{\begin{center}{\Huge \textit{Differential Equations Notes}}\\{{\itshape Based on Lectures and "An Introduction to ODEs"}}\end{center}}
\author{$\theta\omega\theta$}
\affiliation{
Not in University of Cambridge\\
skipped some talks irrelevant to contents\\
}

\emailAdd{not telling you}

\begin{document}
	\maketitle
	\flushbottom
	\newpage
    \pagestyle{fancynotes}
    %main doc
    \part{Basic Calculus}
    \section{Differentiation}
    \subsection{Definitions and methods}
	\begin{definition}[Derivative]
        The derivative of a function $f(x)$ wrt its argument $x$ is the function
        \[
            \frac{\mathrm{d}f}{\mathrm{d}x} = \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} 
        .\]
        We define higher derivatives recursively by 
        \[
            \frac{\mathrm{d}^nf}{\mathrm{d}x^n} = \frac{\mathrm{d}}{\mathrm{d}x}\left( \frac{\mathrm{d}^{n-1}f}{\mathrm{d}x^{n-1}}  \right)   
        .\]
    \end{definition}
    For the derivative to exist, we need
    \[
        \lim_{h \to 0-} \frac{f(x+h)-f(x)}{h} = \lim_{h \to 0+} \frac{f(x+h)-f(x)}{h} 
    .\]
    
    Rules for differentiation:
    \begin{enumerate}
        \item \textbf{Chain rule}: $ (f(g(x)))' = f'(g(x))g'(x) $.
        \item \textbf{Product rule}: $ (u\cdot v)' = u\cdot v'+u'\cdot v $.
        \item \textbf{Leibniz's rule}: generalisation of product rule.\sidenote{There are multiple ways to prove, e.g. by induction.}
        \[
            \frac{\mathrm{d}^n}{\mathrm{d}x^n}(u\cdot v) = \sum_{k=0}^{n}\binom{n}{k}u^{(k)}v^{(n-k)}
        .\]
    \end{enumerate}
    \subsection{Order of magnitude}
    The goal is to compare the sizes of functions, in the vicinity of specific points.
    \begin{definition}[Little and Big o]
        We say $ f(x) = o(g(x)) $ as $x\to x_0$ if $ \lim_{x \to x_0} \frac{f(x)}{g(x)} = 0 $.

        We say $ f(x) = O(g(x)) $ as $x\to x_0$ if $ \exists M, \delta>0, \left| x-x_0 \right| <\delta \Rightarrow \left| f(x) \right| \le M \left| g(x) \right| . $ The infinite case is defined similarly.
    \end{definition}

    To find the tangent line to $f$ at $x_0$, note that 
    \[
        \begin{aligned}
             & \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x=x_0} = \frac{f(x_0+h)-f(x_0)}{h}+ \frac{o(h)}{h} & \text{when $ h \to 0$} \\
             \Longrightarrow & f(x_0+h) = f(x_0)+\frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x=x_0} h + o(h)& \text{when $ h \to 0$} \\
        \end{aligned}
    \]
    \subsection{Taylor's Theorem and L'Hopital's Theorem}
    We want to approximate a function $f(x)$ with a polynomial of order $n$:
    \[
        f(x) = \underbrace{a_0+a_1x+\cdots+a_nx^n}_{P_n(x)}
    .\]
    Differentiating recursively we get 
    \begin{equation}\label{eq:taylor_series}
        P_n(x) = f(x_0)+(x-x_0)f'(x_0)+\cdots+\frac{(x-x_0)^n}{n!}f^{(n)}(x_0).
    \end{equation}
    Alternatively, we can write $ f(x) = P_n(x)+E_n $, where $E_n$ is called the \textit{remainder/error}.

    By generalisation of $ f(x+h) = f(x) + hf'(x)+o(h), h\to 0 $, we get 
    \begin{equation}\label{eq:taylor_series_with_remainder}
        f(x+h) = f(x)+hf'(x)+\frac{h^2}{2}f'(x)+\cdots+\frac{h^n}{n!}f^{(n)}(x)+o(h^n).
    \end{equation}

    By refining the range of $o(h^n)$ we get
    \begin{theorem}[Taylor]\label{thm:taylor_theorem}
        If the first $n+1$ derivatives of $f(x)$ exist, then 
        \[
            f(x+h) = f(x)+hf'(x)+\frac{h^2}{2}f'(x)+\cdots+\frac{h^n}{n!}f^{(n)}(x)+O(h^{n+1}).
        .\]
    \end{theorem}

    Using this we can prove 
    \begin{theorem}[L'Hopital]\label{thm:L'Hopital}
        Let $f$ and $g$ be differentiable at $x=x_0$ and
        \[
            \lim_{x \to x_0} f(x)=f(x_0)=0, \quad \lim_{x \to x_0} g(x)=g(x_0)=0
        .\]
    \end{theorem}
    \begin{proof}(Not rigorous)
        As $x\to x_0$, 
        \[
            \begin{aligned}
                 \frac{f(x)}{g(x)} &= \frac{f(x_0)+(x-x_0)f'(x_0)+o(x-x_0)}{g(x_0)+(x-x_0)g'(x_0)+o(x-x_0)}\\
                 &= \frac{(x-x_0)f'(x_0)+o(x-x_0)}{(x-x_0)g'(x_0)+o(x-x_0)}\\
                 &\to \frac{f'(x_0)}{g'(x_0)}.
            \end{aligned}
        \]
    \end{proof}
    Note that it can be applied recursively.
    \section{Integration}
    \subsection{Definition}
    All functions mentions are assumed to be well-hehaved.

    We evaluate the area under the curve of $f(x)$ by considering
    \[
        \sum_{n=0}^{N-1}f(x_n)\Delta x
    \]
    where $ \Delta x = \frac{b-a}{N} $ and $ x_n = a+n\Delta x. $
    \begin{theorem}[MVT]\label{thm:mean_value_theorem_for_integral}
        For a continuous function $f(x)$:
        \[
            \int_{x_n}^{x_{n+1}} f(x) \,\mathrm{d}x = f(x_c)(x_{n+1}-x_n) \quad \text{for some } x_c\in (x_n,x_{n+1})
        .\]
    \end{theorem}
    Estimate $ f(x_c) $ as follows:
    \[
        f(x_c) = f(x_n)+O(x_c-x_n) = f(x_n)+O(x_{n+1}-x_n)
    .\]\
    Hence
    \[
        \begin{aligned}
            \int_{x_n}^{x_{n+1}} f(x) \,\mathrm{d}x &= f(x_c)(x_{n+1}-x_n)\\
            &= [f(x_n)+O(x_{n+1}-x_n)](x_{n+1}-x_n)\\
            &= \Delta x f(x_n)+O(\Delta x^2).
        \end{aligned}
    \]
    Therefore the error $ \epsilon = O(\Delta x^2) $. It follows that
    \[
        \int_{a}^{b} f(x) \,\mathrm{d}x = \lim_{\Delta x \to 0} \left\{ \left[ \sum_{n=0}^{N-1}f(x_n)\Delta x \right] + O(N\Delta x^2)\right\}
    .\]
    Hence 
    \begin{definition}[Definite integral]
        $\displaystyle \int_{a}^{b} f(x) \,\mathrm{d}x = \lim_{N \to \infty} \sum_{n=0}^{N-1}f(x_n)\Delta x$ 
    \end{definition}
    \subsection{Fundamental Theorem of Calculus}
    \begin{theorem}[FTC]\label{thm:ftc}
        Let
        \[
            F(x) = \int_{a}^{x} f(t) \,\mathrm{d}t
        ,\]
        then
        \[
            \frac{\mathrm{d}F}{\mathrm{d}x} = f(x) 
        .\]
    \end{theorem}
    \begin{proof}
        From the definition of derivative:
        \[
            \begin{aligned}
                 \frac{\mathrm{d}F}{\mathrm{d}x} &= \lim_{h \to 0} \frac{1}{h} \left\{ \int_{a}^{x+h} f(t) \,\mathrm{d}t - \int_{a}^{x} f(t) \,\mathrm{d}t\right\}\\
                 &= \lim_{h \to 0} \frac{1}{h} \int_{x}^{x+h} f(t) \,\mathrm{d}t\\
                 &= \lim_{h \to 0} \frac{1}{h}\left( f(x)h+O(h^2) \right)\\
                 &= f(x).
            \end{aligned}
        \]
    \end{proof}
    \begin{corollary}\label{col:ftc}
            \[
                \frac{\mathrm{d}}{\mathrm{d}x}\int_{x}^{b} -f(t) \,\mathrm{d}t 
            .\]
            \[
                \frac{\mathrm{d}}{\mathrm{d}x} \int_{a}^{g(x)} f(t) \,\mathrm{d}t = \frac{\mathrm{d}}{\mathrm{d}x}F(g(x)) = \frac{\mathrm{d}F}{\mathrm{d}g} \frac{\mathrm{d}g}{\mathrm{d}x} = f(g(x))\frac{\mathrm{d}g}{\mathrm{d}x}   
            .\]
    \end{corollary}
    \begin{definition}[Indefinite integral]
        \[
            \int f(x) \,\mathrm{d}x = \int_{x_0}^{x} f(t) \,\mathrm{d}t
        .\]
    \end{definition}
    \subsection{Techniques of Integration}
    skipped
    \section{Introduction to multivariable functions}\marginnote{Lecture 5.}
        \subsection{Partial derivative}
        \begin{definition}
            The \textit{partial derivative} of $ f(x,y) $ wrt $x$ is 
            \begin{equation}
                \frac{\partial f}{\partial x}\Big|_y = \lim_{\delta x \to 0} \frac{f(x+\delta x,y)-f(x,y)}{\delta x}.
            \end{equation}
            Similarly 
            \[
                \frac{\partial f}{\partial y}\Big|_x = \lim_{\delta y \to 0} \frac{f(x,y+\delta y)-f(x,y)}{\delta y}.
            \]
            We can take them in any order to form \textit{cross derivatives}.
        \end{definition}
        Note that 
        \begin{equation}
            \frac{\partial^2 f}{\partial x \partial y} = \frac{\partial }{\partial x}\left( \frac{\partial f}{\partial y}  \right)=\frac{\partial }{\partial y}\left( \frac{\partial f}{\partial x}  \right) .
        \end{equation}
        \subsection{Multivariable chain rule}
        \begin{theorem}\label{thm:mvchainrule}
            For well-behaved functions, we have 
            \begin{equation}
                \d f=\frac{\partial f}{\partial x} \d x+\frac{\partial f}{\partial y} \d y
            \end{equation}
        \end{theorem}
        \begin{proof}
            Note that 
            \[
                \begin{aligned}
                     \delta f &= f(x+\delta x, y+\delta y)-f(x+\delta x, y)+f(x+\delta x, y)-f(x,y)\\
                     &= f(x+\delta x,y)+\delta y \frac{\partial f}{\partial y}(x+\delta x, y)+o(\delta y)-f(x+\delta x,y)\\
                     &+ f(x,y)+\delta x \frac{\partial f}{\partial x}(x,y)+o(\delta x)-f(x,y) \\
                     &= \delta y \frac{\partial f}{\partial y}(x+\delta x, y)+\delta x \frac{\partial f}{\partial x}(x,y)+o(\delta x)+o(\delta y)\\
                     &= \delta y \left( \frac{\partial f}{\partial y}(x,y)+\delta x \frac{\partial }{\partial x}\left( \frac{\partial f}{\partial y}(x,y)  \right)+o(\delta x)   \right)+\delta x \frac{\partial f}{\partial x}(x,y)+o(\delta x)+o(\delta y)\\
                     &= \delta y \frac{\partial f}{\partial y}(x,y)+\delta x \frac{\partial f}{\partial x}(x,y)+\delta x \delta y \frac{\partial }{\partial x}\left( \frac{\partial f}{\partial y}(x,y)  \right)+o(\delta x)+o(\delta y)+o(\delta x \delta y).
                \end{aligned}
            \]
            Taking limit gives the result.
        \end{proof}
        \begin{remark}
            For $ f(x(t),y(t)) $, we have
            \begin{equation}
                \frac{\d f}{\d t}=\lim _{\delta x, \delta y, \delta t\to 0}\left[\frac{\partial f}{\partial x} \frac{\d x}{\d t}+\frac{\partial f}{\partial y} \frac{\d y}{\d t}\right] = \frac{\partial f}{\partial x} \frac{\d x}{\d t}+\frac{\partial f}{\partial y} \frac{\d y}{\d t}.
            \end{equation}
            And integral form:
            \begin{equation}
                \int \d f=\int \frac{\partial f}{\partial x} \d x+\int \frac{\partial f}{\partial y} \d y
            \end{equation}
            In this case we need to specify the \textit{path} of integral as there might be some priority issues.
        \end{remark}
        \subsection{Applications of multivariable chain rule}
        \subsubsection{Change of variables}
        It is often useful to write a DE in a different coordinate system before solving it. Need to transform the derivatives into the new coordinate system. 
        \begin{example}
            Change from cartesian coordinates to polar coordinates: $ x=r\cos \theta, y=r\sin \theta $. Firstly, write
            \[
                f=f(x(r,\theta),y(r,\theta))
            .\]
            We have 
            \[
                \frac{\partial f}{\partial r} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial r}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial r}=\frac{\partial f}{\partial x}\cos \theta+\frac{\partial f}{\partial y}\sin \theta
                \marginnote{By regarding $ \frac{\partial f}{\partial r}$ as $\frac{\mathrm{d}f}{\mathrm{d}r} $ with $\theta$ fixed, we get this result.}     
            .\]
            Similar for other partial derivatives.
        \end{example}
        \subsubsection{Implicit Differentiation}
        Consider $ f(x,y,z)=c, c\in \mathbb{R} $. $f$ describes a surface in 3d space. $ f(x,y,z)=c $ implicitly defines $ x(y,z),y(x,z),z(x,y) $. However, we can find $ \frac{\partial z}{\partial x}  $ here using implicit differentiation.

        Consider $ f(x,y,z(x,y))=c $.
        \[
            \d f = \frac{\partial f}{\partial x}\d x+ \frac{\partial f}{\partial y}\d y+ \frac{\partial f}{\partial z}\d z   
        .\]
        Finding the partial derivative for $x$: 
        \[
            \begin{aligned}
                && \frac{\partial f}{\partial x}\Big|_y &= \frac{\partial f}{\partial x}\Big|_{yz}\frac{\partial x}{\partial x}\Big|_y+\frac{\partial f}{\partial y}\Big|_{xz}\frac{\partial y}{\partial x}\Big|_y+\frac{\partial f}{\partial z}\Big|_{xy}\frac{\partial z}{\partial x}\Big|_y\marginnote{Notice the subscripts are very important since they discribes different functions}\\
                && &= \frac{\partial f}{\partial x}\Big|_{yz}+\frac{\partial f}{\partial y}\Big|_{xz}\frac{\partial y}{\partial x}\Big|_y+\frac{\partial f}{\partial z}\Big|_{xy}\frac{\partial z}{\partial x}\Big|_y.\\
                &\Longleftrightarrow &\frac{\partial f}{\partial x}\Big|_y&=\frac{\partial f}{\partial x}\Big|_{yz}+\frac{\partial f}{\partial z}\Big|_{xy}\frac{\partial z}{\partial x}\Big|_y  \marginnote{Since $ \frac{\partial y}{\partial x}\Big|_y=0  $}  \\    
                &\Longleftrightarrow &0 &= \frac{\partial f}{\partial x}\Big|_{yz}+\frac{\partial f}{\partial z}\Big|_{xy}\frac{\partial z}{\partial x}\Big|_y \marginnote{Since $f=c$ along the surface $z(x,y)$.} \\
                &\Longleftrightarrow && \boxed{\frac{\partial z}{\partial x}\Big|_y=-\frac{\partial f/ \partial x|_{yz}}{\partial f/\partial z|_{xy}} }    
            \end{aligned}
        \]
        Note that $ \frac{\partial f}{\partial x}\Big|_{yz} \neq 0 $ in general.
        \begin{remark}
            Reciprocal rule still holds as long as the same variable(s) are held fixed. e.g.
            \[
                \frac{\partial r}{\partial x}\Big|_y = \frac{1}{\frac{\partial x}{\partial r}\Big|_y }\quad\text{but}\quad  \frac{\partial r}{\partial x}\Big|_y \neq \frac{1}{\frac{\partial x}{\partial r}\Big|_{\theta} }
            .\]
        \end{remark}
        \subsubsection{Differentiation of an integral wrt its parameters}
        Consider a family of functions $ f(x;\alpha) $, where $ \alpha $ is the parameter. Define
        \[
            I(\alpha) = \int_{a(\alpha)}^{b(\alpha)} f(x;\alpha) \,\mathrm{d}x
        .\]
        \begin{IEEEeqnarray*}{rCl}
            \frac{\mathrm{d}I}{\mathrm{d}\alpha}  & = & \lim_{\delta \alpha \to 0} \frac{I(\alpha+\delta \alpha)-I(\alpha)}{\delta \alpha}\marginnote{Draw a graph to understand the steps.}
        \\
            & = &\lim_{\delta \alpha \to 0} \frac{1}{\delta \alpha}\left[ \int_{a(\alpha+\delta \alpha)}^{b(\alpha+\delta \alpha)} f(x;\alpha+\delta \alpha) \,\mathrm{d}x-\int_{a(\alpha)}^{b(\alpha)} f(x;\alpha) \,\mathrm{d}x \right]
        \\
            & = &\lim_{\delta \alpha \to 0} \frac{1}{\delta \alpha}\left[ \int_{a(\alpha)}^{b(\alpha)}f(x;\alpha+\delta \alpha)- f(x;\alpha) \,\mathrm{d}x -\int_{a(\alpha)}^{a(\alpha+\delta \alpha)} f(x;\alpha+\delta \alpha) \,\mathrm{d}x+\int_{b(\alpha)}^{b(\alpha+\delta \alpha)} f(x;\alpha+\delta \alpha) \,\mathrm{d}x \right]
        \\ 
            & = &\int_{a(\alpha)}^{b(\alpha)}\frac{\partial f}{\partial \alpha}  \,\mathrm{d}x - f(a;\alpha) \lim_{\delta \alpha \to 0} \frac{a(\alpha+\delta \alpha)-a(\alpha)}{\delta \alpha}+ f(b;\alpha)\lim_{\delta \alpha \to 0}\frac{b(\alpha+\delta \alpha)-b(\alpha)}{\delta \alpha}.\marginnote{When $ \delta \alpha $ is very small, we can approximate the latter two integrals with the area of the rectangle of hight $ f(a;\alpha) $ and width $ a(\alpha+\delta \alpha)-a(\alpha) $.}
        \end{IEEEeqnarray*}
        Hence,
        \[
            \boxed{\frac{\mathrm{d}I}{\mathrm{d}\alpha} = \frac{\mathrm{d}}{\mathrm{d}\alpha}\int_{a(\alpha)}^{b(\alpha)} f(x;\alpha) \,\mathrm{d}x =  \int_{a(\alpha)}^{b(\alpha)}\frac{\partial f}{\partial \alpha}  \,\mathrm{d}x+f(b;\alpha)\frac{\mathrm{d}b}{\mathrm{d}\alpha}-f(a;\alpha) \frac{\mathrm{d}a}{\mathrm{d}\alpha} }
        .\]

    \part{First order linear ODEs}
    \section{Terminology}
    \begin{definition}
        An \textit{ordinary differential equation} is a differential equation involving a function of one variable. A \textit{partial differential equation} is (a) differential equation(s) involving a function of more than one variable.

        \textit{n}th order DE: the highest order of derivative is $n$.

        Linear: dependent variable appears linearly.
    \end{definition}
    \section{Prelude: Exponential functions}
    Consider $f=a^x, a>0$, we have
    \[
        \begin{aligned}
             \frac{\mathrm{d}f}{\mathrm{d}x} &= \lim_{h \to 0} \frac{a^{x+h}-a^x}{h}=a^x \lim_{h \to 0} \frac{a^h-1}{h}\\
            &= \lambda a^x.
        \end{aligned}
    \]
    Hence
    \begin{definition}
        Define $ \exp(x)=e^x $ as the solution to the DE 
        \[
            \frac{\mathrm{d}f}{\mathrm{d}x}=f(x), \quad f(0)=1 
        .\]
        Therefore $e$ is the value of $a$ such that $\lambda=1$. i.e.,
        \[
            \lim_{h \to 0} \frac{e^h-1}{h}=1
        .\]
    \end{definition}
    Define $ \ln (x) $ as the inverse of $e^x$ such that $ e^{\ln(x)}=x $.

    Consider $ a^x=e^{\ln(a)x}$, so 
    \[
        \frac{\mathrm{d}f}{\mathrm{d}x}=(\ln a)a^x, \lambda=\ln a 
    .\]

    The exponential function is the \textit{eigenfunction} of the differential operator.

    The \textit{eigenfunction} of an operator is unchanged by the action of the operator, except for a multiplicative scaling by the eigenvalue.
    \section{Rules for linear ODEs}
    \begin{enumerate}[\bfseries 1.]
        \item Any linear homogeneous ODE with constant coefficients has solutions of form $ e^{\lambda x} $, the eigenfunction. By \textit{homogeneous} we mean that all terms involve the dependent variable or its derivatives.

        This means that $y=0$ is a trivial solution for all homogeneous ODEs.

        Constant coefficients imply that the independent variable does not appear explicitly in DE.
        \item For linear homogeneous ODEs, any constant multiple of a solution is also a solution.
        \item An $n$th order ODE has $n$ independent solutions.

        For constant coefficient ODEs, this rule follows from the fundamental theorem of algebra.
        \item An $n$th order ODE requires $n$ initial/boundary conditions.
    \end{enumerate}
    \section{Inhomogeneous(forced) first order ODEs with constant coefficients}
    \subsection{Constant forcing}
    \begin{example}
        Consider the equation 
        \[
            5y'-3y=10
        .\]
        Solution steps:
        \begin{enumerate}
            \item Write the general solution $ y=y_p+y_c $ where $y_p$ is a \textit{particular integral} and $y_c$ is a complementary function
            \item Find $ y_p $ by simply setting $y'=0$. In this case, $y=-10/3$.
            \item Insert general solution into DE:
            \[
              \begin{aligned}
                   &5(y_p+y_c)'-3(y_p+y_c)&=10\\
                   \Longleftrightarrow & 5y_c'+10-3y_c&=10\\
                    \Longleftrightarrow & 5y_c-3y_c'=0.
              \end{aligned}  
            \]
            Note that $ y_c $ is a solution to corresponding homogeneous equation.
            \item Solve for $y_c$. In this case, $y_c=Ae^{3x/5}$.
            \item Combine $y_p$ and $y_c$.
        \end{enumerate}
    \end{example}
    \subsection{Eigenfunction forcing}
    Example problem: In a sample of rock, isotope A decays to isotope B at a rate proportional to $a$, the number of nuclei of A. B decays to C at a rate proportional to $b$, the number of nuclei of $B$. Find $b(t)$.
    
    We have 
    \[
        \begin{aligned}
            &\frac{\mathrm{d}a}{\mathrm{d}t} = -k_a a \Longrightarrow  a = a_0 e^{-k_a t}\\
            &\frac{\mathrm{d}b}{\mathrm{d}t} = k_a a -k_b b, 
        \end{aligned}
    \]
    which means $ \dot{b}+k_b b=k_a a_0 e^{-k_a t} $. RHS is called a \textit{forcing term}, and it is an eigenfunction of differential operator.

    We \textit{guess} the form of the particular integral
    \[
        b_p = ce^{-k_a t}
    ,\]
    then the equation becomes 
    \[
        -k_a c+k_b c = k_a a_0 \Longleftrightarrow c=\frac{k_a}{k_b-k_a}a_0,\quad \text{for }k_b\neq k_a
    .\]
    Since the general solution for the DE is $b=b_p+b_c$,
    \[
        \dot{b_c}+k_b b_c = 0 \Longleftrightarrow b_c = De^{-k_b t}
    .\]
    Hence
    \[
        b=\frac{k_{a}}{k_{b}-k_{a}} a_{0} e^{-k_{a} t}+D e^{-k_{b} t}
    .\]
    If $b(0)=0$, $ D = -c $, then 
    \[
        b = \frac{k_a}{k_b-k_a}a_0\left( e^{-k_a t}-e^{-k_b t} \right)
    .\]
    Taking the ratio of $ b $ and $a$:
    \[
        \frac{b(t)}{a(t)} = \frac{k_a}{k_b-k_a}\left( 1-e^{(k_a-k_b)t} \right)
    .\]
    We can date the age without knowing $a_0$ is. This result allows rocks and other materials to be dated by measuring ratio of isotopes.
    \section{First order ODEs of non-constant coefficients}
    The general form is 
    \[
        a(x)y'+b(x)+y = c(x)
    .\]
    The standard form is 
    \[
        y'+ p(x)y=f(x)
    .\]
    Solved using \textit{integrating factors}, multiply by IF $\mu$:
    \[
        \mu y'+(\mu p)y=\mu f
    .\]
    If $ \mu p=\mu' $, LHS$= (\mu y)' $ by product rule. Hence we want $p = \mu'/\mu$.
    \[
        \int p \,\mathrm{d}x = \int \frac{\mu'}{\mu} \,\mathrm{d}x = \ln \mu \Longrightarrow\boxed{\mu = e^{\int p(x) \,\mathrm{d}x}}
    .\]
    Thus the DE becomes 
    \[
        (\mu y)' = \mu f \Longleftrightarrow y = \frac{1}{\mu}\int \mu f \,\mathrm{d}x
    .\]
    \section{Discrete equations}\marginnote{Lecture 8}
    A \textit{discrete equation} is an equation involving a function evaluated at a discrete set of points.
    \subsection{Numerical integration}
    Consider a discrete representation of $y(x)$, $ y(x_1),\dots,y(x_{n}) $. One approximation to $y'$ is 
    \[
        \frac{\mathrm{d}y}{\mathrm{d}x}\Big|_{x_n} \approx \frac{y_{n+1}-y_n}{h},\quad h=\frac{x_n}{n} 
    ,\]
    given that $x_i$ are uniformly distributed. This is called the \textit{Forward Euler} approximation, but it is not the best approximation of the derivative in most contexts.
    
    \begin{example}
        Consider $ 5y'-3y=0 $. We can approximate the equation by 
        \[
            5\frac{y_{n+1}-y_n}{h}-3y=0
        ,\]
        which is called a \textit{difference equation}, and deduce that 
        \[
            y_{n+1}=\left( 1+\frac{3h}{5} \right)y_n
        ,\]
        which is called a \textit{recurrence relation}.

        Apply recurrence relation repeatedly:
        \[
            y_n=\left( 1+\frac{3h}{5} \right)^{n}y_{0}=\left( 1+\frac{3x_n}{5n} \right)^{n}y_{0}
        .\]
        Euler's definition of $ e^x $ is 
        \[
            e^x=\lim_{n \to \infty} \left( 1+\frac{x}{n} \right)^n
        .\]
        It can be shown that this definition is equivalent to the previous definition. Hence 
        \[
            y(x)=\lim_{n \to \infty} y_n = y_0e^{3x/5}
        .\]
        \bluecomment{Note for finite $n$, $y_n<y(x)$.}
    \end{example}
    \subsection{Series solutions}
    A powerful way to solve ODEs is to seek solutions in the form of an infinite power series. 
    \[
        y(x)=\sum_{n=0}^{\infty}a_n x^n
    .\]
    Plug into DE and find a solution.
    \begin{example}
        Consider $ 5y'-3y=0 $. Let 
        \[
            y(x)=\sum_{n=0}^{\infty}a_n x^n, \quad y'=\sum_{n=1}^{\infty}na_n x^{n-1}
        .\]
        Multiply both sides by $x$:
        \[
            \begin{aligned}
                xy'&=\sum_{n=1}^{\infty}na_nx^n,\\
                xy&= \sum_{n=0}^{\infty}a_n x^{n+1}=\sum_{n=1}^{\infty}a_{n-1} x^{n}.
            \end{aligned}
        \]
        Then the DE becomes 
        \[
            \begin{aligned}
                &5\sum_{n=1}^{\infty}na_nx^n-3\sum_{n=1}^{\infty}a_{n-1} x^{n}=0\\
                \Longleftrightarrow & \sum_{n=1}^{\infty}x^n\left( 5na_n-3a_{n-1} \right)=0.
            \end{aligned}
        \]
        This holds for every $x\in \mathbb{R}$, so it holds if and only if
        \[
            \forall x\in \mathbb{R}, 5na_n-3a_{n-1}=0 \Longleftrightarrow a_n = \frac{3}{5n}a_{n-1} \Longleftrightarrow a_n=\left( \frac{3}{5} \right)^n \frac{a_0}{n!}
        .\]
        Hence
        \[
            y= a_0\sum_{n=0}^{\infty} \left( \frac{3}{5} \right)^n \frac{x^n}{n!} = a_0 e^{3x/5}
        .\]
        This converges for all $x$, so $ y(x)=a_0 e^{3x/5} $ is a solution.
    \end{example}
    \part{First order nonlinear ODEs}
    General form is 
    \begin{equation}
        Q(x,y)\frac{\mathrm{d}y}{\mathrm{d}x}+P(x,y)=0.
    \end{equation}
    \section{Separable equations}
    (10.1) is separable if and only if it can be written in the form 
    \[
        q(y)\d y=p(x)\d x,
    \]
    and we simply solve $x,y$ by integrating both sides.
    \section{Exact equations}
    (10.1) is an \textit{exact equation} if and only if 
    \[
        Q(x,y)\d y+P(x,y)\d x\tag{*}
    \]
    is an \textit{exact differential} of function $f(x,y)$. i.e., $ \d f= Q\d y+P\d x $. If this holds, then (10.1) implies that $ \d f=0 $ and $f(x,y)$ is constant. We can use multivariable chain rule to check.
    \[
        \d f=\frac{\partial f}{\partial x}\d x+ \frac{\partial f}{\partial y}\d y \Longrightarrow \frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{\mathrm{d}y}{\mathrm{d}x}=0     
    .\]
    Comparing with (10.1), if $(*)$ is an exact differential, then $\exists f(x)$ such that 
    \[
        \frac{\partial f}{\partial x}=P(x,y),\quad \frac{\partial f}{\partial y}=Q(x,y)  \tag{**}
    .\]
    Hence 
    \[
        \begin{aligned}
             & \frac{\partial^2 f}{\partial y\partial x} =\frac{\partial P}{\partial y} \land  \frac{\partial^2 f}{\partial x\partial y} =\frac{\partial Q}{\partial x}\\
             \Longleftrightarrow & \boxed{\frac{\partial P}{\partial y}=\frac{\partial Q}{\partial x}}.
        \end{aligned}
    \]
    If it holds throughout a \textit{simply connected} domain $\mathcal{D}$, then $ P\d x+Q\d y $ is an exact differential of a single-valued function $f(x,y)$ in $D$. Henc we can use this to check exact equations.

    $f(x,y)$ can be found by integrating $(**)$.
    \begin{example}
        Consider 
        \[
            6y(y-x)\frac{\mathrm{d}y}{\mathrm{d}x}+(2x-3y^2)=0 
        .\]
        Here $P=2x-3y^2, Q=6y(y-x)$. We have 
        \[
            \frac{\partial P}{\partial y}=-6y= \frac{\partial Q}{\partial x}
        ,\]
        so it is an exact equation.
        Note that 
        \[
            \begin{aligned}
                 &\int \frac{\partial f}{\partial x}  \,\mathrm{d}x=x^2-3xy^2+h(y),\\
                 &\frac{\partial f}{\partial y}=(x^2-3xy^2+h(y))'_y= -6xy+h'(y)=6y(y-x),\\
                 \Longrightarrow & h'=6y^2 \Longrightarrow h=2y^3.
            \end{aligned}
        \]
        Hence $f(x,y)=x^2-3xy^2+2y^3+C$ and 
        \[
            x^2-3xy^2+2y^3=C
        \]
        is the general solution. 
    \end{example}
    \section{Isoclines and solution curves}\marginnote{Lecture 9.}
    Nonlinear equations are not guaranteed to have simple/closed form solutions. Nevertheless we can analyze the behaviour of the system without solving.

    Consider an ODE of the form 
    \[
        \frac{\mathrm{d}y}{\mathrm{d}t}=f(y,t) 
    .\]
    Each intial condition will give a different solution curve.
    \begin{example}
        Consider the equation 
        \[
            \frac{\mathrm{d}y}{\mathrm{d}t}=t(1-y^2)=f(y,t) \tag{$*$}
        .\]
        It is separable:
        \[
            \begin{aligned}
                 &\int \frac{\d y}{1-y^2} \,\mathrm{d}y=\int t \,\mathrm{d}t\\
                 \Longrightarrow &y= \frac{A-e^{-t^2}}{A+e^{-t^2}}.
            \end{aligned}
        \]
        This general solution produces a family of solution curves, parameterised by by $A$.
    \end{example}
    \begin{definition}[Isocline]
        An \textit{isocline} is the curve along which $ f=\dot{y}=C $, where $C$ is a constant.
    \end{definition}
    Procedure of drawing a curve: draw isoclines, inspect the slope of $y$, draw a vector field, and plot the lines.
    \begin{remark}
        Since $f(y,t)$ is single-valued, any two solution curves do not cross.
    \end{remark}
    \section{Fixed(equilibrium) points}
    \begin{definition}
        A fixed point is a point where
        \[
            \frac{\mathrm{d}y}{\mathrm{d}t}=f(y,t)=0 
        .\]
        A fixed point is called \textit{stable}(\textit{unstable}) if solution curves in a small neighbourhood of the fixed point converge(diverge) to(away) the fixed point.
    \end{definition}
    We can analyze the stability of fixed points using a \textit{perturbation} analysis.

    Let $y=a$ be a fixed point of $ \frac{\mathrm{d}y}{\mathrm{d}t}=f(y,t)  $, i.e. $ f(a,t)=0 $. Consider a small perturbation from the fixed point: $ y=a+\epsilon(t) $ We have 
    \[
        \begin{aligned}
            \frac{\mathrm{d}\epsilon}{\mathrm{d}t}&=\frac{\mathrm{d}(y-a)}{\mathrm{d}t}
            = \frac{\mathrm{d}y}{\mathrm{d}t}\\
            &=f(a+\epsilon,t)\\
            &= f(a,t)+\epsilon \frac{\partial f}{\partial y}(a,t)+O(\epsilon^2).
        \end{aligned}
    \]
    For small $ \epsilon $, we have 
    \[
        \frac{\mathrm{d}\epsilon}{\mathrm{d}t}\approx \epsilon \frac{\partial f}{\partial y}(a,t)  
    .\]
    Hence we've converted the non-linear ODE into a linear one wrt $ \epsilon $.

    If $ \lim_{t \to \infty} \epsilon=0 $, then $a$ is a stable fixed point. Conversely if $ \lim_{t \to \infty} \epsilon=\infty  $, then $a$ is an unstable fixed point. If $ f'_y(a,t)=0 $, then we need higher order terms in Taylor series.
    \begin{example}
        Consider $f(y,t)=t(1-y^2)$. The fixed points are $y=\pm 1$. $ f'_y=-2yt $. At $y=1$, we have 
        \[
            \dot{\epsilon}\approx -2\epsilon t \Rightarrow \epsilon=\epsilon_0 e^{-t^2}\to 0
        .\] 
        Hence $1$ is stable.
        
        At $y=-1$, $ \dot{\epsilon}=2t\epsilon \Rightarrow \epsilon=\epsilon_0e^{t^2}\to \infty $. Hence $-1$ is unstable.
    \end{example}
    \section{Autonomous DEs}
    \begin{definition}
        An \textit{autonomous DE} is a special case when $\dot{y}=f(y)$.
    \end{definition}
    In this case, near fixed points $ y=a$, we have $ \dot{\epsilon}=f'_y(a)\epsilon=\epsilon k $, where $k$ is constant. Hence $ \epsilon=\epsilon_0 e^{kt} $. Therefore for autonomous DEs we have 
    \[
      \text{if } \begin{cases}
      f'(a)<0 \Rightarrow &\text{stable F.P.}\\
       f'(a)>0 \Rightarrow &\text{unstable F.P.}\\
      \end{cases}   
    \]
    \section{Phase Portraits}\marginnote{Lecture 10}
    Another way to analyze solutions to a DE is using a geometrical representation of the solution called a \textit{phase portrait}.
    \begin{example}[Chemical kinetics]
        Consider the reaction
        \[
            \ce{NaOH + HCl -> NaCl + H_2O}
        \]
        with 
        \begin{center}
            \begin{tabular}{lccccccc}
              \toprule
              & NaOH & + & HCl & $\rightarrow$ & H$_2$O & + & NaCl \\
              \midrule
              Number of molecules & $a$ & & $b$ & & $c$ & & $c$ \\
              Initial number of molecules & $a_0$ & & $b_0$ & & $0$ & & $0$ \\
              \bottomrule
            \end{tabular}
          \end{center}
          A model of reaction rate is 
          \[
              \frac{\mathrm{d}c}{\mathrm{d}t}=\lambda ab 
          ,\]
          where $ \lambda $ is constant. Atoms are conserved: $ a=a_0-c, b=b_0-c $. Then the equation can be written as 
          \[
              \frac{\mathrm{d}c}{\mathrm{d}t}=\lambda(a_0-c)(b_0-c) 
          .\]
          This is an example of nonlinear first order ODE.

          Plot a 2D phase portrait for this DE. One way is to plot $ \frac{\mathrm{d}c}{\mathrm{d}t}  $ against $t$:
          \begin{center}
            \begin{tikzpicture}
              \draw [->] (-0.5, 0) -- (5, 0) node [right] {$c$};
              \draw [->] (0, -1) -- (0, 4) node [above] {$\dot c$};
              \draw [blue, semithick] (0.5, 3) parabola bend (2.5, -1) (4.5, 3);
              \node at (1.5, 0) [anchor = north east] {$a_0$};
              \node at (3.5, 0) [anchor = north west] {$b_0$};
            \end{tikzpicture}
          \end{center}
          We can analyze the behaviour using 1D phase portrait:
          \begin{center}
            \begin{tikzpicture}
              \draw [->-=0.5] (0, 0) -- (2, 0);
              \draw [->-=0.6] (4, 0) -- (2, 0);
              \draw [->-=0.5, ->] (4, 0) -- (6, 0) node [right] {$c$};
              \draw (2, 0.1) -- (2, -0.1) node [below] {$a_0$};
              \draw (4, 0.1) -- (4, -0.1) node [below] {$b_0$};
            \end{tikzpicture}
          \end{center}
          arrows are drawn by sign of $ \dot{c} $.
    \end{example}
    \begin{example}[Population dynamics]
        Let $ y(t) $ be population, $ \alpha y $ be birth rate and $ \beta y $ be death rate.
        \begin{enumerate}[(a)]
            \item Linear model
            \[
                \frac{\mathrm{d}y}{\mathrm{d}t}=\alpha y-\beta y \Rightarrow y=y_{0}e^{(\alpha-\beta)t} 
            .\]
            If $ \alpha>\beta $, then $ \lim_{t \to \infty} y=\infty  $.
            \item Nonlinear model 
            \[
                \frac{\mathrm{d}y}{\mathrm{d}t} =(\alpha-\beta)y-\gamma y^2
            .\]
            $ \gamma y^2 $ is dominant when $y$ is large. It models increased death rate at high population.

            Equivalently we have $  \dot{y}=ry(1-\frac{y}{\lambda}) $ where $ r=\alpha-\beta, \lambda=\frac{\alpha-\beta}{\gamma} $. $ \lambda $ is called the \textit{carrying capacity}.
            \begin{center}
                \begin{tikzpicture}[yscale = 1.5]
                  \draw [->] (-0.5, 0) -- (4.5, 0) node [right] {$y$};
                  \draw [->] (0, -1) -- (0, 2) node [above] {$\dot{y}$};
                  \node [anchor = north east] at (0, 0) {$O$};
                  \draw [mblue, semithick] (0, 0) parabola bend (1.75, 1.5) (4, -.9796);
                  \node [anchor = north east] at (3.5, 0) {$\lambda$};
              
                  \draw [->-=0.5] (0, -1.5) -- (3.5, -1.5);
                  \draw [-<-=0.2, ->] (3.5, -1.5) -- (4.5, -1.5) node [right] {$y$};
                  \draw (0, -1.4) -- (0, -1.6) node [below] {$O$};
                  \draw (3.5, -1.4) -- (3.5, -1.6) node [below] {$\lambda$};
                \end{tikzpicture}
              \end{center}
        \end{enumerate}
    \end{example}
    \section{Fixed points in discrete equations}
    \subsection{Definitions}
    Consider a first order discrete(difference) equation of the form 
    \[
        x_{n+1}=f(x_n)
    .\]
    Define the \textit{fixed point} as the value of $x_n$ where $ x_{n+1}=x_n $. That is, where
    \[
        f(x_n)=x_n
    .\]
    \subsection{Stability}
    Use perturbation analysis to study its stability. Let $ x_f $ be a stable point of $x_n$, and perturb by a small $ \epsilon $:
    \[
        \begin{aligned}
            f(x_f+\epsilon)&=f(x_f)+\epsilon \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f}+O(\epsilon^2) \\
            &= x_f+\epsilon \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f}+O(\epsilon^2)\\
            &\approx x_f+\epsilon \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f}.
        \end{aligned}
    \]
    Let $ x_n= x_f+\epsilon$, then 
    \[
        x_{n+1}\approx x_f+\epsilon \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f}.
    \]
    Therefore,
    \[
        x_f \text{ is} \begin{cases}
        \text{stable} &\text{if } \left| \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f} \right| <1\\
        \text{unstable} &\text{if }\left| \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_f} \right| >1\\
        \end{cases} 
    \]
    \subsection{Example: Logistic map}
    Nonlinear discrete population model 
    \[
        \frac{x_{n+1}-x_n}{\Delta t}=\lambda x_n-\gamma x_{n}^2
    .\]
    Formally,
    \[
        x_{n+1}=(\lambda \Delta t+1) x_n-\gamma \Delta t x_n^2
    .\]
    A simpler version is 
    \[
        x_{n+1}=rx_n(1-x_n)=f(x_n)
    .\]
    This equation is the Logistic map.
    
    \textbf{Fixed points}: Let $ f(x_n)=x_n $. We have $ x_n(r-1-rx_n)=0 \Rightarrow x_n=0 \lor x_n=1-1/r $.

    \textbf{Stability}: Write $ f(x)=rx(1-x) $, then 
    \[
        \frac{\mathrm{d}f}{\mathrm{d}x}=r(1-2x) 
    .\]
    Hence for $x_n=0$ we have $ \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_0=r $, so if $ 0<r<1 $, it is stable. For $ r>1 $, it is unstable.

    For $ x_n=1-\frac{1}{r} $, $ \frac{\mathrm{d}f}{\mathrm{d}x}\Big|_{x_n}=2-r  $. Then for $ 0<r<1 $ it is unphysical since $x_n<0$. If $ 1<r<3 $, it is stable. For $ r>3 $ it is unstable.
\end{document}