\section{Convexity}
\subsection{Generic optimization problem}

\begin{definition}
    An optimization problem is to 
    \begin{align*}
        \text{minimize}\ \ \ & f(x)\\ 
        \text{subject to}\ \ \ & x\in X, g(x) = b, 
    \end{align*}
    where
    \begin{itemize}
        \item $ x\in \mathbb{R}^{n} $ are called \textbf{decision variables},
        \item $ f: \mathbb{R}^{n}\to \mathbb{R} $ is called \textbf{objective function},
        \item $ X \subseteq \mathbb{R}^{n} $ are called \textbf{regional constraints}
        \item $ g: \mathbb{R}^{n} \to \mathbb{R}^{m} $ are called \textbf{functional constraints},
        \item $ b\in \mathbb{R}^{m} $. 
    \end{itemize}
\end{definition}
\begin{definition}
    The \textbf{feasible set for $x$} is defined by the constraints as 
    \[
        X(b) = \{x: g(x)=b,x\in X\}. 
    \]
    The problem is \textbf{feasible} if $X(b)\neq \varnothing$, and \textbf{bounded} if $f$ is bounded below on $X(b)$. $ x^* $ is an \textbf{optimal solution} if it minimizes $f$ over $X(b)$.
\end{definition}
\begin{note}
    A constraint of the form $ g(x)\le b $ can be turned into an equality constraint by the addition of a \textbf{slack variable} $z$:
    \[
        g(x)\le b \iff g(x) + z = b,\quad z\ge 0.
    \]
\end{note}

\subsection{Gradient descent}
\paragraph{Motivation}
Consider minimizing $f(x)$ over $ x\in \mathbb{R}^{n} $. How do we construct an iterative algorithm to compute the minimal value?

An intuitive idea is to start at some initial point $x_0$ and move ``downhill'' with fixed steps. However we need to be cautious that a global minimum may not exist, may not be unique, or may be hard to find because of local minima. 

\begin{example}
    The following function doesn't have any minimum on $ (0,\infty) $. If we restrict to $ [0,K],K> 0 $, we have to deal with multiple local minima if $K$ is large. 
    \[
        f(x) = 1+3 e^{-x / 2 0}-\frac{1}{98} e^{-x / 1 0}(-10+x)^2+e^{-x / 1 0} \cos (x)
    \]
    \begin{center}
        \includegraphics[scale=0.6]{opt-gradient-descent-counterexample.pdf}
    \end{center}
\end{example}
\begin{example}
    We are not able to use our algorithm effectively on the following function. 
    \begin{center}
        \vspace*{-2.5em}
        \includegraphics[scale=0.6]{opt-gradient-descent-counterexample2.pdf}
    \end{center}
\end{example}

\paragraph{Direction of descent} 
Expand $f$ around a displacement $t u$ where $t>0$ and $u$ is a unit vector: 
\[
    f(x_0 + tu) = f(x_0) + t\, \grad f(x_0)^\top u + o(t).
\] 
Recall that 
\[
    \grad f(x_0) ^\top = \left( \frac{\mathrm{d}f(x_0)}{\mathrm{d}x_1},\dots, \frac{\mathrm{d}f(x_0)}{\mathrm{d}x_n}   \right). 
\]
By Cauchy-Schwarz we see that $ \grad f(x_0)^\top u $ is minimized when 
\[
    u = -\frac{\grad f(x_0)}{\left\| \grad f(x_0) \right\|}.
\]
Therefore the steepest downhill direction from $x_0$ is $ - \grad f(x_0) $.

\paragraph{Gradient descent algorithm}
With the above motivations we can form an algorithm: 
\begin{framed}\vspace{-0.4em}
\begin{enumerate}
    \item Start with an initial guess $x_0\in \mathbb{R}^{n}$.
    \item Pick a step size $t>0$. 
    \item For every $k=0,1,2,\dots$, let $ x_{k+1} = x_k - t \grad f(x_k) $.
\end{enumerate}
\end{framed}
\begin{example}
    Consider $ f(x)=x^2 $. $ x_{k+1} = x_k - t(2x_k) = (1-2t)x_k $. We need $ t<0 $ for convergence $ x_k\to x^* = 0 $.
\end{example}

\subsection{Convexity}
\begin{definition}
    A set $S\in \mathbb{R}$ is \textbf{convex} if $ \forall x,y\in S $ and $ 0\le \lambda\le 1 $, the point $ \lambda x+ (1-\lambda) y \in S $. 
\end{definition}
\begin{definition}
    A function $f:S\to \mathbb{R}$ is \textbf{convex} if 
    \[
        f(\lambda x+(1-\lambda)y)\le \lambda f(x)+ (1-\lambda)f(y),\quad \forall x,y\in S.
    \]
    $f$ is \textbf{strictly convex} if the inequality is always strict; 

    $f$ is \textbf{strongly convex} if $ \exists \alpha>0$ such that $ f(x)-\frac{\alpha}{2}\left\| x \right\|^2 $ is convex; 

    $f$ is \textbf{concave} if $-f$ is convex.
\end{definition}

\begin{example}
    $f(x)=x^2$ is strictly convex and $ f(x)=\log x $ is strictly concave. Linear functions are both convex and concave.
\end{example}

\begin{note}
    Equivalently, $f$ is convex if the set above its graph (called its \textbf{epigraph}) is convex, $\operatorname{epi}(f)=\{(x, t): f(x) \leq t\}$ (see example sheet TBD)
\end{note}

\begin{theorem}[Supporting hyperplane theorem (SHT)]\label{thm:supporting hyperplane}
    Let $f\in S\to \mathbb{R}$ with $S$ a convex set. Then $f$ is convex if and only if for every $x\in S$ there exists $ \lambda(x)\in \mathbb{R}^{n} $ such that for all $y$
    \[
        f(y)\ge f(x) + \lambda(x)^\top (y-x). 
    \]
    If $f$ is differentiable, then $\lambda(x) = \grad f(x)$.
\end{theorem}
The following diagram shows some supporting hyperplanes of convex sets. 
\begin{center}
    \includegraphics[scale=0.4]{Supporting_hyperplane1.pdf}
    \includegraphics[scale=0.4]{Supporting_hyperplane2.pdf}
\end{center}
\begin{proof}
    Let $ y,z\in S, x = py + qz, p+q=1, 0<p<1 $.

    $ ( \Leftarrow ) $. Suppose $ \lambda(x) $ exists. Then 
    \begin{align*}
        p f(y) + q f(z) &\ge p(f(x)+\lambda(x)^\top(y-x)) + q (f(x)+ \lambda(x)^\top (z-x))\\
        &= f(x). 
    \end{align*}
    This is the definition of convexity. 

    $ ( \Leftarrow) $ Assume $f$ is differentiable and convex (non-differentiable functions are above this course). Then 
    \begin{align*}
        & f(py+(1-p)x) \le pf(y) + (1-p)f(x)\\
        \implies & f(x+p(y-x)) -f(x) \le p(f(y)-f(x))\\ 
        \implies & \frac{f(x+p(y-x))-f(x)}{p} \le f(y)-f(x).
    \end{align*}
    Let $p\searrow 0$, LHS becomes $ \grad f(x)^\top (y-x) $ and thus
    \[
        f(y)\ge f(x) + \grad f(x)^\top (y-x).
    \]
    This is the supporting hyperplane.
\end{proof}
\begin{theorem}
    If $f$ is convex and differentiable, $x^*$ is feasible, and $x^*$ is a stationary point ($ \grad f(x^*) = 0 $), then $x^*$ is the global minimizer of $f$.
\end{theorem}
\begin{proof}
    Corollary of theorem \ref{thm:supporting hyperplane}.
\end{proof}

\begin{theorem}
    Let $f: S \rightarrow \mathbb{R}$, with $S$ a convex set. Then 
    \begin{enumerate}[(a)]
        \item $f$ is convex $\Longrightarrow$ every local minimum is a global minimum;
        \item $f$ strictly convex $\Longrightarrow$ the global minimum is unique;
        \item $f$ is continuous and strongly convex, and $S \subseteq \mathbb{R}^n$ closed $\Longrightarrow$ there exists an optimal solution to the problem of minimizing $f$ over $S$.
    \end{enumerate}
\end{theorem}
\begin{proof}
    \begin{enumerate}[(a)]
        \item Let $x^*$ be a local minimium, and $y$ be any other point. Let $z = (1-\lambda)x^*+\lambda y$. By convexity,
        \[
            f(z) \le (1-\lambda) f(x^*) + \lambda f(y).
        \]
        Since $ f(x^*)\le f(z) $ for sufficiently small $\lambda$ by local minimality, $ f(x^*)\le f(y) $ and $x^*$ is a global minimium. 
        \item Suppose $x,y$ are both global minimia. By strict convextiy 
        \[
            f\left( \frac{1}{2}x+\frac{1}{2}y \right) < \frac{1}{2}f(x) + \frac{1}{2} f(y),
        \]
        contradicts with the minimality of $x,y$. Hence it is unique. 
        \item As $f(x)-(\alpha / 2)\|x\|^2$ is convex, the supporting hyperplane theorem gives
        \[
        f(x)-(\alpha / 2)\|x\|^2 \geq f(0)-(\alpha / 2)\|0\|^2+\lambda(0)^{\top}(x-0) .
        \]
        Let $\lambda=\lambda(0) .$ By Cauchy-Schwarz $\lambda^{\top} x \geq-\|\lambda\|\|x\|$, so if $\|x\|>R=2\|\lambda\| / \alpha$,
        \[
        f(x) \geq f(0)-\|\lambda\|\|x\|+(\alpha / 2)\|x\|^2>f(0).
        \]
        So we may restrict to minimizing over the region where $\left\| x \right\| \le R$. We know from analysis that a continuous function attains its minimum on a closed and bounded set.
    \end{enumerate}
\end{proof}

\begin{theorem}[Lower bound on the gradient]\label{thm:lower_bound_gradient}
    Suppose $f: S \rightarrow \mathbb{R}$ is differentiable and strongly convex with constant $\alpha>0$. Then for any $x, y \in S$
    \[
    \|\nabla f(x)\|^2 \geq 2 \alpha(f(x)-f(y))
    \]
\end{theorem}
\begin{note}
    If $y=x^*$ is the minimizer of $f$ then
\[
f\left(x^*\right) \geq f(x)-\frac{1}{2 \alpha}\|\nabla f(x)\|^2 .
\]
\end{note}
\begin{proof}
    Apply supporting hyperplane theorem to $ f(x)-\frac{\alpha}{2} \left\| x \right\|^2 $: 
    \begin{align*}
        f(y) - \frac{\alpha}{2}\left\| y \right\|^2 &\ge f(x) - \frac{\alpha}{2} \left\| x \right\|^2 + (\grad f(x) - \alpha x)^\top (y-x)\tag{By SHT}\\ 
        \iff f(y)-f(x)&\ge \grad f(x)^\top(y-x) + \frac{\alpha}{2}(\left\| y \right\|^2 - \left\| x \right\|^2 - 2 x^\top(y-x))\\ 
        \iff f(y)-f(x)&\ge \grad f(x)^\top(y-x) + \frac{\alpha}{2}(\left\| y \right\|^2 - 2 x^\top y + \left\| x \right\|^2)\\ 
        &= \grad f(x)^\top(y-x) + \frac{\alpha}{2}\left\| y-x \right\|^2\\ 
        & = \frac{\alpha}{2}\left\| (y-x)+\frac{1}{\alpha}\grad f(x) \right\|^2-\frac{1}{2\alpha}\left\| \grad f(x) \right\|^2 \tag{Complete the square}\\ 
        &\ge -\frac{1}{2\alpha}\left\| \grad f(x) \right\|^2.\\
        \iff \left\| \grad f(x) \right\|^2 &\ge 2\alpha (f(x)-f(y)).\qedhere
    \end{align*}
\end{proof}
\begin{remark}
    The term $ \frac{\alpha}{2}\left\| (y-x)+\frac{1}{\alpha}\grad f(x) \right\|^2 $ is minimized (to 0) when $ y = x - \frac{1}{\alpha} \grad f(x) $. Comparing to the gradient descent algorith $ x_{k+1} = x_k - t \grad f(x_k) $, it suggest using $ t = \frac{1}{\alpha} $ as the step size. 
\end{remark}
\newpage