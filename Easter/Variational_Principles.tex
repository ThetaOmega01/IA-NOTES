\documentclass[a4paper]{article}

\newcommand{\triposcourse}{Variational Principles}
\input{./header.tex}
\graphicspath{ {./images/} }
\pgfplotsset{compat=1.17}
\begin{document}
\maketitle
\tableofcontents
\setcounter{section}{-1}
\newpage
\part*{Lecture 1}
\section{Motivation}
The history of variational principles can be dated back to a pretty concrete classical problem known as the Brachistochrone problem:
\begin{example}[The Brachistochrone Problem]
    Consider a particle moving under the influence of gravity on a wire joining points $A$ and $B$.
    What shape of the curve gives the shortest travel time of the particle given that it starts from rest?\\
    Mathematically, we want to minimise the quantity
    $$\tau=\int_A^B\mathrm dt=\int_A^B\frac{\mathrm dL}{v(x,y)}$$
    This problem was first created by Johann Bernoulli in 1696, who posted the problem in a journal as a challenge to the world's mathematcians at the time.
    And the answer given by Newton starts the study of calculus of variations.
    We are not going to fully solve the problem in this section, but instead just give a taste of the whole picture.\\
    Assuming $A$ is the origin and $B=(x_2,y_2)$
    Using energy conservation,
    $$\frac{1}{2}mv^2+mgy=0\implies v=\sqrt{-2gy}$$
    Take $y$ as a function of $x$, then we are aiming at finding the minima of the functional
    $$\tau[y]=\frac{1}{\sqrt{2g}}\int_0^{x_2}\frac{\sqrt{1+(y^\prime)^2}}{\sqrt{-y}}\,\mathrm dx$$
    subject to
    $$\begin{cases}
        y(0)=0\\
        y(x_2)=y_2
    \end{cases}$$
\end{example}
Another very famous and very useful example is geodesics.
\begin{example}[Geodesics]
    On a surface $\Sigma$, a geodesics $\gamma$ is the path of least length joining two given points.
    If $\Sigma$ is the Euclidean plane, it is well-known that $\gamma$ has to be a straight line.
    Let $D[y]$ denote the length of the curve given by $y$ and assume that the curve traverse as a function $y(x)$ from $x_1,x_2$, then minimising the following integral 
    $$D[y]=\int_{x_1}^{x_2}\sqrt{1+(y^\prime)^2}\,\mathrm dx$$
    subject to $y(x_1)=y_1,y(x_2)=y_2$ would be our aim.
\end{example}
In general, the calculus of variations focus on solving optimisation problems of a functional (i.e. functions from a space of functions to the reals)
$$F[y]=\int_{x_1}^{x_2}f(x,y(x),y^\prime(x))\,\mathrm dx$$
among all (sufficiently smooth) functions $y$ subject to initial conditions.
\begin{example}[Examples of Functionals]
    1. For a function $y(x)$, we can define the functional calculating the area under the curve by setting $f(x,y,y^\prime)=y$.\\
    2. We can also define the functional calculating length by
    $$f(x,y,y^\prime)=\sqrt{1+(y^\prime)^2}$$
\end{example}
Notationally, we write $C(\mathbb R)$ as the space of continuous functions from $\mathbb R$ to $\mathbb R$.
We write $C^k(\mathbb R)$ to denote the space of $k$-time differentiable functions with continuous $k^{th}$ derivative.
And $C^k_{\alpha,\beta}(\mathbb R)$ is the space of functions $[\alpha,\beta]\to\mathbb R$ that are $C^k$ on $[\alpha,\beta]$ and vanishes at $\alpha,\beta$.
One should know that these are these are all (infinite-dimensional) vector spaces over $\mathbb R$, the detail of analysis of which will be covered in functional analysis contexts.\\
The reason why the course is called ``variational principles'' instead of ``calculus of variations'' is because what we are studying are principles in nature and laws of physics that follow from extremising functionals.
\begin{example}[Fermat's Principle]
    Light that travels between two points follow the path that extremise the travel time.
\end{example}
\begin{example}[Principle of Least Action]
    Let $T,V$ be the kinetic and potential energies.
    We define the functional on a path
    $$S[\gamma]=\int_{t_1}^{t_2}(T-V)\,\mathrm dt$$
    then the path that a particle travels from $t=t_1$ to $t=t_2$ is the one that extremises $S$.
    Once we have developed our theory further, one can show that Newton's Second Law follows from this principle.\\
    Leibniz's take on this principle is that we live in ``the best of all possible worlds''.
    In some sense, we go from science to something like the field of theology.
    Of course, this is not in the scope of this course.\\
    Richard Feynman's take on this principle is that it is ``wrong''.
    Indeed, in quantum physics, nothing takes a definitive path, but every paths are possible with some probability.
    The principle under this framework, instead, is that the stationary path of a particle is, in fact, the interference along all paths.
\end{example}
What we shall show in this course are the follows:
First, we will see some necessary conditions for a function to be an extrema, like the Euler-Lagrange equations.
Secondly, we will bring this principle on problems in geometry, physics, and problem with constraints (like the isoperimetric inequality).
We will also talk about what is called the ``second variation'' as an analog of the second derivative.
\newpage
\part*{Lecture 2}
\section{Calculus for functions on $ \mathbb{R} ^n $}
Let $C^2(\mathbb R^2)$ denote the collection of twice differentiable functions $\mathbb R^n\to\mathbb R$ with continuous second derivatives.
\begin{definition}
    Let $f\in C^2(\mathbb R^n)$.
    A point $\mathbf{a}\in\mathbb R^n$ is stationary (or is a stationary point of $f$) if $\nabla f(\mathbf{a})=\mathbf{0}$.
\end{definition}
If we expand $f$ near one of its stationary points $\mathbf{x}=\mathbf{a}$, then
\begin{align*}
    f(\mathbf{x})&=f(\mathbf{a})+(\mathbf{x}-\mathbf{a})\cdot\nabla f+\frac{1}{2}(x_i-a_i)(x_j-a_j)\frac{\partial^2f}{\partial x_i\partial x_j}(\mathbf{a})+o(|\mathbf{x}-\mathbf{a}|^2)\\
    &=f(\mathbf{a})+\frac{1}{2}(\mathbf{x}-\mathbf{a})^\top H(\mathbf{a})(\mathbf{x}-\mathbf{a}),
\end{align*}
Where $H_{ij}(\mathbf{a})=\partial^2f/\partial x_i\partial x_j(\mathbf{a})$ is the Hessian, which is a real symmetric matrix for every $\mathbf{a}$.

Shift the origin to set $\mathbf{a}=\mathbf{0}$ and write $H=H(\mathbf{0})$.
As $H$ is real symmetric, we know that in another orthonormal basis $H$ can be diagonalised to $H=\operatorname{diag}(\lambda_1,\ldots,\lambda_n)$, hence in this new coordinate system,
$$f(\mathbf{x})-f(\mathbf{0})=\frac{1}{2}\sum_{i=1}^n\lambda_ix_i^2+o(|x|^2).$$

\begin{theorem}
    Let $\lambda_i$ be as before, then 
    \begin{enumerate}[(i)]
        \item If $\forall i,\lambda_i>0$, then $\mathbf{0}$ is a local minimum.
        \item If instead $\forall i,\lambda_i<0$, then $\mathbf{0}$ is a local maximum.
        \item If some of the eigenvalues are positive and some are negative, then $ \mathbf{0} $ is a saddle point.
        \item If for some $i$ has $\lambda_i=0$, then we need higher order terms. 
    \end{enumerate}
\end{theorem}

In the special case where $n=2$, we have $\det H=\lambda_1\lambda_2$ and $\operatorname{tr} H=\lambda_1+\lambda_2$, both of which are easier to calculate than the actual eigenvalues, so we can formulate a corollary to make things easier.
\begin{corollary}
    \begin{enumerate}[(i)]
        \item If $\det H>0,\operatorname{tr}H>0$, then it is a local minimum.
        \item If $\det H>0,\operatorname{tr}H<0$, it is a local maximum.
        \item If $\det H<0$, it is a saddle point.
        \item If $ \det H=0 $, then we have to look at higher order terms.
    \end{enumerate}
\end{corollary}
Despite these results, one should note that the actual (global) maxima and minima might occur at boundaries where we may not have $\nabla f=0$.

Also, there are plenty of examples of functions who only have saddle points.
Consider a harmonic function $f$ on $D\subset\mathbb R^2$, i.e. $f_{xx}+f_{yy}=0$, then at a stationary point we necessarily have $\operatorname{tr}H=0$, hence either $\det H=0$ or it is a saddle point.

But of course, the theorem and corollary can help a lot in classifying stationary points.
\begin{example}
    Let $f(x,y)=x^3+y^3-3xy$, then $\nabla f=(3x^2-3y,3y^2-3x)^\top$, so the only stationary points are $(x,y)=(0,0),(1,1)$.
    $$H=\begin{pmatrix}
        6x&-3\\
        -3&6y
    \end{pmatrix}$$
    Hence, by the above corollary, $(0,0)$ is a saddle point while $(1,1)$ is a minimum. Near $ f=0, f \sim -3xy $, and $f$ decreases on $y=x$, increases on $y=-x$.
    \begin{center}\vspace{1ex}
        \includegraphics[scale=0.1]{phase_1.1.jpeg}
    \end{center}
\end{example}

\subsection{Constraints and Lagrange Multipliers}
The optimisation problems in $\mathbb R^n$ are most likely coming with some sort of constraints.
We first illustrate this by way of an example
\begin{example}
    Find the circle centered at $(0,0)$ with smallest radius which intersects the parabola $y=x^2-1$.

    \begin{center}
        \includegraphics[scale=0.14]{best_fit_circ.jpeg}
    \end{center}
    
    Naturally, there are many ways to do this.
    One can solve the problem directly by simply substitution
    $$x^2+y^2=x^2+(x^2-1)^2=x^4-x^2+1$$
    which minimum is $3/4$ by either calculus or completing square.
    So the smallest radius is $\sqrt{3}/2$.

    The second method to solve this probelm is called the \textit{Lagrange multipliers}.
    Define a new function
    $$h(x,y,\lambda)=f(x,y)-\lambda g(x,y)$$
    where $f(x,y)=x^2+y^2$ is the function we want to minimise and $g(x,y)=y-x^2+1$ which vanishes if $(x,y)$ is on the parabola, i.e. $ g(x,y)=0 $ is the constraint.
    $\lambda$ here is called the \textit{Lagrange multiplier}.
    We now try to extremise $h$ over $x,y,\lambda$.
    Naturally, we solve the partial derivatives
    $$\begin{cases}
        \partial h/\partial x=2x+2\lambda x=0\\
        \partial h/\partial y=2y-\lambda=0\\
        \partial h/\partial\lambda=y-x^2+1=0
    \end{cases}$$
    Hence the stationary point of $h$ are at $(x,y)=(0,-1),(\pm 1/\sqrt{2},-1/2)$.
    Note that $ f(0,1)=1(\lambda=-2),\ f(\pm 1/\sqrt{2},-1/2)=3/4(\lambda=-1) $, so the result follows.
\end{example}

Why does the method of Lagrange multiplier work?

Geometrically, if we want to minimise a function $f$ subject to $g=0$.
Now $\nabla g$ is always perpendicular to $g=0$.
Then, suppose the minimum of $f$ is $c$, then the graph of $f(x)=c$ would touch $g(x)=0$, therefore their gradients there are parallel, so $\nabla f=\lambda\nabla g$ for some $\lambda$ and hence $\nabla h=0$ (note that $\partial h/\partial\lambda=0$ iff $g=0$), which is exactly the system we want to solve in the last part.

\begin{center}
    \includegraphics[scale=0.15]{best_fit_circ2.jpeg}
\end{center}

If there are many constraints $g_\alpha(\mathbf{x})=0$ of the minimisation problem of $f:\mathbb R^k\to\mathbb R$ where $\alpha=1,\dots,k$, we can define an analogous
$$h(x_1,\ldots,x_k,\lambda_1,\ldots,\lambda_k)=f(\mathbf{x})-\lambda_\alpha g_\alpha(\mathbf{x})$$
with $k$ Lagrange multipliers. Then we can extremise $h$ by solving 
\[
    \frac{\partial h}{\partial x_i} = 0,\quad \frac{\partial h}{\partial \lambda_\alpha}=0.  
\]
It follows by substitution back to $f$.
\newpage
\part*{Lecture 3}
\section{The Euler-Lagrange Equation}
\subsection{Derivation of the Equation}
We now move on to the most important theorem of the course, which gives a necessary condition to extremise a functional in the form
$$F[y]=\int_\alpha^\beta f(x,y,y^\prime)\,\mathrm dx$$
where $f$ is given, and $'$ denotes differentiation.
In contexts of geometry and physics, $y$ is most likely to carry the meaning of the trajectory of some point.
\begin{center}
    \includegraphics[scale=0.12]{euler-lagrange1.jpeg}
\end{center}
We first assume that a extremum $y$ exists, then when we apply a small perturbation $y\mapsto y+\epsilon\eta(x)$ with $\eta(\alpha)=\eta(\beta)=0$ to keep the endpoint fixed.
Now we want to compute $F[y+\epsilon\eta]$, but first of all we will need a lemma.

\begin{lemma}\label{fund_lemma}
    If $g:[\alpha,\beta]\to\mathbb R$ is continuous on $[\alpha,\beta]$ and
    $$\int_\alpha^\beta g(x)\eta(x)\,\mathrm dx=0$$
    for all $\eta\in C([\alpha,\beta])$ with $\eta(\alpha)=\eta(\beta)=0$, then $\forall x\in [\alpha,\beta],g(x)=0$.
\end{lemma}
\begin{proof}
    Assume for sake of contradiction that there exists some $\bar{x}$ on $(\alpha,\beta)$ such that $g(\bar{x})\neq 0$.
    wlog $g(\bar{x})>0$, then by continuity there is an interval $[x_1,x_2]\subset[\alpha,\beta]$ such that $\exists c>0,\forall x\in [x_1,x_2],g(x)>c$.
    Set
    $$\eta(x)=\begin{cases}
        (x-x_1)(x_2-x)\text{, if $x\in [x_1,x_2]$}\\
        0\text{, otherwise}
    \end{cases}$$
    Then
    \begin{align*}
        \int_\alpha^\beta g(x)\eta(x)\,\mathrm dx
        &=\int_{x_1}^{x_2}g(x)(x-x_1)(x_2-x)\,\mathrm dx\\
        &\ge\int_{x_1}^{x_2}c(x-x_1)(x_2-x)\,\mathrm dx>0.
    \end{align*}
    Contradiction.
    So $g=0$ on $(\alpha,\beta)$, and it is also zero at $\alpha,\beta$ by continuity, hence $g=0$ on $[\alpha,\beta]$.
\end{proof}
\begin{remark}
    The $\eta$ we have used above is called a bump function, which is $C^2$ as one can verify.
    One can also make a $C^k$ bump function by considering
    \footnote{It is also easy to construct a $C^{\infty}$ bump function.}
    $$\eta(x)=\begin{cases}
    ((x-x_1)(x_2-x))^{k+1}\text{, if $x\in [x_1,x_2]$}\\
    0\text{, otherwise}
\end{cases}$$
\end{remark}
Now back at $F[y+\epsilon\eta]$, we have
\begin{align*}
    F[y+\epsilon\eta]&=\int_\alpha^\beta f(x,y+\epsilon\eta,y^\prime+\epsilon\eta^\prime)\,\mathrm dx\\
    &=F[y]+\epsilon\int_\alpha^\beta\left( \frac{\partial f}{\partial y}\eta+\frac{\partial f}{\partial y^\prime}\eta^\prime \right)\,\mathrm dx+O(\epsilon^2)
\end{align*}
We will analysis the $O(\epsilon^2)$ remainder later.

For now, we just observe that for $y$ to be an extremum, the first-order term shall vanish, so we want something like 
\[
    \frac{\partial F[y+\epsilon \eta]}{\partial \epsilon} =0.
\]
Integrate the vanishing first-order coefficient by parts,
\begin{align*}
    0&=\left.\frac{\partial f}{\partial y^\prime}\eta\right|_\alpha^\beta+\int_\alpha^\beta\left( \frac{\partial f}{\partial y}\eta-\frac{\mathrm d}{\mathrm dx}\left( \frac{\partial f}{\partial y^\prime} \right)\eta \right)\,\mathrm dx\\
    &=\int_\alpha^\beta\left( \frac{\partial f}{\partial y}-\frac{\mathrm d}{\mathrm dx}\frac{\partial f}{\partial y^\prime} \right)\eta\,\mathrm dx
\end{align*}
By the preceding lemma, we must have
$$\boxed{\frac{\mathrm d}{\mathrm dx}\frac{\partial f}{\partial y^\prime}-\frac{\partial f}{\partial y}=0}$$
This is known as the Euler-Lagrange equation, which is the \textit{necessary condition} for an extremum. This equation is first developed in 1745 in a letter from Lagrange to Euler.
\begin{remark}\ 
    \begin{enumerate}
        \item The Euler-Lagrange equation is a second-order ODE with initial conditions $y(\alpha)=y_1,y(\beta)=y_2$.
        \item Sometimes the LHS is denoted $\delta F[y]/\delta y(x)$ and is called the functional derivative.
        Some author also write $\epsilon\eta=\delta y$, allowing one to write $F[y+\delta y]=F[y]+\delta F[y]$ where
        $$\delta F[y]=\int_\alpha^\beta\frac{\delta F[y]}{\delta y(x)}\delta y(x)\,\mathrm dx$$
        \item Other kinds of boundary conditions are possible, for example $ \frac{\partial f}{\partial y'}\Big|_{\alpha,\beta}=0  $.
        \item Be careful with derivatives as the notation can be a bit confusing.
        The $x,y,y^\prime$ are independent variables when we are talking about partial derivatives of $f(x,y,y^\prime)$.
        \item For any $h(x,y,y^\prime)$, a somewhat useful formula is
        $$\frac{\mathrm dh(x,y(x),y^\prime(x))}{\,\mathrm dx}=\frac{\partial h}{\partial x}+\frac{\partial h}{\partial y}y^\prime+\frac{\partial h}{\partial y^\prime}y^{\prime\prime}$$
        For example, for $f(x,y,y^\prime)=x((y^\prime)^2-y^2)$, we have $\mathrm df/\mathrm dx=(y^\prime)^2-y^2-2xyy^\prime+2y^{\prime\prime}y^\prime x$.
    \end{enumerate}
\end{remark}
\subsection{First Integrals of the Euler-Lagrange Equation}
Now we move on to solve the Euler-Lagrange equations, which is a second order ODE.
In some special cases, this is a quite easy thing to do.
In particular, if $f$ does not explicitly depend on some of its variables, then we can simplify the equation to something that is easier to solve.
These simplifications are often in the form of something being constant.
Expressions like these are called the first integrals of the equations.
Not only are they tools we can use to solve the equation, we can also view them as a conserved quantity of something that is described by a variational problem.
We will discuss the former in this section.
The latter will be mentioned later, when we discuss Noether's Theorem.

Assume that $f$ does not explicitly depend on $y$, then $\partial f/\partial y=0$, so the Euler-Lagrange equation can be rewritten as
$$\frac{\mathrm d}{\mathrm dx}\frac{\partial f}{\partial y^\prime}=0\implies \frac{\partial f}{\partial y^\prime}=\text{const,}$$
which is a first order ODE.
\begin{example}[Geodesics on the Euclidean plane]
    Consider the geodesics on the Euclidean plane.
    We know that we want to extremise the functional
    $$F[y]=\int_\alpha^\beta\sqrt{1+(y^\prime)^2}\,\mathrm dx$$
    In this case, the apparent $f$ does not depend on $y$, hence we can obtain the solution by just solving the first integral
    $$\frac{y^\prime}{\sqrt{1+(y^\prime)^2}}=\text{const.}$$
    Therefore $ y'=m $ and thus $y=mx+c$ for some constants $m,c$, which is our familiar formulation of a straight line, the geodesics of the plane.
\end{example}
\newpage
\part*{Lecture 4}
\begin{example}[Geodesics on a Sphere]\ 
    \begin{center}
        \includegraphics[scale=0.13]{geodesics_sphere.jpeg}
    \end{center}
    Consider the two-dimensional unit sphere $S^2$ in $\mathbb R^3$.
    Use the spherical polar coordinates
    $$x=\sin\theta\sin\phi,y=\sin\theta\cos\phi,z=\cos\theta,\theta\in [0,2\pi),\theta\in[0,\pi)$$
    The line element on $S^2$ inherited from $\mathbb R^3$ then gives
    $$\mathrm ds^2=\mathrm dx^2+\mathrm dy^2+\mathrm dz^2=\mathrm d\theta^2+\sin^2\theta\,\mathrm d\phi^2$$
    A path restricted on the sphere can then be parameterized in terms of $\theta,\phi$.
    Suppose we parameterize the curve by $\phi(\theta)$, then the length functional is
    $$F[\phi]=\int_{\theta_1}^{\theta_2}\sqrt{1+(\phi^\prime)^2\sin^2\theta}\,\mathrm d\theta$$
    We observe that the integral inside does not depend on $\phi$, so we can rewrite the equation to
    $$\frac{\phi^\prime\sin^2\theta}{\sqrt{1+(\phi^\prime)^2\sin^2\theta}}=\frac{\partial f}{\partial \phi^\prime}=\text{const.}=\kappa$$
    Seperating the variables yields
    $$(\phi^\prime)^2=\frac{\kappa^2}{\sin^2\theta(\sin^2\theta-\kappa^2)}\implies\phi=\pm\int\frac{\kappa\,\mathrm d\theta}{\sin\theta\sqrt{\sin^2\theta-\kappa^2}}$$
    which shall produce two solutions, each going one way round.
    To evaluate the integral, we do the substitution $u=\cot\theta$ which produces
    $$\pm\frac{\sqrt{1-\kappa^2}}{\kappa}\cos(\phi-\phi_0)=\cot\theta$$
    for a constant $\phi_0$.
    By considering the geometrical meaning of this equation, it then follows that $\phi(\theta)$ describes a great circle, i.e. a circle in $\mathbb R^3$ which exists as the intersection of $S^2$ and a plane that goes through the origin.
\end{example}

Consider for general case of $f(x,y,y^\prime)$, we have
\begin{align*}
    \frac{\mathrm d}{\mathrm dx}\left( f-y^\prime\frac{\partial f}{\partial y^\prime} \right)&=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}y^\prime+\frac{\partial f}{\partial y^\prime}y^{\prime\prime}-y^\prime\frac{\mathrm d}{\mathrm dx}\frac{\partial f}{\partial y^\prime}-y^{\prime\prime}\frac{\partial f}{\partial y^\prime}\\
    &=y^\prime\left( \frac{\partial f}{\partial y}-\frac{\mathrm d}{\mathrm dx}\frac{\partial f}{\partial y^\prime} \right)+\frac{\partial f}{\partial x}
    =\frac{\partial f}{\partial x}
\end{align*}
If $y$ satisfies the Euler-Lagrange Equation.
So if $f$ does not depend explicitly on $x$, then the above indicates that
$$f-y^\prime\frac{\partial f}{\partial y^\prime}=\text{const.}$$
\begin{example}[The Brachistochrone Problem]
    Consider the functional in the Brachistochrone Problem we defined before, with the initial point assumed to be the origin:
    $$F[y]=\frac{1}{\sqrt{2g}}\int_0^\beta\frac{\sqrt{1+(y^\prime)^2}}{\sqrt{-y}}\,\mathrm dx$$
    As $f$ in this case does not depend explicitly on $x$, we know that
    $$\frac{\sqrt{1+(y^\prime)^2}}{\sqrt{-y}}-y^\prime\frac{y^\prime}{\sqrt{1+(y^\prime)^2}\sqrt{-y}}=\text{const.}=\kappa$$
    So
    $$y^\prime=\pm\frac{\sqrt{1+\kappa^2y}}{\kappa\sqrt{-y}}\implies x=\pm \kappa\int\frac{\sqrt{-y}}{\sqrt{1+\kappa^2y}}\,\mathrm dy$$
    Set $y=-\kappa^{-2}\sin^2(\theta/2)$, so $\mathrm dy=-\kappa^{-2}\sin(\theta/2)\cos(\theta/2)$, so
    \begin{align*}
        x&=\pm \kappa\int(-1)\frac{1}{\kappa^3}\frac{\sin^2(\theta/2)\cos(\theta/2)}{\sqrt{1-\sin^2(\theta/2)}}\,\mathrm d\theta\\
        &=\mp\frac{1}{2\kappa^2}\int(1-\cos\theta)\,\mathrm d\theta\\
        &=\mp\frac{1}{2\kappa^2}(\theta-\sin\theta)+C
    \end{align*}
    where $C$ is a constant.
    By our initial condition, $\theta(0)=0$, so $C=0$.
    Substitute $\theta$ for both $x,y$ and we get the parameterised equations
    $$\begin{cases}
        x=(\theta-\sin\theta)/(2\kappa^2)\\
        y=-\kappa^{-2}\sin^2(\theta/2)
    \end{cases}$$
    which is the equation of a cycloid, i.e. the path traversed by a fixed point on a wheel which rolls on the $x$-axis.
    Hence, the Brachistochrone is a cycloid.

    \begin{center}
        \begin{tikzpicture}
        \coordinate (O) at (0,0);
        \coordinate (A) at (0,3);
        \def\r{1} % radius
        \def\c{1.4} % center
        \coordinate (C) at (\c, \r);
      
      
        \draw[-latex] (O) -- (A) node[anchor=south] {$y$};
        \draw[-latex] (O) -- (2.6*pi,0) node[anchor=west] {$x$};
        \draw[red,domain=-0.5*pi:2.5*pi,samples=50, line width=1] 
             plot ({\x - sin(\x r)},{1 - cos(\x r)});
        \draw[blue, line width=1] (C) circle (\r);
        \draw[] (C) circle (\r);
      
        % coordinate x 
        \def\x{0.4} % coordinate x
        \def\y{0.83} % coordinate y
        \def\xa{0.3} % coordinate x for arc left
        \def\ya{1.2} % coordinate y for arc left
        \coordinate (X) at (\x, 0 );
        \coordinate (Y) at (0, \y );
        \coordinate (XY) at (\x, \y );
      
        \node[anchor=north] at (X) {$x$} ;
      
        % draw center of circle
        \draw[fill=blue] (C) circle (1pt);
      
        % draw radius of the circle
        \draw[] (C) -- node[anchor=south] {\; $a$} (XY);
      
        % bottom of circle, radius to the bottom
        \coordinate (B) at (\c, 0);
        \draw[] (C) -- (B) node[anchor=north] {$a \, \theta$};
      
        % projections of point XY
        \draw[dotted] (XY) -- (X);
        \draw[dotted] (XY) -- (Y) node[anchor=east, xshift=1mm] {$\quad y$};
      
        % arc theta
        % start arc
        \coordinate (S) at (\c, 0.4);
        \draw[->] (S) arc (-90:-165:0.6);
        \node[xshift=-2mm, yshift=-2mm] at (C) {\scriptsize $\theta$};
      
        % arc above
        \coordinate (AA) at (\xa, \ya);
        \draw[-latex, rotate=25] (AA) arc (-220:-260:1.3);
      
        % arc below
        \def\xb{2.5} % coordinate x for arc bottom
        \def\yb{0.8} % coordinate y for arc bottom
        \coordinate (AB) at (\xb, \yb);
        \draw[-latex, rotate=-10] (AB) arc (-5:-45:1.3);
      
      
      
        % XY dot
        \draw[fill=black] (XY) circle (1pt);
      
      
        % top label
        \coordinate (T) at (pi, 2);
        \node[anchor=south] at (T)  {$(\pi a, 2 a )$} ;
        \draw[fill=black] (T) circle (1pt);
      
        % equations
        \coordinate (E) at ( 4,1.2);
        \coordinate (F) at ( 4,0.9);
        \node[] at (E) {\scriptsize $x=a(\theta - \sin \theta)$};
        \node[] at (F) {\scriptsize $y=a(1 - \cos \theta)$};
      
        % label 2pi a
        \coordinate (TPA) at (2*pi, 0);
        \node[anchor=north] at (TPA) {$2 \pi a$};
      
      
        \end{tikzpicture}
      \end{center}
\end{example}

\subsection{Fermat's Principle}
Fermat's Principle postulates that light (or sound) travels along paths between two points that are stationary points of the time variation.
\footnote{In its original form, however, it said that light travels the path that requires the least time, which is not necessarily true in all cases.}
Suppose the light way is described by $y=y(x)$, then the time functional is
$$F[y]=\int\frac{\mathrm dl}{c}=\int_\alpha^\beta\frac{\sqrt{1+(y^\prime)^2}}{c(x,y)}\,\mathrm dx$$
where $c$ is the speed of light in the medium.

First assume that $c=c(x)$ does not depend on $y$, then $f$ does not explicitly depend on $y$, in which case we have the first integral
$$\frac{y^\prime}{\sqrt{1+(y^\prime)^2}c(x)}=\frac{\partial f}{\partial y^\prime}=\text{const.}$$
Suppose the light ray has an initial incident angle of $\theta_1$ upwards, then $\tan\theta_1=y^\prime(\alpha)$.
Let $\tan\theta=y^\prime$ in general, then the above first integral indicates that $\sin\theta/c(x)$ is constant.
This is Snell's Law.
If $c$ is increasing, then the path is concave and if $c$ is decreasing it is convex.
Also, if the light goes through a barrier, to the left of which $c$ is constant at $c_F$ and to the right of which $c_S$ with $c_S<c_F$, then the light ray will refract in the way we all expect.

\newpage
\part*{Lecture 5}
\section{Extensions of the Euler-Lagrange Equations}
\subsection{Euler-Lagrange with Constraints}
Our objective is to extremize the functional
$$F[y]=\int_\alpha^\beta f(x,y,y^\prime)\,\mathrm dx$$
subject to the constraint $G[y]=0$ for a functional $G$ in the form
$$G[y]=\int_\alpha^\beta g(x,y,y^\prime)\,\mathrm dx$$
We can tackle this by using an analog of Lagrange multiplier.
Consider the new functional
$$\Phi[y;\lambda]=F[y]-\lambda G[y]=\int_\alpha^\beta (f-\lambda g)(x,y,y^\prime)\,\mathrm dx$$
from the study of Lagrange multiplier earlier in the $\mathbb R^n$ case, we are inspired to extremise $\Phi$ instead.
The Euler-Lagrange equation form $\Phi$ is then
$$\frac{\mathrm d}{\mathrm dx}\frac{\partial}{\partial y^\prime}(f-\lambda g)=\frac{\partial}{\partial y}(f-\lambda g)$$

\begin{example}[Dido's Problem (aka the Isoperimetric Problem)]
    We want to ask what simple closed plane curve with fixed length $L$ maximises its area.

    We can assume WLOG that the curve is convex and put it on the coordinate plane.
    Then by convexity it is bounded by the lines $x=\alpha,x=\beta$ for some $\alpha,\beta$.

    \begin{center}
        \includegraphics[scale=0.12]{isoparam.jpeg}
    \end{center}

    Also, for each $x\in(\alpha,\beta)$ there are exactly two values $y=y_1,y_2$, $y_1<y_2$ such that $(x,y)$ is on the curve.
    The area element is then $\mathrm dA=(y_2-y_1)\,\mathrm dx$.
    So the functional we want to maximise is
    $$A[y]=\int_\alpha^\beta y_2-y_1\,\mathrm dx=\oint_Cy\,\mathrm dx$$
    subject to the contraint that
    $$L[y]=\oint_C\,\mathrm dl=\oint_C\sqrt{1+(y^\prime)^2}\,\mathrm dx$$
    is constantly $L$.
    To use Lagrange multiplier, we set $h=y-\lambda\sqrt{1+(y^\prime)^2}$, then as $h$ does not explicitly depend on $x$, we can use the first integral
    $$K=\text{const.}=h-y^\prime\frac{\partial h}{\partial y^\prime}=y-\frac{\lambda}{\sqrt{1+(y^\prime)^2}}\implies (y^\prime)^2=\frac{\lambda^2}{(y-K)^2}-1$$
    Hence,
    $$\int\frac{y-K}{\sqrt{\lambda^2-(y-K)^2}}\,\mathrm dy=x-x_0\implies (y-y_0)^2+(x-x_0)^2=\lambda^2$$
    where $x_0,y_0$ are constants.
    This is the equation of a circle, and by the constraint, $\lambda=L/2\pi$.
\end{example}
\begin{example}[The Sturm-Liouville Problem]\label{sturm-liouville}
    Let $\rho=\rho(x)>0$ for $x\in [\alpha,\beta]$.
    Consider the following functional:
    $$F[y]=\int_\alpha^\beta\rho(x)(y^\prime)^2+\sigma(x)y^2\,\mathrm dx$$
    which we want to maximise subject to the condition that
    $$G[y]=\int_\alpha^\beta y^2\,\mathrm dx=1$$
    One will see it again and again in the settings of quantum mechanics.
    So our goal is to extremise
    $$\Phi[y;\lambda]=F[y]-\lambda(G[y]-1)$$
    So
    $$h=\rho(y^\prime)^2+\sigma y^2-\lambda\left( y^2-\frac{1}{\beta-\alpha} \right)$$
    The Euler-Lagrange equation is then
    $$-\frac{\mathrm d}{\mathrm dx}(\rho y^\prime)+\sigma y=\lambda y$$
    We write $\mathcal L(y)$ to denote the differential operator on the left hand side.
    $\mathcal L$ is called the Sturm-Liouville operator.
    Viewing it like this, the ODE is now the eigenvalue problem of the operator $\mathcal L$.
\end{example}
\begin{remark}
    If $\rho=1$, then $\sigma$ can be taken as the potential which makes the equation the (one-dimensional) time-independent Schr\"odinger equation.
\end{remark}
If $\sigma>0$ everywhere, then $F[y]>0$ everywhere.
\begin{claim}
    We claim that he (positive) minimum of $F[y]$ is the lowest eigenvalue of $\mathcal L$.
\end{claim}
\begin{proof}
    We multiply the Sturm-Liouville equation by $y$ on both sides and integrate from $\alpha$ to $\beta$, which gives $F[y]=\lambda G[y]$.
\end{proof}

\subsection{Several Independent Variables}
Let $ \mathbf{y}=(y_1,y_2,\dots,y_n) $ and consider the functional 
\[
    F[\mathbf{y}] = \int_{\alpha}^{\beta} f(x,y_1,\dots,y_n, y_1',\dots,y_n') \,\mathrm{d}x.
\]
Consider a perturbation $ y_i \to y_i + \epsilon_i \eta $ where $ \eta(\alpha)=\eta(\beta)=0 $:
\[
    F[\mathbf{y}+\epsilon \boldsymbol{\eta}] - F[\mathbf{y}] = \epsilon\int_{\alpha}^{\beta} \sum_{i=1}^{n} \eta_i\left( \frac{\mathrm{d}}{\mathrm{d}x}\left( \frac{\partial f}{\partial y_i'}  \right) -\frac{\partial f}{\partial y_i} \right)  \,\mathrm{d}x + \text{boundary terms} + O(\epsilon^2).
\]
Hence by lemma, 
\[
    \frac{\mathrm{d}}{\mathrm{d}x}\left( \frac{\partial f}{\partial y_i'}  \right) = \frac{\partial f}{\partial y_i},\quad i=1,\dots,n.  
\]
This is a system of $n$ 2nd order ODEs. We also have first integrals: if $ \partial f/\partial y_j=0  $ for some $ 1\le j\le n $, then 
\[
    \frac{\partial f}{\partial y_j'}=\text{const}. 
\]
Also if $ \rmd f / \rmd x = 0 $, then (assume summation convention)
\[
    f - y_i' \frac{\partial f}{\partial y_i'}= \text{const}. 
\]
\end{document}