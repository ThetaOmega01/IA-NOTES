\section{Discrete probability distributions}
\subsection{Examples of discrete distributions}
This section we talk about $ (\Omega,\mathscr{F},\mathbb{P}) $ such that $ \Omega=\{\omega_1,\omega_2,\dots\} $ is countable and $ \mathscr{F} = 2^{\Omega} $. If we know $ \mathbb{P}(\{\omega_i\}) $, then this determines $\bbP$. Indeed, let $ A \subseteq \Omega $, then
\[
    \mathbb{P}(A) = \mathbb{P}\left( \bigcup_{\omega_i\in A}\{\omega_i\} \right) = \sum_{i} \mathbb{P}(\{\omega_i\}).
\]
\begin{definition}
    We write $ p_i = \mathbb{P}(\{\omega_i\}) $ and we call it a \textbf{discrete probability distribution}.
\end{definition}
\begin{proposition}
    $ \forall i,p_i\ge 0 $ and $ \sum_i p_i=1 $.
\end{proposition}

\begin{example}[Bernoulli distribution]
    Model the outcome of the toss of a coin. $ \Omega = \{0,1\} $, $ p_1=p,p_0=1-p $.
\end{example}

\begin{example}[Binomial distribution]
    $ B(N,p),N\in \mathbb{N}, p\in [0,1] $. Toss a $p$-coin $N$ times independently. $ \mathbb{P}(k\text{ heads}) = \binom{N}{k}p^k(1-p)^{N-k} $. Here $ \Omega=\{0,1,\dots,N\},p_k=\binom{N}{k}p^k(1-p)^{N-k} $
\end{example}

\begin{example}[Multinomial distribution]
    $ M(N,p_1,p_2,\dots,p_k), N\in \mathbb{N}$, $p_i\ge 0$,$\sum p_i=1 $. We have $k$ boxes and $N$ balls with $ \mathbb{P}(\text{pick box }i)=p_i $. Partition the balls into the boxes. Here $ \Omega = \{(n_1,\dots,n_k)\in \mathbb{N}^k: \sum n_i=N\} $ and 
    \[
        \mathbb{P}(n_i \text{ in box }i \text{ for } i=1,\dots,k) = \binom{N}{n_1,\dots,n_k}p_1^{n_1}p_2^{n_2}\cdots p_k^{n_k}.
    \]
\end{example}

\begin{example}[Geometric distribution]
    Toss a $p$-coin until the first head appears. $ \Omega=\{1,2,\dots\} $ and
    \[
        p_k=\mathbb{P}(\text{head appears at time }k)=(1-p)^{k-1}p.
    \]
    We can also model this as $ \Omega=\{0,1,\dots\} $ and $ \mathbb{P}(k \text{ tails before first head})=p_k'=(1-p)^{k}p $.
\end{example}

\begin{example}[Poisson distribution]
    Used to model the number of occurences of an event in a given intergal of time. e.g. \# of customers that enter a shop in a day. $ \Omega=\{0,1,2,\dots,\} $ and $ p_k = e^{-\lambda}\frac{\lambda^k}{k!}, k\in \Omega, \lambda>0 $. We write it as $ P(\lambda) $. Note that 
    \[
        \sum_{k=0}^{\infty}p_k=\sum_{k=0}^{\infty} e^{-\lambda}\sum_{k=0}^{\infty}\frac{\lambda^k}{k!}=e^{-\lambda}e^\lambda=1.
    \]
\end{example}

Consider the partition of $[0,1]$ in $n$ intervals of length $1/n$ and in each interval customer arrives with probability $p$ and at most $n$ customers will arrive, then
$$\mathbb P(\text{$k$ customers arrived})=\binom{n}{k}p^k(1-p)^{n-k}$$
which is $B(n,p)$.
But if we take $p=\lambda/n$, we have
\begin{proposition}
    $B(n,\lambda/n)\to P(\lambda)$ as $n\to\infty$.
\end{proposition}
\begin{proof}
    Fix $k$, then
    \begin{align*}
        p_k&=\binom{n}{k}\frac{\lambda^k}{n^k}\left(1-\frac{\lambda}{n}\right)^{n-k}\\
        &=\frac{\lambda^k}{k!}\frac{n!}{n^k(n-k)!}(1+\frac{-\lambda}{n})^{n-k}\\
        &\to\frac{\lambda^k}{k!}e^{-\lambda}
    \end{align*}
    as $n\to\infty$, which is exactly the Poisson distribution.
\end{proof}

\subsection{Random variables}
\begin{definition}
    Let $ (\Omega,\mathscr{F},\mathbb{P}) $ be a probability space. A \textbf{random variable} $X$ is a function $ X: \Omega\to \mathbb{R} $ satisfying 
    \[
        \{X\le x\}=\{\omega\in \Omega: X(\omega)\le x\}\in \mathscr{F},\quad \forall x\in \mathbb{R}.
    \]
    For simplicity, write $ \{X\in A\}:= \{\omega:X(\omega)\in A\} $, where $A \subseteq \bbR$.
\end{definition}

\begin{definition}
    Given $ A\in \mathscr{F} $, define the \textbf{indicator} of $A$ to be 
    \[
        1_A (\omega) = \begin{cases}
        1 &\text{if } \omega\in A\\
        0 &\text{otherwise.}\\
        \end{cases} 
    \]
\end{definition}
We see that $1_A$ is a random variable.

\begin{definition}
    Suppose $X$ is a random variable. Define the \textbf{probability distribution function}(pdf) of $X$ to be 
    \[
        F_X(x) = \mathbb{P}(X\le x),\quad F_X: \mathbb{R} \to [0,1].
    \]
\end{definition}

\begin{definition}
    $ (X_1,\dots,X_n) $ is called a \textbf{random variable} in $ \mathbb{R}^{n} $ if it is a function $\Omega\to \mathbb{R}^{n}$ and for any $ x_1,\dots,x_n\in \mathbb{R}  $ we have 
    \[
        \{X_1\le x_1,\dots,X_n\le x_n\} \in \mathscr{F}.
    \]
\end{definition}

\begin{claim}
    This definition is equivalent to saying that $ X_1,\dots,X_n $ are random variables in $\bbR$.
\end{claim}
\begin{proof}[Indeed,]
    $ \{X_1\le x_1,\dots,X_n\le x_n\} = \{X_1\le x_1\}\cap\dots,\cap \{X_n\le x_n \} \in \mathscr{F}$.
\end{proof}

\begin{definition}
    A r.v. $X$ is called \textbf{discrete} if it takes falues in a \textit{countable} set.
    Suppose $X$ takes values in a countable set $S$. For every $x\in S$ we write
    \[
        p_x=\mathbb{P}(X=x) = \mathbb{P}(\{\omega:X(\omega)=x\}).
    \]
    We call $ (p_x)_{x\in S} $ the \textbf{probability mass function}, of $X$ (pmf) or the \textbf{distribution} of $X$.
\end{definition}

\begin{note}
    If $(p_x)$ is Bernoulli etc., we say that $X$ is a \textit{Bernoulli r.v.} or $X$ has the \textit{Bernoulli distribution}.
\end{note}

\begin{definition}
    Suppose that $X_1,\dots,X_n$ are discrete random variables taking values in $S_1,\dots,S_n$. We say $X_1,\dots,X_n$ are \textbf{independent} if 
    \[
        \mathbb{P}(X_1=x_1,\dots,X_n=x_n)=\mathbb{P}(X_1=x_1)\cdots \mathbb{P}(X_n=x_n), \quad \forall x_i\in S_i.
    \]
\end{definition}

\begin{example}
    Toss a $p$-coin $N$ times independently. Let $H=1,T=0$ and take $ \Omega=\{0,1\}^N $. Consider $ \omega = \{\omega_1,\dots,\omega_N\}\in \Omega $, we have 
    \[
        p_\omega = \prod_{k=1}^{N} p^{\omega_k} (1-p)^{1-\omega_k}.
    \]
    Define $ X_k(\omega)=\omega_k, k=1,\dots,N,\omega\in \Omega $, so $X_k$ is a discrete r.v. and gives the $k$th toss. Now 
    \[
        p_1 = \mathbb{P}(X_k=1) = \mathbb{P}(\omega_k=1)=p\quad \text{and}\quad p_0 = (1-p).
    \]
    Hence $X_k$ is Bernoulli with parameter $p$. Claim that $X_i$ are independent. Indeed, 
    \begin{align*}
        \mathbb{P}(X_1=x_1,\dots,X_N=x_N) &= \mathbb{P}(\omega = (x_1,\dots,x_N)) \\
        &= \prod_{k=1}^{N}p^{x_k} (1-p)^{1-x_k}=\prod_{k=1}^{N}\mathbb{P}(X_k=x_k).
    \end{align*}
\end{example}

Define $ S_N(\omega) = X_1(\omega)+\dots+X_N(\omega) $, \# of heads in $N$ tosses, then $ S_N:\Omega\to \{0,\dots,N\} $ and 
\[
    p_k=\mathbb{P}(S_N=k) = \binom{N}{k} p^k(1-p)^{N-k}
\]
so $S_N\sim B(N,p)$.

\subsection{Expectation}
\begin{definition}
    $ (\Omega,\mathscr{F},\mathbb{P}) $ and assume $\Omega$ is finite or countable. Let $ X:\Omega\to \mathbb{R}  $ be a r.v. so that $X$ is discrete. We say $X$ is \textbf{non-negative} if $ X\ge 0 $. Define the \textbf{expectation} $ \bbE[X] $ as 
    \[
        \bbE[X] = \sum_{\omega\in \Omega} X(\omega) \mathbb{P}(\{\omega\}).
    \]
\end{definition}

Let $ \Omega_X=\{X(\omega):\omega\in \Omega\} $, so $ \Omega = \bigcup_{x\in \Omega_X}\{X=x\} $. Now 
\begin{align*}
    \bbE[X]&=\sum_\omega X(\omega)\bbP(\{\omega\}) = \sum_{x\in \Omega_X} \sum_{\omega\in \{X=x\}}X(\omega)\bbP(\{\omega\})\\ 
    &=\sum_{x\in \Omega_X} \sum_{\omega\in \{X=x\}}x\bbP(\{\omega\})=\sum_{x\in \Omega_X}x \bbP(X=x).
\end{align*}
So the expectation of $X$ (also called average value or mean) is an average of the values taken by $X$ with weights $\bbP(X=x)$. So 
\[
    \bbE[X] = \sum_{x\in \Omega_X}x\cdot p_x.
\]

\begin{example}
    Suppose $ X\sim B(N,p) $.
    \begin{align*}
        \bbE[X]&= \sum_{k=0}^{N} kP(X=k)=\sum_{k=0}^{N}k \binom{N}{k}p^k (1-p)^{N-k}\\ 
        &= \sum_{k=0}^{N}\frac{kN!}{k!(N-k)!}p^k (1-p)^{N-k}\\ 
        &= \sum_{k=1}^{N} \frac{(N-1)!Np}{(k-1)!(N-k)!}p^{k-1}(1-p)^{N-k}\\ 
        &= Np \sum_{k=1}^{N} \binom{N-1}{k-1}p^{k-1}(1-p)^{(N-1)-(k-1)}\\ 
        &= Np.
    \end{align*}
\end{example}

\begin{example}
    Let $ X\sim P(\lambda) $.
    \begin{align*}
        \bbE[X]&=\sum_{k=0}^{\infty} k e^{-\lambda} \frac{\lambda^k}{k!} = \sum_{k=1}^{\infty} e^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!}\lambda\\
        &= \lambda \sum_{k=1}^{\infty} e^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!} = \lambda.
    \end{align*}
\end{example}

\begin{definition}
    Let $X$ be a general discrete r.v. We define $ X_+=\max \{X,0\} $ and $ X_-=\max \{-X,0\} $. Then $X=X_+-X_-$ and $ |X|=X_++X_- $. If at least 1 of $ \bbE[X_+],\bbE[X_-] $ is finite, then we define 
    \[
        \bbE[X] = \bbE[X_+]-\bbE[X_-].
    \]
    If both are infinity, then we say the expectation of $X$ is not defined. Whenever we write $ \bbE[X] $ it is assumed to be well-defined. If $ \bbE[|X|]<\infty  $, we say $X$ is \textbf{integrable}.
\end{definition}
When $ \bbE[X] $ is well-defined, we have again that 
\[
    \bbE[X] = \sum_{x\in \Omega_X}x \mathbb{P}(X=x).
\]

\begin{proposition}\label{prop:properties of expectation}
    \begin{enumerate}
        \item If $ X\ge 0 $, then $ \bbE[X]\ge 0 $.
        \item If $X\ge 0 $ and $ \bbE[X]=0 $ then $ \mathbb{P}(X=0)=1$.
        \item If $ c\in \mathbb{R} $, then $ \bbE[cX]=c\bbE[X] $ and $ \bbE[c+X]=c+\bbE[X] $.
        \item If $ X,Y $ are r.v.s that are integrable, then $ \bbE[X+Y]=\bbE[X]+\bbE[Y] $.
        \item Let $ c_n\dots,c_n\in \mathbb{R}  $ and $ X_1,\dots,X_n $ are r.v.s that are integrable, then 
        \[
            \bbE\left[ \sum_{i=1}^{n}c_iX_i \right] = \sum_{i=1}^{n} c_i\bbE[X_i].
        \]
    \end{enumerate}
\end{proposition}

\subsubsection*{More properties of expectation}
\begin{proposition}
    Suppose $ X_1,X_2,\dots $ are non-negative r.v.s. Then 
    \[
        \bbE \left[ \sum_n X_n \right] = \sum_n \bbE[X_n].
    \]
\end{proposition}
\begin{proof}
    Suppose $\Omega$ is countable. Note that 
    \begin{align*}
        \bbE\left[ \sum_n X_n \right] &= \sum_\omega \sum_n X_n(\omega)\mathbb{P}(\{\omega\})\\ 
        &= \sum_n\sum_\omega X_n(\omega)\mathbb{P}(\{\omega\}) = \sum_n\bbE[X_n].
    \end{align*}
\end{proof}

\begin{proposition}
    If $ X=1_A,A\in \mathscr{F} $, then $ \bbE[X]=\mathbb{P}(A) $.
\end{proposition}
\begin{proposition}
    If $g:\bbR\to \bbR$, define $ g(X) $ to be the r.v. $ g(X)(\omega)=g(X(\omega)) $. Then 
    \[
        \bbE[g(X)] = \sum_{x\in \Omega_X} g(x)\mathbb{P}(X=x).
    \]
\end{proposition}
\begin{proof}
    Set $Y=g(X)$. 
    \[
        \bbE[Y] = y \mathbb{P}(Y=y).
    \]
    Note that $ \{Y=y\} = \{\omega:Y(\omega=y)\} = \{\omega:X(\omega)\in g^{-1}(\{y\})\} = \{X\in g^{-1}(y)\} $. So 
    \begin{align*}
        \bbE[Y]&= \sum_{y\in \Omega_Y}y \mathbb{P}(X\in g^{-1}(\{y\}))=\sum_{y\in \Omega_Y}y \sum_{x\in g^{-1}(\{y\})}\mathbb{P}(X=x)\\
        &= \sum_{y\in \Omega_Y}\sum_{x\in g^{-1}(\{y\})}g(x)\mathbb{P}(X=x)= \sum_{x\in \Omega_X} g(x)\mathbb{P}(X=x).\qedhere
    \end{align*}
\end{proof}

\begin{proposition}
    If $ X\ge 0 $ and takes integer values, then 
    \[
        \bbE[X] = \sum_{k=1}^{\infty} \mathbb{P}(X\ge k) = \sum_{k=0}^{\infty}\mathbb{P}(X>k).
    \]
\end{proposition}
\begin{proof}
    Since $X$ takes non-negative integer values, 
    \[
        X = \sum_{k=1}^{\infty}1(X\ge k) = \sum_{k=0}^{\infty}1(X>k).
    \]
    Note also that 
    \[
        x = \sum_{k=1}^{x}1 = \sum_{k=1}^{\infty} 1(x\ge k),
    \]
    so taking expectation gives 
    \begin{align*}
        \bbE[X] &= \sum_{k=1}^{\infty}\bbE[1(X\ge k)] = \sum_{k=0}^{\infty}\bbE[1(X>k)]\\ 
        &= \sum_{k=1}^{\infty} \mathbb{P}(X\ge k) = \sum_{k=0}^{\infty}\mathbb{P}(X>k).\qedhere
    \end{align*}
\end{proof}

\subsubsection*{Another proof of the inclusion-exclusion formula}
\begin{proposition}[Properties of indicator]
    \begin{itemize}
        \item $ 1(A^\complement) = 1-1(A) $.
        \item $ 1(A \cap B) = 1(A)\cdot 1(B) $.
        \item $ 1(A \cup B) = 1-(1-1(A))(1-1(B)) $. More generally,
        \begin{align*}
            &1(A_1\cup \cdots \cup A_n) = 1-\prod_{i=1}^{n}(1-1(A_i))\\ 
            &= \sum_{i=1}^{n}1(A_i) -\sum_{i_1<i_2} 1(A_{i_1}\cap A_{i_2})+\cdots+(-1)^{n+1}1(A_1\cap \cdots\cap A_n).
        \end{align*}
    \end{itemize}
\end{proposition}
\begin{proof}
    Do it yourself.
\end{proof}
Taking $\bbE$ on both sides we get 
\[
    \mathbb{P}{A_1\cup\cdots\cup A_n} = \sum_{i=1}^{n}\mathbb{P}(A_i)-\sum_{i_1<i_2} \mathbb{P}(A_{i_1}\cap A_{i_2})+\cdots+(-1)^{n+1}\mathbb{P}(A_1\cap\cdots\cap A_n),
\]
which is exactly the inclusion-exclusion formula.

\subsection{Variance}
\begin{definition}
    Let $X$ be a r.v. and $ r\in \mathbb{N} $. We call $ \bbE[X^r] $ (as long as it is well-defined) the \textbf{$r$th moment} of $X$.
\end{definition}
\begin{definition}
    The \textbf{variance} of $X$, denoted as $ \var(X) $, is defined to be 
    \[
        \var(X) = \bbE[(X-\bbE[X])^2].
    \]
    The variance is a measure of how concentrated $X$ is around its expectation.

    Define $ \sigma=\sqrt{\var(X)} $ the \textbf{standard deviation} of $X$.
\end{definition}
\subsubsection*{Properties of variance}
\begin{proposition}
    \begin{enumerate}
        \item $ \var(X)\ge 0 $. If $ \var(X)=0 $ then $ \mathbb{P}(X=\bbE[X])=1 $.
        \item If $c\in \bbR$ then $ \var(cX)=c^2\var(X) $ and $ \var(X+c)=\var(X) $.
        \item $ \var(X)=\bbE[X^2]-(\bbE[X])^2 $.
    \end{enumerate}
\end{proposition}
\begin{proof}
    We only prove 3 since other are trivial. Note that 
    \begin{align*}
        \var(X)&= \bbE[(X-\bbE[x])^2] = \bbE[X^2-2X\bbE[X]+(\bbE[X])^2]\\ 
        &= \bbE[X^2]-2\bbE[X]\bbE[X]+(\bbE[X])^2=\bbE[X^2]-(\bbE[X])^2,
    \end{align*}
    as claimed.
\end{proof}

\begin{proposition}
    $ \displaystyle \var(X) = \min_{c\in \bbR}\bbE[(X-c)^2] $ and the min is achieved for $ c=\bbE[X] $.
\end{proposition}
\begin{proof}
    Let $ f(c)=\bbE[(X-c)^2] = \bbE[X^2]-2c\bbE[X]+c^2 $. Using usual algebras gives the result.
\end{proof}

\begin{example}
    \begin{enumerate}
        \item Suppose $ X\sim B(n,p) $, so $ \bbE[X]=np $.
        \begin{align*}
            \var(X) &= \bbE[X^2]-(\bbE[X])^2 = \bbE[X(X-1)]+\bbE[X]-(\bbE[X])^2\\ 
            &= \sum_{k=2}^{n}k(k-1) \binom{n}{k}p^k (1-p)^{n-k} +np-n^2p^2\\ 
            &= n(n-1)p^2 \sum_{k=2}^{n}\binom{n-2}{k-2}p^{k-2}(1-p)^{n-k}+np-n^2p^2\\ 
            &= n(n-1)p^2+np-n^2p^2\\ 
            &= np(1-p).
        \end{align*}
        \item $ X\sim P(\lambda) $, then 
        \begin{align*}
            \var(X) &= \bbE[X^2]-\lambda^2 = \bbE[X(X-1)]+\bbE[X]-\lambda^2\\ 
            &= \sum_{k=2}^{\infty} k(k-1)e^{-\lambda}\frac{\lambda^k}{k!}+\lambda-\lambda^2\\ 
            &= e^{-\lambda}\lambda^2\sum_{k=2}^{\infty}\frac{\lambda^{k-2}}{(k-2)!}+\lambda-\lambda^2\\ 
            &= \lambda.
        \end{align*}
    \end{enumerate}
\end{example}