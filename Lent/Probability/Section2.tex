\section{Discrete probability distributions}
\subsection{Examples of discrete distributions}
This section we talk about $ (\Omega,\mathscr{F},\mathbb{P}) $ such that $ \Omega=\{\omega_1,\omega_2,\dots\} $ is countable and $ \mathscr{F} = 2^{\Omega} $. If we know $ \mathbb{P}(\{\omega_i\}) $, then this determines $\bbP$. Indeed, let $ A \subseteq \Omega $, then
\[
    \mathbb{P}(A) = \mathbb{P}\left( \bigcup_{\omega_i\in A}\{\omega_i\} \right) = \sum_{i} \mathbb{P}(\{\omega_i\}).
\]
\begin{definition}
    We write $ p_i = \mathbb{P}(\{\omega_i\}) $ and we call it a \textbf{discrete probability distribution}.
\end{definition}
\begin{proposition}
    $ \forall i,p_i\ge 0 $ and $ \sum_i p_i=1 $.
\end{proposition}

\begin{example}[Bernoulli distribution]
    Model the outcome of the toss of a coin. $ \Omega = \{0,1\} $, $ p_1=p,p_0=1-p $.
\end{example}

\begin{example}[Binomial distribution]
    $ B(N,p),N\in \mathbb{N}, p\in [0,1] $. Toss a $p$-coin $N$ times independently. $ \mathbb{P}(k\text{ heads}) = \binom{N}{k}p^k(1-p)^{N-k} $. Here $ \Omega=\{0,1,\dots,N\},p_k=\binom{N}{k}p^k(1-p)^{N-k} $
\end{example}

\begin{example}[Multinomial distribution]
    $ M(N,p_1,p_2,\dots,p_k), N\in \mathbb{N}$, $p_i\ge 0$,$\sum p_i=1 $. We have $k$ boxes and $N$ balls with $ \mathbb{P}(\text{pick box }i)=p_i $. Partition the balls into the boxes. Here $ \Omega = \{(n_1,\dots,n_k)\in \mathbb{N}^k: \sum n_i=N\} $ and 
    \[
        \mathbb{P}(n_i \text{ in box }i \text{ for } i=1,\dots,k) = \binom{N}{n_1,\dots,n_k}p_1^{n_1}p_2^{n_2}\cdots p_k^{n_k}.
    \]
\end{example}

\begin{example}[Geometric distribution]
    Toss a $p$-coin until the first head appears. $ \Omega=\{1,2,\dots\} $ and
    \[
        p_k=\mathbb{P}(\text{head appears at time }k)=(1-p)^{k-1}p.
    \]
    We can also model this as $ \Omega=\{0,1,\dots\} $ and $ \mathbb{P}(k \text{ tails before first head})=p_k'=(1-p)^{k}p $.
\end{example}

\begin{example}[Poisson distribution]
    Used to model the number of occurences of an event in a given interval of time. e.g. \# of customers that enter a shop in a day. $ \Omega=\{0,1,2,\dots,\} $ and $ p_k = e^{-\lambda}\frac{\lambda^k}{k!}, k\in \Omega, \lambda>0 $. We write it as $ P(\lambda) $. Note that 
    \[
        \sum_{k=0}^{\infty}p_k=\sum_{k=0}^{\infty} e^{-\lambda}\sum_{k=0}^{\infty}\frac{\lambda^k}{k!}=e^{-\lambda}e^\lambda=1.
    \]
\end{example}

Consider the partition of $[0,1]$ in $n$ intervals of length $1/n$ and in each interval customer arrives with probability $p$ and at most $n$ customers will arrive, then
$$\mathbb P(\text{$k$ customers arrived})=\binom{n}{k}p^k(1-p)^{n-k}$$
which is $B(n,p)$.
But if we take $p=\lambda/n$, we have
\begin{proposition}
    $B(n,\lambda/n)\to P(\lambda)$ as $n\to\infty$.
\end{proposition}
\begin{proof}
    Fix $k$, then
    \begin{align*}
        p_k&=\binom{n}{k}\frac{\lambda^k}{n^k}\left(1-\frac{\lambda}{n}\right)^{n-k}\\
        &=\frac{\lambda^k}{k!}\frac{n!}{n^k(n-k)!}(1+\frac{-\lambda}{n})^{n-k}\\
        &\to\frac{\lambda^k}{k!}e^{-\lambda}
    \end{align*}
    as $n\to\infty$, which is exactly the Poisson distribution.
\end{proof}

\subsection{Random variables}
\begin{definition}
    Let $ (\Omega,\mathscr{F},\mathbb{P}) $ be a probability space. A \textbf{random variable} $X$ is a function $ X: \Omega\to \mathbb{R} $ satisfying 
    \[
        \{X\le x\}=\{\omega\in \Omega: X(\omega)\le x\}\in \mathscr{F},\quad \forall x\in \mathbb{R}.
    \]
    For simplicity, write $ \{X\in A\}:= \{\omega:X(\omega)\in A\} $, where $A \subseteq \bbR$.
\end{definition}

\begin{definition}
    Given $ A\in \mathscr{F} $, define the \textbf{indicator} of $A$ to be 
    \[
        1_A (\omega) = \begin{cases}
        1 &\text{if } \omega\in A\\
        0 &\text{otherwise.}\\
        \end{cases} 
    \]
\end{definition}
We see that $1_A$ is a random variable.

\begin{definition}
    Suppose $X$ is a random variable. Define the \textbf{probability distribution function}(pdf) of $X$ to be 
    \[
        F_X(x) = \mathbb{P}(X\le x),\quad F_X: \mathbb{R} \to [0,1].
    \]
\end{definition}

\begin{definition}
    $ (X_1,\dots,X_n) $ is called a \textbf{random variable} in $ \mathbb{R}^{n} $ if it is a function $\Omega\to \mathbb{R}^{n}$ and for any $ x_1,\dots,x_n\in \mathbb{R}  $ we have 
    \[
        \{X_1\le x_1,\dots,X_n\le x_n\} \in \mathscr{F}.
    \]
\end{definition}

\begin{claim}
    This definition is equivalent to saying that $ X_1,\dots,X_n $ are random variables in $\bbR$.
\end{claim}
\begin{proof}[Indeed,]
    $ \{X_1\le x_1,\dots,X_n\le x_n\} = \{X_1\le x_1\}\cap\dots,\cap \{X_n\le x_n \} \in \mathscr{F}$.
\end{proof}

\begin{definition}
    A r.v. $X$ is called \textbf{discrete} if it takes values in a \textit{countable} set.
    Suppose $X$ takes values in a countable set $S$. For every $x\in S$ we write
    \[
        p_x=\mathbb{P}(X=x) = \mathbb{P}(\{\omega:X(\omega)=x\}).
    \]
    We call $ (p_x)_{x\in S} $ the \textbf{probability mass function}, of $X$ (pmf) or the \textbf{distribution} of $X$.
\end{definition}

\begin{note}
    If $(p_x)$ is Bernoulli etc., we say that $X$ is a \textit{Bernoulli r.v.} or $X$ has the \textit{Bernoulli distribution}.
\end{note}

\begin{definition}
    Suppose that $X_1,\dots,X_n$ are discrete random variables taking values in $S_1,\dots,S_n$. We say $X_1,\dots,X_n$ are \textbf{independent} if 
    \[
        \mathbb{P}(X_1=x_1,\dots,X_n=x_n)=\mathbb{P}(X_1=x_1)\cdots \mathbb{P}(X_n=x_n), \quad \forall x_i\in S_i.
    \]
\end{definition}

\begin{example}
    Toss a $p$-coin $N$ times independently. Let $H=1,T=0$ and take $ \Omega=\{0,1\}^N $. Consider $ \omega = \{\omega_1,\dots,\omega_N\}\in \Omega $, we have 
    \[
        p_\omega = \prod_{k=1}^{N} p^{\omega_k} (1-p)^{1-\omega_k}.
    \]
    Define $ X_k(\omega)=\omega_k, k=1,\dots,N,\omega\in \Omega $, so $X_k$ is a discrete r.v. and gives the $k$th toss. Now 
    \[
        p_1 = \mathbb{P}(X_k=1) = \mathbb{P}(\omega_k=1)=p\quad \text{and}\quad p_0 = (1-p).
    \]
    Hence $X_k$ is Bernoulli with parameter $p$. Claim that $X_i$ are independent. Indeed, 
    \begin{align*}
        \mathbb{P}(X_1=x_1,\dots,X_N=x_N) &= \mathbb{P}(\omega = (x_1,\dots,x_N)) \\
        &= \prod_{k=1}^{N}p^{x_k} (1-p)^{1-x_k}=\prod_{k=1}^{N}\mathbb{P}(X_k=x_k).
    \end{align*}
\end{example}

Define $ S_N(\omega) = X_1(\omega)+\dots+X_N(\omega) $, \# of heads in $N$ tosses, then $ S_N:\Omega\to \{0,\dots,N\} $ and 
\[
    p_k=\mathbb{P}(S_N=k) = \binom{N}{k} p^k(1-p)^{N-k}
\]
so $S_N\sim B(N,p)$.

\subsection{Expectation}
\begin{definition}
    $ (\Omega,\mathscr{F},\mathbb{P}) $ and assume $\Omega$ is finite or countable. Let $ X:\Omega\to \mathbb{R}  $ be a r.v. so that $X$ is discrete. We say $X$ is \textbf{non-negative} if $ X\ge 0 $. Define the \textbf{expectation} $ \bbE[X] $ as 
    \[
        \bbE[X] = \sum_{\omega\in \Omega} X(\omega) \mathbb{P}(\{\omega\}).
    \]
\end{definition}

Let $ \Omega_X=\{X(\omega):\omega\in \Omega\} $, so $ \Omega = \bigcup_{x\in \Omega_X}\{X=x\} $. Now 
\begin{align*}
    \bbE[X]&=\sum_\omega X(\omega)\bbP(\{\omega\}) = \sum_{x\in \Omega_X} \sum_{\omega\in \{X=x\}}X(\omega)\bbP(\{\omega\})\\ 
    &=\sum_{x\in \Omega_X} \sum_{\omega\in \{X=x\}}x\bbP(\{\omega\})=\sum_{x\in \Omega_X}x \bbP(X=x).
\end{align*}
So the expectation of $X$ (also called average value or mean) is an average of the values taken by $X$ with weights $\bbP(X=x)$. So 
\[
    \bbE[X] = \sum_{x\in \Omega_X}x\cdot p_x.
\]

\begin{example}
    Suppose $ X\sim B(N,p) $.
    \begin{align*}
        \bbE[X]&= \sum_{k=0}^{N} kP(X=k)=\sum_{k=0}^{N}k \binom{N}{k}p^k (1-p)^{N-k}\\ 
        &= \sum_{k=0}^{N}\frac{kN!}{k!(N-k)!}p^k (1-p)^{N-k}\\ 
        &= \sum_{k=1}^{N} \frac{(N-1)!Np}{(k-1)!(N-k)!}p^{k-1}(1-p)^{N-k}\\ 
        &= Np \sum_{k=1}^{N} \binom{N-1}{k-1}p^{k-1}(1-p)^{(N-1)-(k-1)}\\ 
        &= Np.
    \end{align*}
\end{example}

\begin{example}
    Let $ X\sim P(\lambda) $.
    \begin{align*}
        \bbE[X]&=\sum_{k=0}^{\infty} k e^{-\lambda} \frac{\lambda^k}{k!} = \sum_{k=1}^{\infty} e^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!}\lambda\\
        &= \lambda \sum_{k=1}^{\infty} e^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!} = \lambda.
    \end{align*}
\end{example}

\begin{definition}
    Let $X$ be a general discrete r.v. We define $ X_+=\max \{X,0\} $ and $ X_-=\max \{-X,0\} $. Then $X=X_+-X_-$ and $ |X|=X_++X_- $. If at least 1 of $ \bbE[X_+],\bbE[X_-] $ is finite, then we define 
    \[
        \bbE[X] = \bbE[X_+]-\bbE[X_-].
    \]
    If both are infinity, then we say the expectation of $X$ is not defined. Whenever we write $ \bbE[X] $ it is assumed to be well-defined. If $ \bbE[|X|]<\infty  $, we say $X$ is \textbf{integrable}.
\end{definition}
When $ \bbE[X] $ is well-defined, we have again that 
\[
    \bbE[X] = \sum_{x\in \Omega_X}x \mathbb{P}(X=x).
\]

\begin{proposition}\label{prop:properties of expectation}
    \begin{enumerate}
        \item If $ X\ge 0 $, then $ \bbE[X]\ge 0 $.
        \item If $X\ge 0 $ and $ \bbE[X]=0 $ then $ \mathbb{P}(X=0)=1$.
        \item If $ c\in \mathbb{R} $, then $ \bbE[cX]=c\bbE[X] $ and $ \bbE[c+X]=c+\bbE[X] $.
        \item If $ X,Y $ are r.v.s that are integrable, then $ \bbE[X+Y]=\bbE[X]+\bbE[Y] $.
        \item Let $ c_n\dots,c_n\in \mathbb{R}  $ and $ X_1,\dots,X_n $ are r.v.s that are integrable, then 
        \[
            \bbE\left[ \sum_{i=1}^{n}c_iX_i \right] = \sum_{i=1}^{n} c_i\bbE[X_i].
        \]
    \end{enumerate}
\end{proposition}

\subsubsection*{More properties of expectation}
\begin{proposition}
    Suppose $ X_1,X_2,\dots $ are non-negative r.v.s. Then 
    \[
        \bbE \left[ \sum_n X_n \right] = \sum_n \bbE[X_n].
    \]
\end{proposition}
\begin{proof}
    Suppose $\Omega$ is countable. Note that 
    \begin{align*}
        \bbE\left[ \sum_n X_n \right] &= \sum_\omega \sum_n X_n(\omega)\mathbb{P}(\{\omega\})\\ 
        &= \sum_n\sum_\omega X_n(\omega)\mathbb{P}(\{\omega\}) = \sum_n\bbE[X_n].
    \end{align*}
\end{proof}

\begin{proposition}
    If $ X=1_A,A\in \mathscr{F} $, then $ \bbE[X]=\mathbb{P}(A) $.
\end{proposition}
\begin{proposition}
    If $g:\bbR\to \bbR$, define $ g(X) $ to be the r.v. $ g(X)(\omega)=g(X(\omega)) $. Then 
    \[
        \bbE[g(X)] = \sum_{x\in \Omega_X} g(x)\mathbb{P}(X=x).
    \]
\end{proposition}
\begin{proof}
    Set $Y=g(X)$. 
    \[
        \bbE[Y] = y \mathbb{P}(Y=y).
    \]
    Note that $ \{Y=y\} = \{\omega:Y(\omega=y)\} = \{\omega:X(\omega)\in g^{-1}(\{y\})\} = \{X\in g^{-1}(y)\} $. So 
    \begin{align*}
        \bbE[Y]&= \sum_{y\in \Omega_Y}y \mathbb{P}(X\in g^{-1}(\{y\}))=\sum_{y\in \Omega_Y}y \sum_{x\in g^{-1}(\{y\})}\mathbb{P}(X=x)\\
        &= \sum_{y\in \Omega_Y}\sum_{x\in g^{-1}(\{y\})}g(x)\mathbb{P}(X=x)= \sum_{x\in \Omega_X} g(x)\mathbb{P}(X=x).\qedhere
    \end{align*}
\end{proof}

\begin{proposition}
    If $ X\ge 0 $ and takes integer values, then 
    \[
        \bbE[X] = \sum_{k=1}^{\infty} \mathbb{P}(X\ge k) = \sum_{k=0}^{\infty}\mathbb{P}(X>k).
    \]
\end{proposition}
\begin{proof}
    Since $X$ takes non-negative integer values, 
    \[
        X = \sum_{k=1}^{\infty}1(X\ge k) = \sum_{k=0}^{\infty}1(X>k).
    \]
    Note also that 
    \[
        x = \sum_{k=1}^{x}1 = \sum_{k=1}^{\infty} 1(x\ge k),
    \]
    so taking expectation gives 
    \begin{align*}
        \bbE[X] &= \sum_{k=1}^{\infty}\bbE[1(X\ge k)] = \sum_{k=0}^{\infty}\bbE[1(X>k)]\\ 
        &= \sum_{k=1}^{\infty} \mathbb{P}(X\ge k) = \sum_{k=0}^{\infty}\mathbb{P}(X>k).\qedhere
    \end{align*}
\end{proof}

\subsubsection*{Another proof of the inclusion-exclusion formula}
\begin{proposition}[Properties of indicator]
    \begin{itemize}
        \item $ 1(A^\complement) = 1-1(A) $.
        \item $ 1(A \cap B) = 1(A)\cdot 1(B) $.
        \item $ 1(A \cup B) = 1-(1-1(A))(1-1(B)) $. More generally,
        \begin{align*}
            &1(A_1\cup \cdots \cup A_n) = 1-\prod_{i=1}^{n}(1-1(A_i))\\ 
            &= \sum_{i=1}^{n}1(A_i) -\sum_{i_1<i_2} 1(A_{i_1}\cap A_{i_2})+\cdots+(-1)^{n+1}1(A_1\cap \cdots\cap A_n).
        \end{align*}
    \end{itemize}
\end{proposition}
\begin{proof}
    Do it yourself.
\end{proof}
Taking $\bbE$ on both sides we get 
\[
    \mathbb{P}{(A_1\cup\cdots\cup A_n)} = \sum_{i=1}^{n}\mathbb{P}(A_i)-\sum_{i_1<i_2} \mathbb{P}(A_{i_1}\cap A_{i_2})+\cdots+(-1)^{n+1}\mathbb{P}(A_1\cap\cdots\cap A_n),
\]
which is exactly the inclusion-exclusion formula.

\subsection{Variance}
\begin{definition}
    Let $X$ be a r.v. and $ r\in \mathbb{N} $. We call $ \bbE[X^r] $ (as long as it is well-defined) the \textbf{$r$th moment} of $X$.
\end{definition}
\begin{definition}
    The \textbf{variance} of $X$, denoted as $ \var(X) $, is defined to be 
    \[
        \var(X) = \bbE[(X-\bbE[X])^2].
    \]
    The variance is a measure of how concentrated $X$ is around its expectation.

    Define $ \sigma=\sqrt{\var(X)} $ the \textbf{standard deviation} of $X$.
\end{definition}
\subsubsection*{Properties of variance}
\begin{proposition}
    \begin{enumerate}
        \item $ \var(X)\ge 0 $. If $ \var(X)=0 $ then $ \mathbb{P}(X=\bbE[X])=1 $.
        \item If $c\in \bbR$ then $ \var(cX)=c^2\var(X) $ and $ \var(X+c)=\var(X) $.
        \item $ \var(X)=\bbE[X^2]-(\bbE[X])^2 $.
    \end{enumerate}
\end{proposition}
\begin{proof}
    We only prove 3 since other are trivial. Note that 
    \begin{align*}
        \var(X)&= \bbE[(X-\bbE[x])^2] = \bbE[X^2-2X\bbE[X]+(\bbE[X])^2]\\ 
        &= \bbE[X^2]-2\bbE[X]\bbE[X]+(\bbE[X])^2=\bbE[X^2]-(\bbE[X])^2,
    \end{align*}
    as claimed.
\end{proof}

\begin{proposition}
    $ \displaystyle \var(X) = \min_{c\in \bbR}\bbE[(X-c)^2] $ and the min is achieved for $ c=\bbE[X] $.
\end{proposition}
\begin{proof}
    Let $ f(c)=\bbE[(X-c)^2] = \bbE[X^2]-2c\bbE[X]+c^2 $. Using usual algebras gives the result.
\end{proof}

\begin{example}
    \begin{enumerate}
        \item Suppose $ X\sim B(n,p) $, so $ \bbE[X]=np $.
        \begin{align*}
            \var(X) &= \bbE[X^2]-(\bbE[X])^2 = \bbE[X(X-1)]+\bbE[X]-(\bbE[X])^2\\ 
            &= \sum_{k=2}^{n}k(k-1) \binom{n}{k}p^k (1-p)^{n-k} +np-n^2p^2\\ 
            &= n(n-1)p^2 \sum_{k=2}^{n}\binom{n-2}{k-2}p^{k-2}(1-p)^{n-k}+np-n^2p^2\\ 
            &= n(n-1)p^2+np-n^2p^2\\ 
            &= np(1-p).
        \end{align*}
        \item $ X\sim P(\lambda) $, then 
        \begin{align*}
            \var(X) &= \bbE[X^2]-\lambda^2 = \bbE[X(X-1)]+\bbE[X]-\lambda^2\\ 
            &= \sum_{k=2}^{\infty} k(k-1)e^{-\lambda}\frac{\lambda^k}{k!}+\lambda-\lambda^2\\ 
            &= e^{-\lambda}\lambda^2\sum_{k=2}^{\infty}\frac{\lambda^{k-2}}{(k-2)!}+\lambda-\lambda^2\\ 
            &= \lambda.
        \end{align*}
    \end{enumerate}
\end{example}

\subsection{Covariance}
\begin{definition}
    Let $X,Y$ be r.v.s. Their \textbf{covariance} is defined as 
    \[
        \cov (X,Y) = \bbE[(X-\bbE[X])(Y-\bbE[Y])].
    \]
    It is a measure of how dependent $X,Y$ are.
\end{definition}
\begin{proposition}
    \begin{enumerate}
        \item $ \cov(X,Y)=\cov(Y,X) $.
        \item $ \cov(X,X)=\var(X) $.
        \item $ \cov(X,Y) = \bbE[XY]-\bbE[X]\bbE[Y] $.
        \item Let $c\in \bbR$, Then $ \cov(cX,Y) = c\cov(X,Y) $ and $ \cov(c+X,Y)=\cov(X,Y) $.
        \item $ \var(X+Y) = \var(X)+\var(Y)+2\cov(X,Y) $.
        \item $ \forall c\in \mathbb{R}, \cov(c,X)=0 $.
        \item If $ X,Y,Z $ are r.v.s, then 
        \[
            \cov(X+Y,Z) = \cov(X,Z)+\cov(Y,Z).
        \]
        More generally, for $ c_1,\dots,c_n,d_1,\dots,d_n\in \mathbb{R}, X_1,\dots,X_n,Y_1,\dots,Y_n $ r.v.s, we have 
        \[
            \cov \left( \sum_{i=1}^{n}c_i X_i,\sum_{i=1}^{n}d_i Y_i \right) = \sum_{i=1}^{n}\sum_{j=1}^{n}c_i d_j\cov(X_i,Y_j).
        \]
        In particular, 
        \[
            \var\left( \sum_{i=1}^{n}X_i \right) = \sum_{i=1}^{n}\var(X_i)+\sum_{i\neq j} \cov(X_i,X_j).
        \]
    \end{enumerate}
\end{proposition}
\begin{proof}
    Note that 
    \[
        (X-\bbE[X])(Y-\bbE[Y]) = XY-X\bbE[Y]-Y\bbE[X]+\bbE[X]\bbE[Y].\tag{$*$}
    \]
    \begin{enumerate}
        \item It follows by changing the order of $X,Y$ in ($*$).
        \item Replace $Y$ by $X$ in ($*$).
        \item Take expectation on ($*$).
        \item Replace $X$ by $cX,c+X$ and take expectation on ($*$).
        \item Note that 
        \begin{align*}
            \var(X+Y)&= \bbE[(X+Y)^2]-(\bbE[X+Y])^2\\ 
            &= \bbE[X^2]+\bbE[Y^2]+2\bbE[XY]-(\bbE[X]+\bbE[Y])^2\\ 
            &= \var(X)+\var(Y)+2\cov(X,Y).
        \end{align*}
        \item Replace $Y$ with $c$.
        \item By substitution in ($*$) and linearity of $ \bbE $.
    \end{enumerate}
\end{proof}

Recall that $ X\independent Y \Leftrightarrow \mathbb{P}(X=x,Y=y)=\mathbb{P}(X=x)\mathbb{P}(Y=y), \forall x,y $.

\begin{proposition}
    Let $X,Y$ be 2 independent r.v.s and let $ f,g:\mathbb{R} \to \mathbb{R} $ such that
    \[
        \bbE[f(X)g(Y)],\bbE[f(X)],\bbE[g(Y)]
    \]
    are well-defined. Then
    \[
        \bbE[f(X)g(Y)] = \bbE[f(X)]\bbE[g(Y)].
    \]
\end{proposition}
\begin{proof}
    \begin{align*}
        \bbE[f(X)g(Y)]&= \sum_{x,y} f(x)g(y) \mathbb{P}(X=x,Y=y)\\ 
        &= \sum_{x,y} f(x)g(y) \mathbb{P}(X=x)\mathbb{P}(Y=y)\\ 
        &= \sum_{x} f(x)\mathbb{P}(X=x) \sum_y g(y)\mathbb{P}(Y=y)\\ 
        &= \bbE[f(X)]\bbE[g(Y)].\qedhere
    \end{align*}
\end{proof}

\begin{proposition}
    If $ X \independent Y $, then $ \cov(X,Y)=0 $.
\end{proposition}
\begin{proof}
    \begin{align*}
        \cov(X,Y) &= \bbE[(X-\bbE[X])(Y-\bbE[Y])] \\
        &= \bbE[(X-\bbE[X])]\bbE[(Y-\bbE[Y])]\\ 
        &= 0.\qedhere
    \end{align*}
\end{proof}
\begin{note}
    If $ \cov(X,Y)=0 $, it is not generally true that $ X \independent Y $. Let $ X_1,X_2,X_3 $ be independent r.v.s such that $ X_i\sim B(1/2) $. Define 
    \[
        \begin{aligned}
            Y_1 &= 2X_1-1,\  Y_2 = 2X_2-1,\\ 
            Z_1&=X_3Y_1,\ Z_2 =X_3Y_2.
        \end{aligned}
    \]
    We see that $ \bbE[Y_1]=\bbE[Y_2]=\bbE[Z_1]=\bbE[Z_2]=0 $, and $ \cov(Z_1,Z_2) = \bbE[Z_1Z_2] = \bbE[X_3^2Y_1Y_2]=0 $. However, $ Z_1,Z_2 $ are not independent since 
    \[
        \mathbb{P}(Z_1=0,Z_2=0) = \mathbb{P}(Z_3=0) = \frac{1}{2}
    \]
    but 
    \[
        \mathbb{P}(Z_1=0)\mathbb{P}(Z_2=0) = \mathbb{P}(X_3=0)^2=\frac{1}{4}.
    \]
\end{note}
\begin{corollary}
    If $ X,Y $ are independent, then $ \var(X+Y) = \var(X)+\var(Y) $.
\end{corollary}

\subsection{Inequalities}
\begin{proposition}[Markov's inequality]\label{prop:Markov's inequality}
    Suppose $ X\ge 0 $ is a r.v, then $ \forall a>0 $, 
    \[
        \mathbb{P}(X\ge a) \le \frac{\bbE[X]}{a}.
    \]
\end{proposition}
\begin{proof}
    Observe that $ X\ge a \cdot 1(X\ge a) $. Taking expectations get 
    \[
        \bbE[X]\ge \bbE[a \cdot 1(X\ge a)] = a \mathbb{P}(X\ge a) \Longrightarrow \mathbb{P}(X\ge a)\le \frac{\bbE[X]}{a}.\qedhere
    \]
\end{proof}
\begin{proposition}[Chebyshev's inequality]\label{prop:Chebyshev's inequality}
    Let $X$ be a r.v. with $ \bbE[X]<\infty  $. Then $ \forall a>0 $,
    \[
        \mathbb{P}(|X-\bbE[X]|\ge a)\le \frac{\var(X)}{a^2}.
    \]
\end{proposition}
\begin{proof}
    \begin{align*}
        \mathbb{P}(|X-\bbE[X]|\ge a) &= \mathbb{P}(|X-\bbE[X]|^2\ge a^2)\\ 
        &\le \frac{\bbE\left[ |X-\bbE[X]|^2 \right]}{a^2} = \frac{\var(X)}{a^2}.\qedhere
    \end{align*}
\end{proof}

\begin{proposition}[Cauchy-Schwartz inequality]\label{prop:Cauchy-Schwartz inequality}
    If $X,Y$ are r.v.s, then 
    \[
        \bbE[|XY|]\le \sqrt{\bbE[X^2]\bbE[Y^2]}.
    \]
\end{proposition}
\begin{proof}
    It suffice to prove it for $ \bbE[X^2],\bbE[X^2]<\infty $ and $ X,Y\ge 0 $. Note that 
    \[
        XY\le \frac{1}{2}(X^2+Y^2) \Longrightarrow \bbE[XY] = \frac{1}{2}\left( \bbE[X^2]+\bbE[Y^2] \right)<\infty .
    \]
    Assume $ \bbE[X^2],\bbE[Y^2]>0 $, otherwise the result it trivial since either $ \bbE[X^2],\bbE[Y^2]=0 $ inplies $XY\equiv 0$. Let $t\in \bbR$ and consider 
    \begin{align*}
        &\ 0\le (X-tY^2) = X^2-2tXY-t^2Y^2\\ 
        \Longrightarrow &\  f(t) = \bbE[X^2]-2t\bbE[XY]+t^2\bbE[Y^2]\ge 0\\ 
        \Longrightarrow &\  \min f(t)\quad\text{for}\quad t_0 = \frac{\bbE[XY]}{\bbE[Y^2]} \\ 
        \Longrightarrow &\  f(t_0)=\bbE[X^2]-\frac{2\bbE[XY]^2}{\bbE[Y^2]}+\frac{\bbE[XY]^2}{\bbE[Y^2]}\ge 0\\ 
        \Longrightarrow &\  \bbE[XY]^2\le \bbE[X^2]\bbE[Y^2]\\ 
        \Longrightarrow &\  \bbE[|XY|]\le \sqrt{\bbE[X^2]\bbE[Y^2]},
    \end{align*}
    as claimed.
\end{proof}
\begin{remark}
    Equality occurs when $ \bbE[(X-tY)^2]=0 $ for $ t=\bbE[XY]/\bbE[Y^2] $. Notice that 
    \[
        \bbE[(X-tY)^2]=0 \Longrightarrow \mathbb{P}(X=tY)=1.
    \]
\end{remark}
\begin{definition}[\href{https://www.princeton.edu/~aaa/Public/Teaching/ORF523/S16/ORF523_S16_Lec7_gh.pdf}{Convex functions}]
    A function $ f:\mathbb{R} \to \mathbb{R} $ is \textbf{convex} if $ \forall x,y\in \mathbb{R}  $ and $ \forall t\in [0,1] $,
    \[
        f(tx+(1-t)y)\le tf(x)+(1-t)f(y).
    \]
\end{definition}
\begin{lemma}\label{lma:Jensen}
    Let $ f:\mathbb{R} \to \mathbb{R}  $ be convex, then $f$ is the supremum of all the lines lying below it. In other words,
    \[
        \forall m\in \mathbb{R}, \exists a,b\in \mathbb{R}, f(m)=am+b \land f(x)\ge ax+b,\forall x.
    \]
\end{lemma}
\begin{proof}
    Let $m\in \bbR$ and $ x<m<y $. Then $ \exists t\in [0,1], m=tx+(1-t)y $. By convexity, $ f(m)=tf(m)+(1-t)f(m)\le tf(x)+(1-t)f(y) $. Therefore 
    \[
        t(f(m)-f(x))\le (1-t)(f(y)-f(m)) \Longrightarrow \frac{f(m)-f(x)}{m-x}\le \frac{f(y)-f(m)}{y-m}.
    \]
    So there exists
    \[
        a= \sup_{x<m} \frac{f(m)-f(x)}{m-x},  \ \frac{f(m)-f(x)}{m-x}\le a\le \frac{f(y)-f(m)}{y-m},\ \forall x<m<y.
    \]
    Rearranging gives 
    \[
        f(x)\ge a(x \underbrace{-m)+f(m)}_{b},
    \]
    as claimed.
\end{proof}
\begin{proposition}[Jensen's inequality]\label{prop:Jensen's inequality}
    Let $X$ be a r.v. and let $f$ be a convex function. Then 
    \[
        \bbE[f(X)]\ge f(\bbE[X]).
    \]
\end{proposition}
\begin{note}
    Notice that $ \var(X)=\bbE[(X-\bbE[X])^2]=\bbE[X^2]-(\bbE[X])^2\ge 0 $, so $ \bbE[X^2]\ge (\bbE[X])^2 $. This is a way to remember the direction of the inequality.
\end{note}
\begin{proof}
    Set $ m=\bbE[X] $ in lemma \ref{lma:Jensen}. Then $ \exists a,b\in \mathbb{R} $, 
    \[
        f(m)=am+b \Longleftrightarrow f(\bbE[X])=a\bbE[X]+b
    \]
    and $ f(X)\ge aX+b $. Taking expectation we get 
    \[
        \bbE[f(X)]\ge a\bbE[X]+b=f(\bbE[X]).\qedhere
    \]
\end{proof}
\begin{remark}
    The equality holds for $ X\equiv \bbE[X] $ (check it!).
\end{remark}
\begin{corollary}
    Let $f$ be a convex function and let $ x_1,\dots,x_n\in \mathbb{R}  $. Then 
    \[
        \frac{1}{n}\sum_{k=1}^{n} f(x_k)\ge f\left( \frac{1}{n}\sum_{k=1}^{n}x_k \right).
    \]
\end{corollary}
\begin{proof}
    Define $X$ to be r.v. taking $ \{x_1,\dots,x_n\} $ with equal probability, and use Jensen's inequality.
\end{proof}
\begin{corollary}[AM-GM inequality]\label{prop:AM-GM inequality}
    Let $ x_1,\dots,x_n\in \mathbb{R}  $, then 
    \[
        \left( \prod_{k=1}^{n} x_k\right)^{\frac1n}\le \frac{1}{n}\sum_{k=1}^{n}x_k.
    \]
\end{corollary}
\begin{proof}
    Take $f(x)=-\log x$.
\end{proof}
\subsection{Conditional expectation}
\begin{definition}[Conditional expectation]
    Let $B\in \mathscr{F},\bbP(B)>0$ and let $ X $ be a r.v. Define the \textbf{conditional expectation} as
    \[
        \bbE[X|B] = \frac{\bbE[X\cdot 1(B)]}{\bbP(B)}.
    \]
\end{definition}
\begin{proposition}[Law of total expectation]\label{prop:Law of total expectation}
    Suppose $X\ge 0$ and let $ (\Omega_n) $ be a partition of $\Omega$ into disjoint events. Then 
    \[
        \bbE[X] = \sum_{n} \bbE[X|\Omega_n]\mathbb{P}(\Omega_n).
    \]
\end{proposition}
\begin{proof}
    Note that $ X=X \cdot 1(\Omega)=\sum_{n} X \cdot 1(\Omega_n) $. Taking expectation gives
    \[
    \bbE[X] = \bbE\left[ \sum_{n} X \cdot 1(\Omega_n) \right] = \sum_{n}\bbE[X\cdot 1(\Omega_n)]=\sum_{n} \bbE[X|\Omega_n]\mathbb{P}(\Omega_n),
    \]
    as claimed.
\end{proof}

\begin{definition}
    Let $ X_1,\dots,X_n $ be discrete r.v.s. Their \textbf{joint distribution} is defined to be 
    \[
        \mathbb{P}(X_1=x_1,\dots,X_n=x_n), \quad \forall x_i\in \Omega_{X_i}.
    \]
\end{definition}

Note that 
\begin{align*}
    \mathbb{P}(X_1=x_1) &= \mathbb{P}\left( \{X_1=x_1\} \cap \left( \bigcup_{i=2}^{n} \bigcup_{x_i} \{X_i=x_i\} \right) \right)\\ 
    &= \sum_{x_2,\dots,x_n} \mathbb{P}(X_1=x_1,\dots,X_n=x_n),
\end{align*}
and similarly 
\begin{align*}
    \mathbb{P}(X_i=x_i) &= \mathbb{P}\left( \{X_1=x_1\} \cap \left( \bigcup_{i=2}^{n} \bigcup_{x_i} \{X_i=x_i\} \right) \right)\\ 
    &= \sum_{\{x_j\}_{j=1}^n\setminus\{x_i\}} \mathbb{P}(X_1=x_1,\dots,X_n=x_n).
\end{align*}
\begin{definition}
    Call $ (\mathbb{P}(X_i=x_i) )_{x_i}$ then \textbf{marginal distribution} of $X_i$.
\end{definition}

\begin{definition}
    Let $X,Y$ be 2 r.v.s. The \textbf{conditional distribution} of $X$ given $ Y=y\ (y\in \Omega_Y) $ is defined to be 
    \[
        \mathbb{P}(X=x|Y=y),\quad x\in \Omega_X.
    \]
\end{definition}
Note that 
\[
    \mathbb{P}(X=x) = \sum_{y} \mathbb{P}(X=x,Y=y) = \sum_{y} \mathbb{P}(X=x|Y=x)\mathbb{P}(Y=y),
\]
which is the law of total probability.
\subsubsection*{Distribution of the sum of independent r.v.s}
Let $ X,Y $ be 2 independent discrete r.v.s. Consider 
\begin{align*}
    \mathbb{P}(X+Y=z) &= \sum_{y} \mathbb{P}(X+y=z,Y=y)\\ 
    &= \sum_{y} \mathbb{P}(X=z-y,Y=y)\\ 
    &= \sum_{y} \mathbb{P}(X=z-y)\mathbb{P}(Y=y).
\end{align*}
This last sum is called the \textbf{convolution} of the distributions $X,Y$.

Similarly,
\[
    \mathbb{P}(X+Y=z)=\sum_{x} \mathbb{P}(X=x)\mathbb{P}(Y=z-x).
\]
\begin{example}
    Let $ X \sim P(\lambda) $ and $ Y \sim P(\mu) $ be independent.
    \begin{align*}
        \mathbb{P}(X+Y=n)&= \sum_{r=0}^{n}\mathbb{P}(X=r)\mathbb{P}(Y=n-r)\\ 
        &= \sum_{r=0}^{n}e^{-\lambda}\cdot \frac{\lambda^r}{r!}\cdot e^{-\mu}\frac{\mu^{n-r}}{(n-r)!}\\ 
        &= \frac{e^{-(\lambda+\mu)}}{n!}\sum_{r=0}^{n} \binom{n}{r} \lambda^r \mu^{n-r}\\ 
        &= \frac{(\lambda+\mu)^n}{n!}e^{-(\lambda+\mu)},
    \end{align*}
    so $ X+Y \sim P(\lambda+\mu) $.
\end{example}
\begin{definition}
    Let $ X,Y $ be 2 discrete r.v.s. The \textbf{conditional expectation} of $X$ given $Y=y$ is $ \bbE[X|Y=y]=\bbE[X\cdot 1(Y=y)]/\mathbb{P}(Y=y) $.
\end{definition}
Note that 
\begin{align*}
    \bbE[X|Y=y] &= \frac{\bbE[X\cdot 1(Y=y)]}{\mathbb{P}(Y=y)}\\
     &= \frac{1}{\mathbb{P}(Y=y)}\sum_{x} x \cdot \mathbb{P}(X=x,Y=y)
     \\ &= \sum_{x} x \cdot \mathbb{P}(X=x|Y=y).
\end{align*}
We observe that for every $y\in \Omega_Y$, $\bbE[X|Y=y]$ is a function of $y$ only. Set $ g(y)=\bbE[X|Y=y] $.

\begin{definition}
    Define the conditional expectation of $X$ given $Y$, written as $ \bbE[X|Y] $ for the random variable $g(Y)$.
\end{definition}
\begin{remark}
    $ \bbE[X|Y] $ is a \textit{random variable} and depends only on $y$.
\end{remark}
Now 
\begin{align*}
    \bbE[X|Y] &= g(Y)\cdot 1 = g(Y) \sum_{y} 1(Y=y) = \sum_{y} g(Y)1(Y=y)\\ 
    &= \sum_{y} g(y)1(Y=y)=\sum_{y} \bbE[X|Y=y]1(Y=y).
\end{align*}
\begin{example}\label{eg:2.11}
    Toss a $p$-coin $n$ times independently. Write 
    \[
        X_i = 1(i\text{th toss is a H}),\quad i=1,\dots,n,\quad Y_n=X_1+\cdots+X_n.
    \]
    What is $ \bbE[X_1|Y] $?

    Set $ g(y)=\bbE[X_1|Y_n=y] $, then $g(Y_n)=\bbE[X_1|Y_n]$. Let $ y\in \{0,\dots,n\} $. Then $g(y) = \bbE[X_1|Y_n=y] = \mathbb{P}(X_1=1|Y_n=y).$ 

    If $ y=0 $, $\mathbb{P}(X_1=1|Y_n=0)=0$. If $ y\neq 0 $, we have 
    \begin{align*}
        \mathbb{P}(X_1=1|Y_n=y) &= \frac{\bbP(X_1=1,Y_n=y)}{\bbP(Y_n=y)}\\ 
        &= \frac{\bbP(X_1=1,X_2+\cdots+X_n=y-1)}{\bbP(Y_n=y)}.
    \end{align*}
    Since $X_i$ are iid, we have 
    \begin{align*}
        \mathbb{P}(X_1=1,X_2+\cdots+X_n=y-1)&= \mathbb{P}(X_1=1)\cdot \mathbb{P}(X_2+\cdots+X_n=y-1)\\ 
        &= p \cdot \binom{n-1}{y-1}p^{y-1}(1-p)^{n-y}.
    \end{align*}
    On the other hand, $ \mathbb{P}(Y_n=y) = \binom{n}{y}p^{y}(1-p)^{n-y} $, so 
    \[
        \mathbb{P}(X_1=1|Y_n=y) = \frac{p \cdot \binom{n-1}{y-1}p^{y-1}(1-p)^{n-y}}{\binom{n}{y}p^{y}(1-p)^{n-y}} = \frac{y}{n} = g(y).
    \]
    Therefore 
    \[
        \bbE[X_1|Y_n] = g(Y_n) = \frac{Y_n}{n}.
    \]
\end{example}
\subsubsection*{Properties of conditional expectation}
\begin{proposition}\label{prop:Properties of conditional expectation}\ 
    \begin{itemize}
        \item $ \forall c\in \mathbb{R},\ \bbE[cX|Y]=c \cdot [X|Y] $ and $ \bbE[c|Y]=c $.
        \item If $ X_1,\dots,X_n $ are r.v.s, then 
        \[
            \bbE\left[ \sum_{i=1}^{n}X_i\Big|Y \right] = \sum_{i=1}^{n} \bbE[X_i|Y].
        \]
        \item $ \bbE[\bbE[X|Y]]=\bbE[X] $.
        \item Let $X,Y$ be 2 independent r.v.s, then $ \bbE[X|Y]=\bbE[X] $.
    \end{itemize}
\end{proposition}
\begin{proof}[Proof of 3]
    Note that $ \bbE[X|Y]=\sum_{y} 1(Y=y)\bbE[X|Y=y] $. Taking expectation gives 
    \begin{align*}
        \bbE[\bbE[X|Y]]&= \sum_{y} \bbE[X|Y=y] \cdot \bbE[1(Y=y)]\\
        &= \sum_{y} \bbE[X|Y=y] \bbP(Y=y) = \sum_{y} \frac{\bbE[X\cdot 1(Y=y)]}{\bbP(Y=y)}\bbP(Y=y)\\ 
        &= \sum_{y} \bbE[X\cdot 1(Y=y)] = \bbE\left[ X\cdot \sum_{y} 1(Y=y) \right] = \bbE[X].
    \end{align*}
    Alternatively,
    \[
        \sum_{y} \bbE[X|Y=y] \bbP(Y=y) = \sum_{x,y} x\bbP(X=x|Y=y)\mathbb{P}(Y=y)=\bbE[X].\qedhere
    \]
\end{proof}
\begin{proof}[Proof of 4]
    \begin{align*}
        \bbE[X|Y]&= \sum_{y} 1(Y=y) \cdot \bbE[X|Y=y]\\ 
        &= \sum_{y} 1(Y=y) \sum_{x} x \cdot \bbP(X=x|Y=y)\\
        &= \sum_{y} 1(Y=y) \sum_{x} x \cdot \mathbb{P}(X=x) = \bbE[X].\qedhere
    \end{align*}
\end{proof}
\begin{lemma}\label{lma:2.25}
    If $ Y \independent Z $, then for any function $f$ we have $ f(Y) \independent Z $.
\end{lemma}
\begin{proof}[Indeed,]
    for any function $f$, we have
    \begin{align*}
        \mathbb{P}(f(Y)=r,Z=z)&= \mathbb{P}(Y\in f^{-1}(\{r\})), Z=z)\\ 
        &= \sum_{y\in f^{-1}(\{r\})} \mathbb{P}(Y=y,Z=z)\\ 
        &= \sum_{y\in f^{-1}(\{r\})} \mathbb{P}(Y=y) \mathbb{P}(Z=z)\\ 
        &= \mathbb{P}(f(Y)=r)\mathbb{P}(Z=z).\qedhere
    \end{align*}
\end{proof}
\begin{proposition}
    Suppose $Y,Z$ are independent r.v.s, then 
    \[
        \bbE[\bbE[X|Y]|Z] = \bbE[X].
    \]
\end{proposition}
\begin{proof}
    We have $ \bbE[X|Y] = g(Y) $, where $g(Y)$ is defined before. If $ Y \independent Z $, then $ g(Y) \independent Z $ by lemma \ref{lma:2.25}. From proposition \ref{prop:Properties of conditional expectation}, 
    \[
        \bbE[g(Y)|Z] = \bbE[g(Y)] = \bbE[\bbE[X|Y]] = \bbE[X].\qedhere
    \]
\end{proof}

\begin{proposition}
    Suppose $h:\bbR\to\bbR$, then 
    \[
        \bbE[h(Y) \cdot X|Y] = h(Y) \cdot \bbE[X|Y].
    \]
\end{proposition}
\begin{proof}
    Note that 
    \[
        \bbE[h(Y)\cdot X|Y=y]=\bbE[h(y)\cdot X|Y=y] = h(y)\bbE[X|Y=y],
    \]
    so plugging in $Y$ gives the result.
\end{proof}
\begin{corollary}
    $ \bbE[\bbE[X|Y]|Y]=\bbE[X|Y] $ and $ \bbE[X|X]=X $.
\end{corollary}

\begin{example}
    Refer to example \ref{eg:2.11}. We can prove it by the properties now: By symmetry for all $i$, 
    \[
        \bbE[X_i|Y_n]=\bbE[X_1|Y_n].
    \]
    Now note that 
    \[
        \bbE\left[ \sum_{i=1}^{n}X_i\Big|Y_n \right] = \sum_{i=1}^{n}\bbE[X_i|Y_n] = n \bbE[X_1|Y_n].
    \]
    Since $ \sum X_i = Y_n $, we have 
    \[
        \bbE[X_1|Y_n] = \frac{1}{n}\bbE[Y_n|Y_n] = \frac{Y_n}{n}.\qedhere
    \]
\end{example}

\subsection{Random walks}
\begin{definition}
    A \textbf{random process}(or stochasic process) is a sequence of random variables $ (X_n)_{n\in \mathbb{N}} $.
\end{definition}
\begin{definition}
    A \textbf{random walk} is a random process that can be expressed in the following way: 
    \[
        X_n = x+Y_1+\cdots+Y_n,
    \]
    where $Y_i$ are iid rvs and $x$ is a deterministic number.
\end{definition}
\begin{definition}
    A random walk on $\bbZ$ is defined by 
    \[
        \bbP(Y_i=1) = p,\quad \mathbb{P}(Y_i=-1)=q=1-p.
    \]
    The random walk is \textbf{symmetric} if $p=q=1/2$.
\end{definition}

\begin{notation}
    We write $ \mathbb{P}_x $ for the probability measure $ \mathbb{P}(\cdot | X_0=x) $, i.e. for $ A\in \mathscr{F}, \mathbb{P}_x(A) = \mathbb{P}(A|X_0=x) $. Also write $ \bbE_x $ for $ \bbE[\cdot|X_0=x] $.
\end{notation}

We can think of $X_n$ as the fortune of a gambler who bets 1 at every step and either doubles it with probability $p$ or loses it with probability $q$.
\begin{example}[Gambler's ruin estimate]
    Suppose the gambler starts with \pounds$ x $ at time 0. What is the probability he reaches $a$ before going bankrupt?

    Define $ h(x) = \mathbb{P}_x(X_n \text{ hits }a \text{ before hitting }0) $. By the law of total probability, we have
    \begin{align*}
        h(x)&=\bbP_x(\cdots|Y_1=1)\mathbb{P}(Y_1=1)+\mathbb{P}_x(\cdots|Y_1=-1)\mathbb{P}(Y_1=-1)\\ 
       &= p h(x+1)+q h(x-1).
    \end{align*}
    Note that $ h(0)=0,h(a)=1$.
    \begin{itemize}
        \item If $ p=q=1/2 $, then $ h(x)-h(x+1)=h(x-1)-h(x) $, so $ h(x) = x/a $.
        \item If $ p\neq q $, the characteristic equation is $ p\lambda^2-\lambda+q=0 \Rightarrow \lambda=1,\frac{q}{p} $. So the general solution is 
        \[
            h(x) = A+B\left( \frac{q}{p} \right)^n = \frac{\left( \frac{q}{p} \right)^x-1}{\left( \frac{q}{p} \right)^a-1}.
        \]
    \end{itemize}
\end{example}
\begin{example}[Expected time to absorption]
    Define $ T =\min \{n\ge 0: X_n\in \{0,a\}\} $. i.e. $T$ is the \textit{first} time $X$ hits $0$ or $a$. What is $ \tau_x = \bbE_x[T] $? Here $ 0<x<a $.

    By the law of total expectation\footnote{We are calculating the \textit{time} it takes, so we have to take $Y_1$ into account.},
    \begin{align*}
        \tau_x &= p\bbE_x[T|Y_1=1]+q\bbE_x[T|Y_1=-1]\\ 
        &= p(1+\bbE_{x+1}[T])+q(1+\bbE_{x-1}[T])\\ 
        &= 1+ p \tau_{x+1}+q \tau_{x-1}
    \end{align*}
    with I.C.s $ \tau_0=\tau_a=0 $.
    \begin{itemize}
        \item If $ p=q=1/2 $, trying solution of the form $ Ax^2 $ gives $A=-1$, and thus the general solution is 
        \[
            \tau_x = Ax^2+Bx+C = x(a-x).
        \]
        \item If $ p\neq q $, trying particular solution $ Cx $ gives $ C=\frac{1}{q-p} $ and (by previous homogeneous case) the general solution is 
        \[
            \tau_x = \frac{x}{q-p}+A+B\left( \frac{q}{p} \right)^x = \frac{x}{q-p}-\frac{q}{q-p}\frac{\left( \frac{q}{p} \right)^x-1}{\left( \frac{q}{p} \right)^a-1}.
        \]
    \end{itemize}
\end{example}