\section{Discrete probability distributions}
\subsection{Examples of discrete distributions}
This section we talk about $ (\Omega,\mathscr{F},\mathbb{P}) $ such that $ \Omega=\{\omega_1,\omega_2,\dots\} $ is countable and $ \mathscr{F} = 2^{\Omega} $. If we know $ \mathbb{P}(\{\omega_i\}) $, then this determines $\bbP$. Indeed, let $ A \subseteq \Omega $, then
\[
    \mathbb{P}(A) = \mathbb{P}\left( \bigcup_{\omega_i\in A}\{\omega_i\} \right) = \sum_{i} \mathbb{P}(\{\omega_i\}).
\]
\begin{definition}
    We write $ p_i = \mathbb{P}(\{\omega_i\}) $ and we call it a \textbf{discrete probability distribution}.
\end{definition}
\begin{proposition}
    $ \forall i,p_i\ge 0 $ and $ \sum_i p_i=1 $.
\end{proposition}

\begin{example}[Bernoulli distribution]
    Model the outcome of the toss of a coin. $ \Omega = \{0,1\} $, $ p_1=p,p_0=1-p $.
\end{example}

\begin{example}[Binomial distribution]
    $ B(N,p),N\in \mathbb{N}, p\in [0,1] $. Toss a $p$-coin $N$ times independently. $ \mathbb{P}(k\text{ heads}) = \binom{N}{k}p^k(1-p)^{N-k} $. Here $ \Omega=\{0,1,\dots,N\},p_k=\binom{N}{k}p^k(1-p)^{N-k} $
\end{example}

\begin{example}[Multinomial distribution]
    $ M(N,p_1,p_2,\dots,p_k), N\in \mathbb{N}$, $p_i\ge 0$,$\sum p_i=1 $. We have $k$ boxes and $N$ balls with $ \mathbb{P}(\text{pick box }i)=p_i $. Partition the balls into the boxes. Here $ \Omega = \{(n_1,\dots,n_k)\in \mathbb{N}^k: \sum n_i=N\} $ and 
    \[
        \mathbb{P}(n_i \text{ in box }i \text{ for } i=1,\dots,k) = \binom{N}{n_1,\dots,n_k}p_1^{n_1}p_2^{n_2}\cdots p_k^{n_k}.
    \]
\end{example}

\begin{example}[Geometric distribution]
    Toss a $p$-coin until the first head appears. $ \Omega=\{1,2,\dots\} $ and
    \[
        p_k=\mathbb{P}(\text{head appears at time }k)=(1-p)^{k-1}p.
    \]
    We can also model this as $ \Omega=\{0,1,\dots\} $ and $ \mathbb{P}(k \text{ tails before first head})=p_k'=(1-p)^{k}p $.
\end{example}

\begin{example}[Poisson distribution]
    Used to model the number of occurences of an event in a given interval of time. e.g. \# of customers that enter a shop in a day. $ \Omega=\{0,1,2,\dots,\} $ and $ p_k = e^{-\lambda}\frac{\lambda^k}{k!}, k\in \Omega, \lambda>0 $. We write it as $ P(\lambda) $. Note that 
    \[
        \sum_{k=0}^{\infty}p_k=\sum_{k=0}^{\infty} e^{-\lambda}\sum_{k=0}^{\infty}\frac{\lambda^k}{k!}=e^{-\lambda}e^\lambda=1.
    \]
\end{example}

Consider the partition of $[0,1]$ in $n$ intervals of length $1/n$ and in each interval customer arrives with probability $p$ and at most $n$ customers will arrive, then
$$\mathbb P(\text{$k$ customers arrived})=\binom{n}{k}p^k(1-p)^{n-k}$$
which is $B(n,p)$.
But if we take $p=\lambda/n$, we have
\begin{proposition}
    $B(n,\lambda/n)\to P(\lambda)$ as $n\to\infty$.
\end{proposition}
\begin{proof}
    Fix $k$, then
    \begin{align*}
        p_k&=\binom{n}{k}\frac{\lambda^k}{n^k}\left(1-\frac{\lambda}{n}\right)^{n-k}\\
        &=\frac{\lambda^k}{k!}\frac{n!}{n^k(n-k)!}(1+\frac{-\lambda}{n})^{n-k}\\
        &\to\frac{\lambda^k}{k!}e^{-\lambda}
    \end{align*}
    as $n\to\infty$, which is exactly the Poisson distribution.
\end{proof}

\subsection{Random variables}
\begin{definition}
    Let $ (\Omega,\mathscr{F},\mathbb{P}) $ be a probability space. A \textbf{random variable} $X$ is a function $ X: \Omega\to \mathbb{R} $ satisfying 
    \[
        \{X\le x\}=\{\omega\in \Omega: X(\omega)\le x\}\in \mathscr{F},\quad \forall x\in \mathbb{R}.
    \]
    For simplicity, write $ \{X\in A\}:= \{\omega:X(\omega)\in A\} $, where $A \subseteq \bbR$.
\end{definition}

\begin{definition}
    Given $ A\in \mathscr{F} $, define the \textbf{indicator} of $A$ to be 
    \[
        1_A (\omega) = \begin{cases}
        1 &\text{if } \omega\in A\\
        0 &\text{otherwise.}\\
        \end{cases} 
    \]
\end{definition}
We see that $1_A$ is a random variable.

\begin{definition}
    Suppose $X$ is a random variable. Define the \textbf{probability distribution function}(pdf) of $X$ to be 
    \[
        F_X(x) = \mathbb{P}(X\le x),\quad F_X: \mathbb{R} \to [0,1].
    \]
\end{definition}

\begin{definition}
    $ (X_1,\dots,X_n) $ is called a \textbf{random variable} in $ \mathbb{R}^{n} $ if it is a function $\Omega\to \mathbb{R}^{n}$ and for any $ x_1,\dots,x_n\in \mathbb{R}  $ we have 
    \[
        \{X_1\le x_1,\dots,X_n\le x_n\} \in \mathscr{F}.
    \]
\end{definition}

\begin{claim}
    This definition is equivalent to saying that $ X_1,\dots,X_n $ are random variables in $\bbR$.
\end{claim}
\begin{proof}[Indeed,]
    $ \{X_1\le x_1,\dots,X_n\le x_n\} = \{X_1\le x_1\}\cap\dots,\cap \{X_n\le x_n \} \in \mathscr{F}$.
\end{proof}

\begin{definition}
    A r.v. $X$ is called \textbf{discrete} if it takes values in a \textit{countable} set.
    Suppose $X$ takes values in a countable set $S$. For every $x\in S$ we write
    \[
        p_x=\mathbb{P}(X=x) = \mathbb{P}(\{\omega:X(\omega)=x\}).
    \]
    We call $ (p_x)_{x\in S} $ the \textbf{probability mass function}, of $X$ (pmf) or the \textbf{distribution} of $X$.
\end{definition}

\begin{note}
    If $(p_x)$ is Bernoulli etc., we say that $X$ is a \textit{Bernoulli r.v.} or $X$ has the \textit{Bernoulli distribution}.
\end{note}

\begin{definition}
    Suppose that $X_1,\dots,X_n$ are discrete random variables taking values in $S_1,\dots,S_n$. We say $X_1,\dots,X_n$ are \textbf{independent} if 
    \[
        \mathbb{P}(X_1=x_1,\dots,X_n=x_n)=\mathbb{P}(X_1=x_1)\cdots \mathbb{P}(X_n=x_n), \quad \forall x_i\in S_i.
    \]
\end{definition}

\begin{example}
    Toss a $p$-coin $N$ times independently. Let $H=1,T=0$ and take $ \Omega=\{0,1\}^N $. Consider $ \omega = \{\omega_1,\dots,\omega_N\}\in \Omega $, we have 
    \[
        p_\omega = \prod_{k=1}^{N} p^{\omega_k} (1-p)^{1-\omega_k}.
    \]
    Define $ X_k(\omega)=\omega_k, k=1,\dots,N,\omega\in \Omega $, so $X_k$ is a discrete r.v. and gives the $k$th toss. Now 
    \[
        p_1 = \mathbb{P}(X_k=1) = \mathbb{P}(\omega_k=1)=p\quad \text{and}\quad p_0 = (1-p).
    \]
    Hence $X_k$ is Bernoulli with parameter $p$. Claim that $X_i$ are independent. Indeed, 
    \begin{align*}
        \mathbb{P}(X_1=x_1,\dots,X_N=x_N) &= \mathbb{P}(\omega = (x_1,\dots,x_N)) \\
        &= \prod_{k=1}^{N}p^{x_k} (1-p)^{1-x_k}=\prod_{k=1}^{N}\mathbb{P}(X_k=x_k).
    \end{align*}
\end{example}

Define $ S_N(\omega) = X_1(\omega)+\dots+X_N(\omega) $, \# of heads in $N$ tosses, then $ S_N:\Omega\to \{0,\dots,N\} $ and 
\[
    p_k=\mathbb{P}(S_N=k) = \binom{N}{k} p^k(1-p)^{N-k}
\]
so $S_N\sim B(N,p)$.

\subsection{Expectation}
\begin{definition}
    $ (\Omega,\mathscr{F},\mathbb{P}) $ and assume $\Omega$ is finite or countable. Let $ X:\Omega\to \mathbb{R}  $ be a r.v. so that $X$ is discrete. We say $X$ is \textbf{non-negative} if $ X\ge 0 $. Define the \textbf{expectation} $ \bbE[X] $ as 
    \[
        \bbE[X] = \sum_{\omega\in \Omega} X(\omega) \mathbb{P}(\{\omega\}).
    \]
\end{definition}

Let $ \Omega_X=\{X(\omega):\omega\in \Omega\} $, so $ \Omega = \bigcup_{x\in \Omega_X}\{X=x\} $. Now 
\begin{align*}
    \bbE[X]&=\sum_\omega X(\omega)\bbP(\{\omega\}) = \sum_{x\in \Omega_X} \sum_{\omega\in \{X=x\}}X(\omega)\bbP(\{\omega\})\\ 
    &=\sum_{x\in \Omega_X} \sum_{\omega\in \{X=x\}}x\bbP(\{\omega\})=\sum_{x\in \Omega_X}x \bbP(X=x).
\end{align*}
So the expectation of $X$ (also called average value or mean) is an average of the values taken by $X$ with weights $\bbP(X=x)$. So 
\[
    \bbE[X] = \sum_{x\in \Omega_X}x\cdot p_x.
\]

\begin{example}
    Suppose $ X\sim B(N,p) $.
    \begin{align*}
        \bbE[X]&= \sum_{k=0}^{N} kP(X=k)=\sum_{k=0}^{N}k \binom{N}{k}p^k (1-p)^{N-k}\\ 
        &= \sum_{k=0}^{N}\frac{kN!}{k!(N-k)!}p^k (1-p)^{N-k}\\ 
        &= \sum_{k=1}^{N} \frac{(N-1)!Np}{(k-1)!(N-k)!}p^{k-1}(1-p)^{N-k}\\ 
        &= Np \sum_{k=1}^{N} \binom{N-1}{k-1}p^{k-1}(1-p)^{(N-1)-(k-1)}\\ 
        &= Np.
    \end{align*}
\end{example}

\begin{example}
    Let $ X\sim P(\lambda) $.
    \begin{align*}
        \bbE[X]&=\sum_{k=0}^{\infty} k e^{-\lambda} \frac{\lambda^k}{k!} = \sum_{k=1}^{\infty} e^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!}\lambda\\
        &= \lambda \sum_{k=1}^{\infty} e^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!} = \lambda.
    \end{align*}
\end{example}

\begin{definition}
    Let $X$ be a general discrete r.v. We define $ X_+=\max \{X,0\} $ and $ X_-=\max \{-X,0\} $. Then $X=X_+-X_-$ and $ |X|=X_++X_- $. If at least 1 of $ \bbE[X_+],\bbE[X_-] $ is finite, then we define 
    \[
        \bbE[X] = \bbE[X_+]-\bbE[X_-].
    \]
    If both are infinity, then we say the expectation of $X$ is not defined. Whenever we write $ \bbE[X] $ it is assumed to be well-defined. If $ \bbE[|X|]<\infty  $, we say $X$ is \textbf{integrable}.
\end{definition}
When $ \bbE[X] $ is well-defined, we have again that 
\[
    \bbE[X] = \sum_{x\in \Omega_X}x \mathbb{P}(X=x).
\]

\begin{proposition}\label{prop:properties of expectation}
    \begin{enumerate}
        \item If $ X\ge 0 $, then $ \bbE[X]\ge 0 $.
        \item If $X\ge 0 $ and $ \bbE[X]=0 $ then $ \mathbb{P}(X=0)=1$.
        \item If $ c\in \mathbb{R} $, then $ \bbE[cX]=c\bbE[X] $ and $ \bbE[c+X]=c+\bbE[X] $.
        \item If $ X,Y $ are r.v.s that are integrable, then $ \bbE[X+Y]=\bbE[X]+\bbE[Y] $.
        \item Let $ c_n\dots,c_n\in \mathbb{R}  $ and $ X_1,\dots,X_n $ are r.v.s that are integrable, then 
        \[
            \bbE\left[ \sum_{i=1}^{n}c_iX_i \right] = \sum_{i=1}^{n} c_i\bbE[X_i].
        \]
    \end{enumerate}
\end{proposition}

\subsubsection*{More properties of expectation}
\begin{proposition}
    Suppose $ X_1,X_2,\dots $ are non-negative r.v.s. Then 
    \[
        \bbE \left[ \sum_n X_n \right] = \sum_n \bbE[X_n].
    \]
\end{proposition}
\begin{proof}
    Suppose $\Omega$ is countable. Note that 
    \begin{align*}
        \bbE\left[ \sum_n X_n \right] &= \sum_\omega \sum_n X_n(\omega)\mathbb{P}(\{\omega\})\\ 
        &= \sum_n\sum_\omega X_n(\omega)\mathbb{P}(\{\omega\}) = \sum_n\bbE[X_n].
    \end{align*}
\end{proof}

\begin{proposition}
    If $ X=1_A,A\in \mathscr{F} $, then $ \bbE[X]=\mathbb{P}(A) $.
\end{proposition}
\begin{proposition}
    If $g:\bbR\to \bbR$, define $ g(X) $ to be the r.v. $ g(X)(\omega)=g(X(\omega)) $. Then 
    \[
        \bbE[g(X)] = \sum_{x\in \Omega_X} g(x)\mathbb{P}(X=x).
    \]
\end{proposition}
\begin{proof}
    Set $Y=g(X)$. 
    \[
        \bbE[Y] = y \mathbb{P}(Y=y).
    \]
    Note that $ \{Y=y\} = \{\omega:Y(\omega=y)\} = \{\omega:X(\omega)\in g^{-1}(\{y\})\} = \{X\in g^{-1}(y)\} $. So 
    \begin{align*}
        \bbE[Y]&= \sum_{y\in \Omega_Y}y \mathbb{P}(X\in g^{-1}(\{y\}))=\sum_{y\in \Omega_Y}y \sum_{x\in g^{-1}(\{y\})}\mathbb{P}(X=x)\\
        &= \sum_{y\in \Omega_Y}\sum_{x\in g^{-1}(\{y\})}g(x)\mathbb{P}(X=x)= \sum_{x\in \Omega_X} g(x)\mathbb{P}(X=x).\qedhere
    \end{align*}
\end{proof}

\begin{proposition}
    If $ X\ge 0 $ and takes integer values, then 
    \[
        \bbE[X] = \sum_{k=1}^{\infty} \mathbb{P}(X\ge k) = \sum_{k=0}^{\infty}\mathbb{P}(X>k).
    \]
\end{proposition}
\begin{proof}
    Since $X$ takes non-negative integer values, 
    \[
        X = \sum_{k=1}^{\infty}1(X\ge k) = \sum_{k=0}^{\infty}1(X>k).
    \]
    Note also that 
    \[
        x = \sum_{k=1}^{x}1 = \sum_{k=1}^{\infty} 1(x\ge k),
    \]
    so taking expectation gives 
    \begin{align*}
        \bbE[X] &= \sum_{k=1}^{\infty}\bbE[1(X\ge k)] = \sum_{k=0}^{\infty}\bbE[1(X>k)]\\ 
        &= \sum_{k=1}^{\infty} \mathbb{P}(X\ge k) = \sum_{k=0}^{\infty}\mathbb{P}(X>k).\qedhere
    \end{align*}
\end{proof}

\subsubsection*{Another proof of the inclusion-exclusion formula}
\begin{proposition}[Properties of indicator]
    \begin{itemize}
        \item $ 1(A^\complement) = 1-1(A) $.
        \item $ 1(A \cap B) = 1(A)\cdot 1(B) $.
        \item $ 1(A \cup B) = 1-(1-1(A))(1-1(B)) $. More generally,
        \begin{align*}
            &1(A_1\cup \cdots \cup A_n) = 1-\prod_{i=1}^{n}(1-1(A_i))\\ 
            &= \sum_{i=1}^{n}1(A_i) -\sum_{i_1<i_2} 1(A_{i_1}\cap A_{i_2})+\cdots+(-1)^{n+1}1(A_1\cap \cdots\cap A_n).
        \end{align*}
    \end{itemize}
\end{proposition}
\begin{proof}
    Do it yourself.
\end{proof}
Taking $\bbE$ on both sides we get 
\[
    \mathbb{P}{(A_1\cup\cdots\cup A_n)} = \sum_{i=1}^{n}\mathbb{P}(A_i)-\sum_{i_1<i_2} \mathbb{P}(A_{i_1}\cap A_{i_2})+\cdots+(-1)^{n+1}\mathbb{P}(A_1\cap\cdots\cap A_n),
\]
which is exactly the inclusion-exclusion formula.

\subsection{Variance}
\begin{definition}
    Let $X$ be a r.v. and $ r\in \mathbb{N} $. We call $ \bbE[X^r] $ (as long as it is well-defined) the \textbf{$r$th moment} of $X$.
\end{definition}
\begin{definition}
    The \textbf{variance} of $X$, denoted as $ \var(X) $, is defined to be 
    \[
        \var(X) = \bbE[(X-\bbE[X])^2].
    \]
    The variance is a measure of how concentrated $X$ is around its expectation.

    Define $ \sigma=\sqrt{\var(X)} $ the \textbf{standard deviation} of $X$.
\end{definition}
\subsubsection*{Properties of variance}
\begin{proposition}
    \begin{enumerate}
        \item $ \var(X)\ge 0 $. If $ \var(X)=0 $ then $ \mathbb{P}(X=\bbE[X])=1 $.
        \item If $c\in \bbR$ then $ \var(cX)=c^2\var(X) $ and $ \var(X+c)=\var(X) $.
        \item $ \var(X)=\bbE[X^2]-(\bbE[X])^2 $.
    \end{enumerate}
\end{proposition}
\begin{proof}
    We only prove 3 since other are trivial. Note that 
    \begin{align*}
        \var(X)&= \bbE[(X-\bbE[x])^2] = \bbE[X^2-2X\bbE[X]+(\bbE[X])^2]\\ 
        &= \bbE[X^2]-2\bbE[X]\bbE[X]+(\bbE[X])^2=\bbE[X^2]-(\bbE[X])^2,
    \end{align*}
    as claimed.
\end{proof}

\begin{proposition}
    $ \displaystyle \var(X) = \min_{c\in \bbR}\bbE[(X-c)^2] $ and the min is achieved for $ c=\bbE[X] $.
\end{proposition}
\begin{proof}
    Let $ f(c)=\bbE[(X-c)^2] = \bbE[X^2]-2c\bbE[X]+c^2 $. Using usual algebras gives the result.
\end{proof}

\begin{example}
    \begin{enumerate}
        \item Suppose $ X\sim B(n,p) $, so $ \bbE[X]=np $.
        \begin{align*}
            \var(X) &= \bbE[X^2]-(\bbE[X])^2 = \bbE[X(X-1)]+\bbE[X]-(\bbE[X])^2\\ 
            &= \sum_{k=2}^{n}k(k-1) \binom{n}{k}p^k (1-p)^{n-k} +np-n^2p^2\\ 
            &= n(n-1)p^2 \sum_{k=2}^{n}\binom{n-2}{k-2}p^{k-2}(1-p)^{n-k}+np-n^2p^2\\ 
            &= n(n-1)p^2+np-n^2p^2\\ 
            &= np(1-p).
        \end{align*}
        \item $ X\sim P(\lambda) $, then 
        \begin{align*}
            \var(X) &= \bbE[X^2]-\lambda^2 = \bbE[X(X-1)]+\bbE[X]-\lambda^2\\ 
            &= \sum_{k=2}^{\infty} k(k-1)e^{-\lambda}\frac{\lambda^k}{k!}+\lambda-\lambda^2\\ 
            &= e^{-\lambda}\lambda^2\sum_{k=2}^{\infty}\frac{\lambda^{k-2}}{(k-2)!}+\lambda-\lambda^2\\ 
            &= \lambda.
        \end{align*}
    \end{enumerate}
\end{example}

\subsection{Covariance}
\begin{definition}
    Let $X,Y$ be r.v.s. Their \textbf{covariance} is defined as 
    \[
        \cov (X,Y) = \bbE[(X-\bbE[X])(Y-\bbE[Y])].
    \]
    It is a measure of how dependent $X,Y$ are.
\end{definition}
\begin{proposition}
    \begin{enumerate}
        \item $ \cov(X,Y)=\cov(Y,X) $.
        \item $ \cov(X,X)=\var(X) $.
        \item $ \cov(X,Y) = \bbE[XY]-\bbE[X]\bbE[Y] $.
        \item Let $c\in \bbR$, Then $ \cov(cX,Y) = c\cov(X,Y) $ and $ \cov(c+X,Y)=\cov(X,Y) $.
        \item $ \var(X+Y) = \var(X)+\var(Y)+2\cov(X,Y) $.
        \item $ \forall c\in \mathbb{R}, \cov(c,X)=0 $.
        \item If $ X,Y,Z $ are r.v.s, then 
        \[
            \cov(X+Y,Z) = \cov(X,Z)+\cov(Y,Z).
        \]
        More generally, for $ c_1,\dots,c_n,d_1,\dots,d_n\in \mathbb{R}, X_1,\dots,X_n,Y_1,\dots,Y_n $ r.v.s, we have 
        \[
            \cov \left( \sum_{i=1}^{n}c_i X_i,\sum_{i=1}^{n}d_i Y_i \right) = \sum_{i=1}^{n}\sum_{j=1}^{n}c_i d_j\cov(X_i,Y_j).
        \]
        In particular, 
        \[
            \var\left( \sum_{i=1}^{n}X_i \right) = \sum_{i=1}^{n}\var(X_i)+\sum_{i\neq j} \cov(X_i,X_j).
        \]
    \end{enumerate}
\end{proposition}
\begin{proof}
    Note that 
    \[
        (X-\bbE[X])(Y-\bbE[Y]) = XY-X\bbE[Y]-Y\bbE[X]+\bbE[X]\bbE[Y].\tag{$*$}
    \]
    \begin{enumerate}
        \item It follows by changing the order of $X,Y$ in ($*$).
        \item Replace $Y$ by $X$ in ($*$).
        \item Take expectation on ($*$).
        \item Replace $X$ by $cX,c+X$ and take expectation on ($*$).
        \item Note that 
        \begin{align*}
            \var(X+Y)&= \bbE[(X+Y)^2]-(\bbE[X+Y])^2\\ 
            &= \bbE[X^2]+\bbE[Y^2]+2\bbE[XY]-(\bbE[X]+\bbE[Y])^2\\ 
            &= \var(X)+\var(Y)+2\cov(X,Y).
        \end{align*}
        \item Replace $Y$ with $c$.
        \item By substitution in ($*$) and linearity of $ \bbE $.
    \end{enumerate}
\end{proof}

Recall that $ X\independent Y \Leftrightarrow \mathbb{P}(X=x,Y=y)=\mathbb{P}(X=x)\mathbb{P}(Y=y), \forall x,y $.

\begin{proposition}
    Let $X,Y$ be 2 independent r.v.s and let $ f,g:\mathbb{R} \to \mathbb{R} $ such that
    \[
        \bbE[f(X)g(Y)],\bbE[f(X)],\bbE[g(Y)]
    \]
    are well-defined. Then
    \[
        \bbE[f(X)g(Y)] = \bbE[f(X)]\bbE[g(Y)].
    \]
\end{proposition}
\begin{proof}
    \begin{align*}
        \bbE[f(X)g(Y)]&= \sum_{x,y} f(x)g(y) \mathbb{P}(X=x,Y=y)\\ 
        &= \sum_{x,y} f(x)g(y) \mathbb{P}(X=x)\mathbb{P}(Y=y)\\ 
        &= \sum_{x} f(x)\mathbb{P}(X=x) \sum_y g(y)\mathbb{P}(Y=y)\\ 
        &= \bbE[f(X)]\bbE[g(Y)].\qedhere
    \end{align*}
\end{proof}

\begin{proposition}
    If $ X \independent Y $, then $ \cov(X,Y)=0 $.
\end{proposition}
\begin{proof}
    \begin{align*}
        \cov(X,Y) &= \bbE[(X-\bbE[X])(Y-\bbE[Y])] \\
        &= \bbE[(X-\bbE[X])]\bbE[(Y-\bbE[Y])]\\ 
        &= 0.\qedhere
    \end{align*}
\end{proof}
\begin{note}
    If $ \cov(X,Y)=0 $, it is not generally true that $ X \independent Y $. Let $ X_1,X_2,X_3 $ be independent r.v.s such that $ X_i\sim B(1/2) $. Define 
    \[
        \begin{aligned}
            Y_1 &= 2X_1-1,\  Y_2 = 2X_2-1,\\ 
            Z_1&=X_3Y_1,\ Z_2 =X_3Y_2.
        \end{aligned}
    \]
    We see that $ \bbE[Y_1]=\bbE[Y_2]=\bbE[Z_1]=\bbE[Z_2]=0 $, and $ \cov(Z_1,Z_2) = \bbE[Z_1Z_2] = \bbE[X_3^2Y_1Y_2]=0 $. However, $ Z_1,Z_2 $ are not independent since 
    \[
        \mathbb{P}(Z_1=0,Z_2=0) = \mathbb{P}(Z_3=0) = \frac{1}{2}
    \]
    but 
    \[
        \mathbb{P}(Z_1=0)\mathbb{P}(Z_2=0) = \mathbb{P}(X_3=0)^2=\frac{1}{4}.
    \]
\end{note}
\begin{corollary}
    If $ X,Y $ are independent, then $ \var(X+Y) = \var(X)+\var(Y) $.
\end{corollary}

\subsection{Inequalities}
\begin{proposition}[Markov's inequality]\label{prop:Markov's inequality}
    Suppose $ X\ge 0 $ is a r.v, then $ \forall a>0 $, 
    \[
        \mathbb{P}(X\ge a) \le \frac{\bbE[X]}{a}.
    \]
\end{proposition}
\begin{proof}
    Observe that $ X\ge a \cdot 1(X\ge a) $. Taking expectations get 
    \[
        \bbE[X]\ge \bbE[a \cdot 1(X\ge a)] = a \mathbb{P}(X\ge a) \Longrightarrow \mathbb{P}(X\ge a)\le \frac{\bbE[X]}{a}.\qedhere
    \]
\end{proof}
\begin{proposition}[Chebyshev's inequality]\label{prop:Chebyshev's inequality}
    Let $X$ be a r.v. with $ \bbE[X]<\infty  $. Then $ \forall a>0 $,
    \[
        \mathbb{P}(|X-\bbE[X]|\ge a)\le \frac{\var(X)}{a^2}.
    \]
\end{proposition}
\begin{proof}
    \begin{align*}
        \mathbb{P}(|X-\bbE[X]|\ge a) &= \mathbb{P}(|X-\bbE[X]|^2\ge a^2)\\ 
        &\le \frac{\bbE\left[ |X-\bbE[X]|^2 \right]}{a^2} = \frac{\var(X)}{a^2}.\qedhere
    \end{align*}
\end{proof}

\begin{proposition}[Cauchy-Schwartz inequality]\label{prop:Cauchy-Schwartz inequality}
    If $X,Y$ are r.v.s, then 
    \[
        \bbE[|XY|]\le \sqrt{\bbE[X^2]\bbE[Y^2]}.
    \]
\end{proposition}
\begin{proof}
    It suffice to prove it for $ \bbE[X^2],\bbE[X^2]<\infty $ and $ X,Y\ge 0 $. Note that 
    \[
        XY\le \frac{1}{2}(X^2+Y^2) \Longrightarrow \bbE[XY] = \frac{1}{2}\left( \bbE[X^2]+\bbE[Y^2] \right)<\infty .
    \]
    Assume $ \bbE[X^2],\bbE[Y^2]>0 $, otherwise the result it trivial since either $ \bbE[X^2],\bbE[Y^2]=0 $ inplies $XY\equiv 0$. Let $t\in \bbR$ and consider 
    \begin{align*}
        &\ 0\le (X-tY^2) = X^2-2tXY-t^2Y^2\\ 
        \Longrightarrow &\  f(t) = \bbE[X^2]-2t\bbE[XY]+t^2\bbE[Y^2]\ge 0\\ 
        \Longrightarrow &\  \min f(t)\quad\text{for}\quad t_0 = \frac{\bbE[XY]}{\bbE[Y^2]} \\ 
        \Longrightarrow &\  f(t_0)=\bbE[X^2]-\frac{2\bbE[XY]^2}{\bbE[Y^2]}+\frac{\bbE[XY]^2}{\bbE[Y^2]}\ge 0\\ 
        \Longrightarrow &\  \bbE[XY]^2\le \bbE[X^2]\bbE[Y^2]\\ 
        \Longrightarrow &\  \bbE[|XY|]\le \sqrt{\bbE[X^2]\bbE[Y^2]},
    \end{align*}
    as claimed.
\end{proof}
\begin{remark}
    Equality occurs when $ \bbE[(X-tY)^2]=0 $ for $ t=\bbE[XY]/\bbE[Y^2] $. Notice that 
    \[
        \bbE[(X-tY)^2]=0 \Longrightarrow \mathbb{P}(X=tY)=1.
    \]
\end{remark}
\begin{definition}[\href{https://www.princeton.edu/~aaa/Public/Teaching/ORF523/S16/ORF523_S16_Lec7_gh.pdf}{Convex functions}]
    A function $ f:\mathbb{R} \to \mathbb{R} $ is \textbf{convex} if $ \forall x,y\in \mathbb{R}  $ and $ \forall t\in [0,1] $,
    \[
        f(tx+(1-t)y)\le tf(x)+(1-t)f(y).
    \]
\end{definition}
\begin{lemma}\label{lma:Jensen}
    Let $ f:\mathbb{R} \to \mathbb{R}  $ be convex, then $f$ is the supremum of all the lines lying below it. In other words,
    \[
        \forall m\in \mathbb{R}, \exists a,b\in \mathbb{R}, f(m)=am+b \land f(x)\ge ax+b,\forall x.
    \]
\end{lemma}
\begin{proof}
    Let $m\in \bbR$ and $ x<m<y $. Then $ \exists t\in [0,1], m=tx+(1-t)y $. By convexity, $ f(m)=tf(m)+(1-t)f(m)\le tf(x)+(1-t)f(y) $. Therefore 
    \[
        t(f(m)-f(x))\le (1-t)(f(y)-f(m)) \Longrightarrow \frac{f(m)-f(x)}{m-x}\le \frac{f(y)-f(m)}{y-m}.
    \]
    So there exists
    \[
        a= \sup_{x<m} \frac{f(m)-f(x)}{m-x},  \ \frac{f(m)-f(x)}{m-x}\le a\le \frac{f(y)-f(m)}{y-m},\ \forall x<m<y.
    \]
    Rearranging gives 
    \[
        f(x)\ge a(x \underbrace{-m)+f(m)}_{b},
    \]
    as claimed.
\end{proof}
\begin{proposition}[Jensen's inequality]\label{prop:Jensen's inequality}
    Let $X$ be a r.v. and let $f$ be a convex function. Then 
    \[
        \bbE[f(X)]\ge f(\bbE[X]).
    \]
\end{proposition}
\begin{note}
    Notice that $ \var(X)=\bbE[(X-\bbE[X])^2]=\bbE[X^2]-(\bbE[X])^2\ge 0 $, so $ \bbE[X^2]\ge (\bbE[X])^2 $. This is a way to remember the direction of the inequality.
\end{note}
\begin{proof}
    Set $ m=\bbE[X] $ in lemma \ref{lma:Jensen}. Then $ \exists a,b\in \mathbb{R} $, 
    \[
        f(m)=am+b \Longleftrightarrow f(\bbE[X])=a\bbE[X]+b
    \]
    and $ f(X)\ge aX+b $. Taking expectation we get 
    \[
        \bbE[f(X)]\ge a\bbE[X]+b=f(\bbE[X]).\qedhere
    \]
\end{proof}
\begin{remark}
    The equality holds for $ X\equiv \bbE[X] $ (check it!).
\end{remark}
\begin{corollary}
    Let $f$ be a convex function and let $ x_1,\dots,x_n\in \mathbb{R}  $. Then 
    \[
        \frac{1}{n}\sum_{k=1}^{n} f(x_k)\ge f\left( \frac{1}{n}\sum_{k=1}^{n}x_k \right).
    \]
\end{corollary}
\begin{proof}
    Define $X$ to be r.v. taking $ \{x_1,\dots,x_n\} $ with equal probability, and use Jensen's inequality.
\end{proof}
\begin{corollary}[AM-GM inequality]\label{prop:AM-GM inequality}
    Let $ x_1,\dots,x_n\in \mathbb{R}  $, then 
    \[
        \left( \prod_{k=1}^{n} x_k\right)^{\frac1n}\le \frac{1}{n}\sum_{k=1}^{n}x_k.
    \]
\end{corollary}
\begin{proof}
    Take $f(x)=-\log x$.
\end{proof}
\subsection{Conditional expectation}
\begin{definition}[Conditional expectation]
    Let $B\in \mathscr{F},\bbP(B)>0$ and let $ X $ be a r.v. Define the \textbf{conditional expectation} as
    \[
        \bbE[X|B] = \frac{\bbE[X\cdot 1(B)]}{\bbP(B)}.
    \]
\end{definition}
\begin{proposition}[Law of total expectation]\label{prop:Law of total expectation}
    Suppose $X\ge 0$ and let $ (\Omega_n) $ be a partition of $\Omega$ into disjoint events. Then 
    \[
        \bbE[X] = \sum_{n} \bbE[X|\Omega_n]\mathbb{P}(\Omega_n).
    \]
\end{proposition}
\begin{proof}
    Note that $ X=X \cdot 1(\Omega)=\sum_{n} X \cdot 1(\Omega_n) $. Taking expectation gives
    \[
    \bbE[X] = \bbE\left[ \sum_{n} X \cdot 1(\Omega_n) \right] = \sum_{n}\bbE[X\cdot 1(\Omega_n)]=\sum_{n} \bbE[X|\Omega_n]\mathbb{P}(\Omega_n),
    \]
    as claimed.
\end{proof}