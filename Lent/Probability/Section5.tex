\section{Multidimensional Gaussian r.v.s}
\subsection{Basic definitions}
\begin{definition}
    A random variable $X$ in $\mathbb R$ is called \textbf{Gaussian} if $X=\mu+\sigma Z$ for $Z\sim\mathcal N(0,1)$ and $ \mu\in \mathbb{R}, \sigma\in [0,\infty) $.
    It has density, as we have seen,
    $$f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/(2\sigma^2)}$$
    and we denote this by $X\sim\mathcal N(\mu,\sigma^2)$.
    We want to generalize this to higher dimensions.
\end{definition}
\begin{definition}
    A random variable $\bfX=(X_1,\ldots,X_n)$ is \textbf{Gaussian} if for any $\bfu\in\mathbb R^n$, $\bfu^\top \bfX$ is Gaussian in $\mathbb R$.
    We call $\bfX$ a \textbf{Gaussian vector}.
\end{definition}

\begin{example}
    Suppose that $ \bfX $ is Gaussian and we have an $m\times n$ matrix $A$ and $\bfb\in\mathbb R^m$, then $A\bfX+\bfb$ is also Gaussian. Indeed, let $ \bfu\in \mathbb{R}^{m} $, then 
    \[
        \bfu^\top (A\bfX+\bfb) = (\bfu^\top A)\bfX+\bfb.
    \]
    Set $ \bfv = A^\top \bfu $, then 
    \[
        \bfu^\top (A\bfX+\bfb) = \bfv^\top \bfX + \bfu^\top \bfb.
    \]
    Since $\bfX$ is Gaussian, $ \bfv^\top \bfX $ is Gaussian and $ \bfv^\top \bfX + \bfu^\top \bfb $ is Gaussian.
\end{example}
\subsection{Mean, variance and covariance}

\begin{definition}
    The \textbf{mean} or \textbf{expectation} $ \boldsymbol{\mu} $ of $\bfX$ is defined by 
    \[
        \boldsymbol{\mu} = \bbE[\bfX] = \begin{pmatrix}
            \bbE[X_1] \\ \vdots \\ \bbE[X_n]
        \end{pmatrix},\quad \mu_i = \bbE[X_i].
    \]
    The \textbf{variance} $ V\in \mcM_{n \times n}(\bbR) $ of $ \bfX $ is defined by 
    \begin{align*}
        V &= \var(\bfX)= \bbE\left[ (\bfX-\boldsymbol{\mu})(\bfX-\boldsymbol{\mu})^\top \right] \\ 
        &= (\bbE[(X_i-\mu_i)(X_j-\mu_j)])_{ij} = (\cov(X_i,X_j))_{i,j}.
    \end{align*}
\end{definition}
We see that $V$ is a symmetric matrix since $ \cov(X_i,X_j)=\cov(X_j,X_i) $. Also $ \forall \bfu\in \mathbb{R}^{n} $,
\[
    \bbE[\bfu^\top\bfX] = \bfu^\top \boldsymbol{\mu},\quad \var(\bfu^\top \bfX) = \bfu^\top V \bfu, \Longrightarrow \boxed{\bfu^\top \bfX\sim\mathcal N(\bfu^\top \boldsymbol{\mu},\bfu^\top V\bfu)}
\]
\begin{proposition}
    $V$ is a non-negative definite matrix. That is $ \forall \bfu\in \mathbb{R}^{n}, \bfu^\top V\bfu\ge 0 $.
\end{proposition}
\begin{proof}
    Let $\bfu\in \mathbb{R}^{n}$. Then $ \var(\bfu^\top \bfX)=\bfu^\top V\bfu\ge 0 $.
\end{proof}
\subsection{Moment generating functions}
\begin{definition}
    Take any $ \boldsymbol{\lambda}\in \mathbb{R}^{n} $. Define the MGF of $\bfX$ as
    \[
        m(\boldsymbol{\lambda}) = \bbE[ e^{\boldsymbol{\lambda}^\top \bfX} ].
    \]
\end{definition}

Since $ \boldsymbol{\lambda}^\top \bfX \sim \mcN(\boldsymbol{\lambda}^\top \boldsymbol{\mu}, \boldsymbol{\lambda}^\top V \boldsymbol{\lambda}) $,
\[
    m(\boldsymbol{\lambda}) = \bbE[ e^{\boldsymbol{\lambda}^\top \bfX} ]= e^{\boldsymbol{\lambda}^\top \boldsymbol{\mu}+ \boldsymbol{\lambda}^\top V \boldsymbol{\lambda}/2}.
\]
So by uniqueness of MGFs, we see that the distribution of a Gaussian vector is uniquely characterized by its mean $\boldsymbol{\mu}$ and variance $V$, so we write $\bfX\sim\mathcal N(\boldsymbol{\mu},V)$.

\paragraph{Standard Gaussian distribution.} Let $ Z_1,\dots,Z_n \sim \mcN(0,1) $ be iid rvs. Set $ \bfZ = (Z_1,\dots,Z_n)^\top  $. Claim that $\bfZ$ is Gaussian. Indeed, given $ \bfu\in \mathbb{R}^{n} $, we want to show that
\begin{align*}
    \bfu^\top\bfZ &= \sum_{i=1}^{n} u_i Z_i
\end{align*}
is normal. We do this by considering expectation. Let $ \lambda\in \mathbb{R} $, 
\begin{align*}
    \bbE[ e^{\lambda \sum_{i=1}^{n}u_iZ_i}] &= \bbE \left[ \prod_{i=1}^{n} e^{\lambda u_i Z_i} \right] = \prod_{i=1}^{n}\bbE[e^{\lambda u_i Z_i}]\\ 
    &= \prod_{i=1}^{n} e^{(\lambda u_i)^2/2} = e^{\lambda^2|\bfu|^2/2},
\end{align*}
so $ \bfu^\top \bfZ \sim \mcN(0,|\bfu|^2) $. Note that $ \bbE[\bfZ]=\mathbf{0},\ \var(\bfZ)= I $, where $I$ is the $n\times n$ identity matrix. So $ \bfZ = \mcN(\mathbf{0}, I) $.

\paragraph{Construction from the standard.} Let $ \boldsymbol{\mu}\in \mathbb{R}^{n} $ and $ V $ a non-negative definite matrix. We want to construct a Gaussian vector with mean $ \boldsymbol{\mu} $ and variance $ V $ using the standard vector $ \bfZ $. When $n=1$ it is just the privious case $ X=\mu+\sigma Z \sim \mcN(\mu,\sigma^2) $. If $ n\ge 2 $, note that since $V$ is symmetric and non-negative definite, we can diagonalise $V$ as 
\[
    V = U^\top D U,\quad U^\top =U^{-1},\quad D= \begin{pmatrix}
        \lambda_1 &  & 0 \\
         & \ddots  &  \\
        0 &  & \lambda_n \\
    \end{pmatrix}
\]
where $ \lambda_i\ge 0 $. We defined the square root of $V$ to be $ \sigma\in \mcM_{n \times n}(\bbR) $ as 
\[
    \sigma = U^\top \sqrt{D} U,\quad \sqrt{D}= \begin{pmatrix}
        \sqrt{\lambda_1} &  & 0 \\
         & \ddots  &  \\
        0 &  & \sqrt{\lambda_n} \\
    \end{pmatrix}.
\]
Indeed we have $ \sigma \sigma = V $.

Now we can construct our r.v. as 
\[
    \bfX = \boldsymbol{\mu}+ \sigma \bfZ.
\]
\begin{claim}
    $ \bfX \sim \mcN(\boldsymbol{\mu},V) $.
\end{claim}
\begin{proof}
    By example 5.1, $\bfX$ is Gaussian. Also $ \bbE[\bfX]=\boldsymbol{\mu} $ and
    \begin{align*}
        \var(\bfX)&= \bbE[(\bfX-\boldsymbol{\mu})(\bfX-\boldsymbol{\mu})^\top]=\bbE[(\sigma\bfZ)(\sigma\bfZ)^\top ]\\ 
        &= \bbE[\sigma\bfZ \bfZ^\top \sigma^\top ] = \sigma \bbE[\bfZ \bfZ^\top]\sigma = \sigma I \sigma = V.\qedhere
    \end{align*}
\end{proof}

\paragraph{Density of $ \bfX \sim \mcN(\boldsymbol{\mu},V) $.} Assume $V$ is positive definite, then $\det V=\prod_i\lambda_i>0$. Also since $\bfX=\boldsymbol{\mu}+\sigma \bfZ$, we can invert to get $\bfZ=\sigma^{-1}(\bfX-\boldsymbol{\mu})$, hence
\begin{align*}
    f_\bfX(\bfx)&=f_\bfZ(\bfz)|J|\\
    &=\prod_{i=1}^n\frac{e^{-z_i^2/2}}{\sqrt{2\pi}}|\det(\sigma^{-1})|=\frac{1}{\sqrt{(2\pi)^n\det V}}e^{-\bfz^\top \bfz/2}\\
    &=\frac{1}{\sqrt{(2\pi)^n\det V}}\exp \left( \frac{-(\bfx-\boldsymbol{\mu})^\top V^{-1}(\bfx-\boldsymbol{\mu})}{2} \right).
\end{align*}
If we only assume $V$ is nonnegative definite, then we can have $0$ eigenvalues hence $0$ determinant.
In this case, we can change the basis to get something of the form
$$\begin{pmatrix}
    U&0\\
    0&0
\end{pmatrix}$$
where $U$ is positive definite $m\times m$ matrix and $\boldsymbol{\mu}=(\boldsymbol{\lambda}^\top,\boldsymbol{\nu}^\top)^\top$ where $\boldsymbol{\lambda}\in\mathbb R^m,\boldsymbol{\nu}\in\mathbb R^{n-m}$, then we write $\bfX=(\bfY^\top,\boldsymbol{\nu}^\top)^\top$ where $\bfY$ has
$$f_\bfY(\bfy)=\frac{1}{\sqrt{(2\pi)^n\det U}}\exp \left( \frac{-(\bfy-\boldsymbol{\lambda})^\top U^{-1}(\bfy-\boldsymbol{\lambda})}{2} \right).$$

\begin{lemma}
    If $X_i$ are independent, then $V$ is diagonal.
\end{lemma}
\begin{proof}
    Since $X_i$ are independent, $ \cov(X_i,X_j)=0,\ i\neq j $ so $ V $ is diagonal.
\end{proof}
\begin{proposition}
    Let $\bfX=(X_1,\ldots,X_n)$ be Gaussian, and suppose that $\cov(X_i,X_j)=0$ when $i\neq j$, then $X_i$ are independent Gaussians.
\end{proposition}
\begin{proof}
    It follows that $V$ is diagonal. Then $V^{-1}$ is also diagonal and the density $ f_{\bfX}(\bfx) $ factorises into a product. Indeed,
    \[
        (\bfx-\boldsymbol{\mu})^\top V^{-1} (\bfx-\boldsymbol{\mu}) = \sum_{i=1}^{n}\frac{(x_i-\mu_i)^2}{\lambda_i},
    \]
    so 
    \[
        f_{\bfx}(\bfx) = \frac{1}{\sqrt{(2\pi)^n\det V}} \exp \left( - \sum_{i=1}^{n}\frac{(x_i-\mu_i)^2}{2\lambda_i}\right).
    \]
    So $X_i$ are independent.
\end{proof}
\begin{proof}[Alternatively,]
    note that $ m(\boldsymbol{\theta})=e^{\boldsymbol{\theta}^\top \boldsymbol{\mu}+ \boldsymbol{\theta}^\top V \boldsymbol{\theta}/2} = e^{\sum \theta_i \mu_i} \cdot e^{(\sum \theta_i^2 \lambda_i)/2} $, so $ m(\boldsymbol{\theta}) $ factorises into mgfs of Gaussians in $\bbR$. Hence $ X_i $ are independent Gaussians.
\end{proof}

So for Gaussian vectors, we have 
\[
    (X_1,\dots,X_n) \text{ independent} \Longleftrightarrow \cov(X_i,X_j)=0,\quad i\neq j.
\]

\subsection{Bivariate Gaussians}
Set $ \bfX=(X_1,X_2)\in \mathbb{R}^{2} $ Gaussian. Let $ \mu_k=\bbE[X_k],\ k=1,2 $ and $ \sigma_k^2 = \var(X_k) $. Define
$$\rho=\operatorname{Corr}(X_1,X_2)=\frac{\operatorname{Cov}(X_1,X_2)}{\sqrt{\operatorname{Var}(X_1)\operatorname{Var}(X_2)}}.$$

\begin{claim}
    $ \rho\in [-1,1] $.
\end{claim}
\begin{proof}
    From Cauchy-Schwartz.
\end{proof}

Hence we have
$$V=\operatorname{Var}(X)=\begin{pmatrix}
    \sigma_1^2&\rho\sigma_1\sigma_2\\
    \rho\sigma_1\sigma_2&\sigma_2^2
\end{pmatrix}.$$


\begin{claim}
    For all $ \sigma_k>0,\rho\in [-1,1] $, $V$ is non-negative definite.
\end{claim}
Indeed, for any $\bfx=(x_1,x_2)$, we have $\bfx^\top V\bfx\ge 0$ by calculation. In particular, if $\rho=0$ and $\sigma_k>0$, then $f_{X_1,X_2}(x_1,x_2)$ can be found by multiplying the $\mathcal N(\mu_k,\sigma_k)$ since $X_1,X_2$ are independent as we have seen before.


\begin{sque}
    Let $ (X_1,X_2) $ be Gaussian. What is $ \bbE[X_2|X_1] $?
\end{sque}

Let $a\in\mathbb R$, then $\operatorname{Cov}(X_2-aX_1,X_1)=\operatorname{Cov}(X_1,X_2)-a\operatorname{Var}(X_1)=\rho\sigma_1\sigma_2-a\sigma_1^2$. Take $a=\rho\sigma_1/\sigma_2$ and $Y=X_2-aX_1$, then $\operatorname{Cov}(X_1,Y)=0$ and we can write
$$\begin{pmatrix}
    X_1\\
    Y
\end{pmatrix}=\begin{pmatrix}
    1&0\\
    -a&1
\end{pmatrix}\begin{pmatrix}
    X_1\\
    X_2
\end{pmatrix},$$
so $(X_1,Y)$ is also Gaussian. So $X_1,Y$ are independent and we can write $X_2=Y+aX_1$ and
\[
    \boxed{\mathbb E[X_2|X_1]=\mathbb E[Y]+aX_2}
\]
\begin{note}
    Note that if $ (X_1,X_2) $ is Gaussian, then $ (X_2-aX_1,X_1) $ is Gaussian and $ X_2-aX_1 \independent X_1 $. We can write $ X_2=X_2-aX_1+aX_1 $ and thus given $ X_1 $, 
    \[
        X_2 \sim \mcN (aX_1+\mu_2-a\mu_1, \var(X_2-aX_1)),
    \]
    where $ \var(X_2-aX_1) =\var(X_2)+a^2\var(X_1)-2a\cov(X_1,X_2)  $.
\end{note}

\begin{theorem}[Multivariate CLT]
    Let $ \bfX $ be a random vector in $ \mathbb{R}^{k} $ with $ \boldsymbol{\mu}=\bbE[\bfX] $ and variance matrix $ \Sigma $. Let $ \bfX_1,\bfX_2,\dots $ be iid with the same distribution as $ \bfX $. Then 
    \[
        \bfS_n=\frac{1}{\sqrt{n}}\sum_{i=1}^n(\bfX_i-\mathbb E[\bfX_i])\xrightarrow{\text{(d)}}\mathcal N(\boldsymbol{\mu},\Sigma).
    \]
    Convergence in distribution means that $ \forall B \subseteq \mathbb{R}^{k} $, $ \mathbb{P}(S_n\in B) \to \mathbb{P}(\mcN(\boldsymbol{\mu},\Sigma)\in B) $.
\end{theorem}