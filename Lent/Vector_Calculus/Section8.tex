\section{Cartesian Tensors}
\subsection{A closer look at vectors}
Let $ \{\bfe_i\} $ be a right-handed orthonormal basis wrt a fixed set of Cartesian axes. When we write a vector as
\[
    \bfx = x_i\bfe_i,
\]
it is often tempting to identify the vector $ \bfx $ with the components $ x_i $. However, this would be wrong! Suppose someone else comes along and wants to describe the same vector, but with respect to their favourite set of right-handed Cartesian axes, which are different to yours but coincide at the origin. Denoting the basis vectors with respect to their Cartesian
axes by $ \{\bfe_i'\} $ then the same vector will be described via
\[
    \bfx = x_i'\bfe_i'.
\]
We must have 
\[
    x_j\bfe_j=x_j'\bfe_j'.
\]
Since both $\left\{\mathbf{e}_{j}\right\}$ and $\left\{\mathbf{e}_{j}^{\prime}\right\}$ are an orthonormal set of vectors, we have
\begin{equation}
    \mathbf{e}_{i} \cdot \mathbf{e}_{j}=\delta_{i j} \quad \text { and } \quad \mathbf{e}_{i}^{\prime} \cdot \mathbf{e}_{j}^{\prime}=\delta_{i j}.\tag{$\star$}
\end{equation}
Dotting both sides of ($\star$) with $\mathbf{e}_{i}^{\prime}$ we find
\[
x_{i}^{\prime}=\delta_{i j} x_{j}^{\prime}=\left(\mathbf{e}_{i}^{\prime} \cdot \mathbf{e}_{j}^{\prime}\right) x_{j}^{\prime}=\mathbf{e}_{i}^{\prime} \cdot\left(\mathbf{e}_{j}^{\prime} x_{j}^{\prime}\right)=\left(\mathbf{e}_{i}^{\prime} \cdot \mathbf{e}_{j}\right) x_{j}.
\]
If we define $R_{i j}=\mathbf{e}_{i}^{\prime} \cdot \mathbf{e}_{j}$, then we have the simple relation
\begin{equation}
    x_{i}^{\prime}=R_{i j} x_{j}.\tag{$\dagger$}
\end{equation}
Equivalently, if we instead dot both sides of $(\star)$ with $\mathbf{e}_{i}$ we would have found
\[
x_{i}=\delta_{i j} x_{j}=\left(\mathbf{e}_{i} \cdot \mathbf{e}_{j}\right) x_{j}=\left(\mathbf{e}_{i} \cdot \mathbf{e}_{j}^{\prime}\right) x_{j}^{\prime}=\left(\mathbf{e}_{j}^{\prime} \cdot \mathbf{e}_{i}\right) x_{j}^{\prime}.
\]
i.e.
\[
x_{i}=R_{j i} x_{j}^{\prime} \Longleftrightarrow x_{j}=R_{k j} x_{k}^{\prime}.
\]
If we now use the relation ($\dagger$) in the form $x_{j}^{\prime}=R_{j k} x_{k}$ we get
\[
x_{i}=R_{j i} R_{j k} x_{k} \Longleftrightarrow \left(\delta_{i k}-R_{j i} R_{j k}\right) x_{k}=0.
\]
Since this is true for all choices of the numbers $\left(x_{1}, x_{2}, x_{3}\right)$ we conclude
\[
R_{i j} R_{k j}=\delta_{i k}.
\]

If we let $R$ denote the matrix with entries $R_{i j}$ then this reads $RR^{\top}=I .$ So $R_{i j}$ are the components of an orthogonal matrix. Since
\[
x_{j} \mathbf{e}_{j}=x_{i}^{\prime} \mathbf{e}_{i}^{\prime}=R_{i j} x_{j} \mathbf{e}_{i}^{\prime}
\]
must hold for all choices of $x_{j}$, we find that the basis vectors are related via
\[
\mathbf{e}_{j}=R_{i j} \mathbf{e}_{i}^{\prime}.
\]
In particular, since both systems are right handed
\[
1=\mathbf{e}_{1} \cdot\left(\mathbf{e}_{2} \times \mathbf{e}_{3}\right)=R_{i 1} R_{j 2} R_{k 3}\ \mathbf{e}_{i}^{\prime} \cdot\left(\mathbf{e}_{j}^{\prime} \times \mathbf{e}_{k}^{\prime}\right)=R_{i 1} R_{j 2} R_{k 3} \epsilon_{i j k}=\operatorname{det}(R).
\]
where we used $\mathbf{e}_{i}^{\prime} \cdot\left(\mathbf{e}_{j}^{\prime} \times \mathbf{e}_{k}^{\prime}\right)=\epsilon_{i j k}$ and the determinant formula you've seen for $3 \times 3$ matrices in IA Vectors \& Matrices
\[
\operatorname{det}(A)=\epsilon_{i j k} A_{i 1} A_{j 2} A_{k 3}
\]
So the matrix $R$ with entries $R_{i j}$ is orthogonal with $\operatorname{det}(R)=1 .$ So it is a rotation. 

\textit{Moral of the story}: If we transform from one set of right-handed Cartesian basis vectors $\left\{\mathbf{e}_{i}\right\}$ to another $\left\{\mathbf{e}_{i}^{\prime}\right\}$ then the components of a vector $\mathbf{v}$ will transform according to
\[
    \boxed{v_{i}^{\prime}=R_{i j} v_{j}}
\]
where $R_{i j}=\mathbf{e}_{i}^{\prime} \cdot \mathbf{e}_{j}$ are the components of an rotation matrix $R,$ i.e. $R^{\top} R=RR^{\top}=I$
and $\operatorname{det}(R)=1$. Objects whose components transform in this way are called \textbf{vectors} or \textbf{rank 1 Cartesian tensors}, or just \textbf{rank 1 tensors} for short. If you prefer a more abstract point of view: you can think of a rank 1 tensor as an equivalence class $\left[v_{i}\right]$, each element of which corresponds to the elements of the vector $\mathbf{v}$ in a given basis. Elements in the same equivalence class are related via a transformation of the above form.

\subsection{A closer look at scalars}
Consider the dot product
\[
    \sigma=\bfa \cdot \bfb
\]
for two fixed vectors $\mathbf{a}, \mathbf{b}$. This should be entirely independent of how we describe the basis vectors a and $\mathbf{b}$, since it has a geometric interpretation in terms of the lengths $|\mathbf{a}|,|\mathbf{b}|$ and the angle between them. In terms of the basis vectors $\left\{\mathbf{e}_{i}\right\}$ with $\mathbf{a}=a_{i} \mathbf{e}_{i}$ etc. we have
\[
\sigma=a_{i} b_{j}\left(\mathbf{e}_{i} \cdot \mathbf{e}_{j}\right)=a_{i} b_{j} \delta_{i j}=a_{i} b_{i}.
\]
If we instead used the basis $\left\{\mathbf{e}_{i}^{\prime}\right\}$ we would have found
\[
\sigma^{\prime}=a_{i}^{\prime} b_{i}^{\prime}
\]
where we've used $\sigma^{\prime}$ to denote the dot product, because we don't yet know if it will be the same in this basis. Using $a_{i}^{\prime}=R_{i p} a_{p}$ and $b_{i}^{\prime}=R_{i q} b_{q}$ we get
\[
\sigma^{\prime}=R_{i p} R_{i q} a_{p} b_{q}=\delta_{p q} a_{p} b_{q}=a_{p} b_{p}=\sigma
\]
We call objects that transform like this \textbf{scalars}, they do not change when we transform one set of right-handed Cartesian basis vectors $\left\{\mathbf{e}_{i}\right\}$ to another $\left\{\mathbf{e}_{i}^{\prime}\right\}$. 

\textit{Moral of the story}: An object that transforms as
\[
    \boxed{\sigma^{\prime}=\sigma}
\]
when we change from one right-handed set of Cartesian basis vectors $\left\{\mathbf{e}_{i}\right\}$ to another $\left\{\mathbf{e}_{i}^{\prime}\right\}$ is called a \textbf{scalar} or \textbf{rank 0 tensor}. The abstract point of view in terms of equivalence classes is dull here: each $[\sigma]$ contains only one element.

\subsection{A closer look at linear maps}
Let $\mathbf{n}$ be a fixed unit vector in $\mathbb{R}^{3}$ and consider the linear map $T: \mathbb{R}^{3} \rightarrow \mathbb{R}^{3}$ defined by
\[
T: \mathbf{x} \mapsto \mathbf{y}=T(\mathbf{x})=\mathbf{x}-(\mathbf{x} \cdot \mathbf{n}) \mathbf{n}
\]
Note that the definition of $T$ does not depend on what basis we're using. The map $T$ gives an orthogonal projection of the vector $\mathbf{x}$ into the plane with normal $\mathbf{n}$. Note that it really is a linear map
\[
T(\alpha \mathbf{x}+\beta \mathbf{y})=\alpha \mathbf{x}+\beta \mathbf{y}-\alpha(\mathbf{x} \cdot \mathbf{n}) \mathbf{n}-\beta(\mathbf{y} \cdot \mathbf{n}) \mathbf{n}=\alpha T(\mathbf{x})+\beta T(\mathbf{y})
\]
for $\mathbf{x}, \mathbf{y} \in \mathbb{R}^{3}$ and scalars $\alpha, \beta$. Suppose we choose to use a Cartesian basis vectors $\left\{\mathbf{e}_{i}\right\}$. Then we can write $\mathbf{x}=x_{i} \mathbf{e}_{i}, \mathbf{y}=y_{i} \mathbf{e}_{i}$ and $\mathbf{n}=n_{i} \mathbf{e}_{i},$ so our definition gives
\[
y_{i} \mathbf{e}_{i}=T(\mathbf{x})=x_{j} T\left(\mathbf{e}_{j}\right)=x_{j}\left(\mathbf{e}_{j}-n_{j} n_{i} \mathbf{e}_{i}\right)=\left(\delta_{i j}-n_{i} n_{j}\right) x_{j} \mathbf{e}_{i}.
\]
Set $T_{i j}=\delta_{i j}-n_{i} n_{j} .$ These are referred to as the components of the linear map $T$ with respect to the basis $\left\{\mathbf{e}_{i}\right\}$. So that we've found
\[
y_{i}=T_{i j} x_{j}.
\]
If we did exactly the same thing but with a different set of orthonormal right-handed basis vectors $\left\{\mathbf{e}_{i}^{\prime}\right\},$ for which $\mathbf{x}=x_{i}^{\prime} \mathbf{e}_{i}^{\prime},$ etc. we would have found
\[
y_{i}^{\prime}=T_{i j}^{\prime} x_{j}^{\prime}
\]
where $T_{i j}^{\prime}=\delta_{i j}-n_{i}^{\prime} n_{j}^{\prime} .$ Using $n_{i}^{\prime}=R_{i j} n_{j}$ gives
\[
\begin{aligned}
T_{i j}^{\prime} &=\delta_{i j}-R_{i p} R_{j q} n_{p} n_{q} \\
&=R_{i p} R_{j q}\left(\delta_{p q}-n_{p} n_{q}\right) \\
&=R_{i p} R_{j q} T_{p q}.
\end{aligned}
\]
So we see the components of the linear map $T$ transform according to\footnote{If $T$ denotes the matrix with entries $T_{ij}$ etc, then this
formula just says $ T'=RTR^{\top } $, which is exactly how a matrix transforms under a change of basis generated by an orthogonal transformation, i.e. $ R^{-1}=R^\top $.}
\[
    \boxed{T_{i j}^{\prime}=R_{i p} R_{j q} T_{p q}}
\]
Objects whose components transform in this way are called \textbf{rank 2 Cartesian tensors}, or simply \textbf{rank 2 tensors} for short\footnote{Some authors like to call these \textbf{dyads}, but we start to run out of words as the rank of the tensors increase, so we usually just call them rank 2 tensors.}. Again, if you like to think in terms of more abstract definitions, you can think of a rank 2 tensor as an equivalence class $\left[T_{i j}\right]$, each element of which corresponds to the components of a tensor $T$ in a given basis. Elements of the
same equivalence class are related via a transformation of the above form.

\subsection{Cartesian Tensors of rank $ n $}
\begin{definition}
    A tensor of rank $n$ has components $T_{i j \cdots k}$ (with $n$ indices) with respect to each Cartesian basis $\left\{\mathbf{e}_{i}\right\}$ that transform as
    \[
        T_{i j \cdots k}^{\prime}=R_{i p} R_{j q} \cdots R_{k r} T_{p q \cdots r}
    \]
    upon changing from from one right-handed Cartesian basis to another. Here the $R_{i j}=\bfe_i' \cdot \bfe_j$ represent components of an rotation matrix, so in particular $R_{i p} R_{j p}=\delta_{i j}$.
\end{definition}

We will be fast and loose with some of our language: we will refer to things like $T_{i j \cdots k}$ as a tensor, rather than the components of a tensor, knowing that these things will change depending on what basis we choose.
\begin{example}
    If $u_{i}, v_{j}, \dots, w_{k}$ are the components of $n$ vectors, then
    \[
    T_{i j \cdots k}=u_{i} v_{j} \cdots w_{k}
    \]
    define the components of an $n$ th rank tensor. We can check:
    \[
    T_{i j \cdots k}^{\prime}=u_{i}^{\prime} v_{j}^{\prime} \cdots w_{k}^{\prime}=R_{i p} R_{j q} \cdots R_{k r} u_{p} v_{q} \cdots w_{r}=R_{i p} R_{j q} \cdots R_{k r} T_{p q \cdots r}.
    \]
\end{example}
\begin{example}
    The Kronecker delta $\delta_{i j}$ has been defined without any reference to any basis
    \[
    \delta_{i j}=\begin{cases}
        1, & i=j \\
        0, & i \neq j
    \end{cases} 
    \]
    So by definition it is the same, no matter what basis we happen to be using. Hence
    \[
    \delta_{i j}^{\prime}=\delta_{i j}
    \]
    But we also see that
    \[
    R_{i p} R_{j q} \delta_{p q}=R_{i p} R_{j p}=\delta_{i j}=\delta_{i j}^{\prime}
    \]
    Hence $\delta_{i j}^{\prime}=R_{i p} R_{j q} \delta_{p q},$ so $\delta_{i j}$ really is a rank 2 tensor.
\end{example}
\begin{example}
    We have also defined the Levi Civita symbol without reference to any basis
    \[
    \epsilon_{i j k}=\begin{cases}
        +1, & (i j k) \text { an even permutation of }(123), \\
        -1, & (i j k) \text { an odd permutation of }(123), \\
        0, & \text { otherwise. }
    \end{cases} 
    \]
    So by definition $\epsilon_{i j k}=\epsilon_{i j k}^{\prime} .$ But we see that
    \[
    R_{i p} R_{j q} R_{k r} \epsilon_{p q r}=\operatorname{det}(R) \epsilon_{i j k}=\epsilon_{i j k}=\epsilon_{i j k}^{\prime}
    \]
    Hence $\epsilon_{i j k}^{\prime}=R_{i p} R_{j q} R_{k r} \epsilon_{p q r}$ really is a rank 3 tensor.
\end{example}
\begin{example}
    Experimental evidence suggests that there is a linear relationship between the current $\mathbf{J}$ produced in a given medium that is subject to an electric field $\mathbf{E}$. Such a
    relationship can be written $\mathbf{J}=\sigma \mathbf{E}$, or in suffix notation
    \[
    J_{i}=\sigma_{i j} E_{j}
    \]
    for some array of numbers $\left\{\sigma_{i j}\right\}$. Let's show that this thing is actually a tensor: known as the electrical conductivity tensor. Since $\mathbf{E}$ and $\mathbf{J}$ are vectors, if we change our coordinate frame we will find\footnote{Here we use $E_{j}^{\prime}=R_{j q} E_{q} \Leftrightarrow E_{q}=R_{j q} E_{j}^{\prime}$, which just reflects the fact that $R^{-1}=R^{\top } \text { for } R \in \SO_3$.}
    \[
    \sigma_{i j}^{\prime} E_{j}^{\prime}=J_{i}^{\prime}=R_{i p} J_{p}=R_{i p} \sigma_{p q} E_{q}=R_{i p} \sigma_{p q} R_{j q} E_{j}^{\prime}
    \]
    and since this must hold for all $E_{j}^{\prime}$ we conclude
    \[
    \sigma_{i j}^{\prime}=R_{i p} R_{j q} \sigma_{p q}
    \]
    so $\sigma$ is a second rank tensor. This is an example of the quotient theorem, which we will
    prove in a few lectures.
\end{example}
\begin{example}
    Not all things are tensors. Suppose that for a given Cartesian basis we define
    the array
    \[
    \left(A_{i j}\right)=\begin{pmatrix}
        1 & 0 & 3 \\
    2 & 2 & 0 \\
    8 & 4 & 7
    \end{pmatrix}
    \]
    And we define $A_{i j}^{\prime}=0$ in all other Cartesian bases. Then $A_{i j}$ are not the components of
    a tensor.
\end{example}

Tensors of the same rank can be added. 
\begin{definition}
    If $A$ and $B$ are $n$th rank tensors with components $A_{i j \cdots k}$ and $B_{i j \cdots k}$ (so $n$ indices on both) then we define
    \[
    (A+B)_{i j \cdots k}=A_{i j \cdots k}+B_{i j \cdots k}.
    \]
    If $\alpha$ is a scalar then we define
    \[
    (\alpha A)_{i j \cdots k}=\alpha A_{i j \cdots k}.
    \]
\end{definition}
It is readily seen that both $A+B$ and $\alpha A$ are $n$ th rank tensors. \begin{definition}
    We define the tensor
    product of an $m$th \textbf{rank tensor} $U$ and an $n$ th rank tensor $V$ by
    \[
    (U \otimes V)_{i j \cdots k p q \cdots r}=U_{i j \cdots k} V_{p q \cdots r}
    \]
    where
    \[
    \underbrace{i j \cdots k}_{m \text { indices }} \underbrace{p q \cdots r}_{n \text { indices }}.
    \]
\end{definition}

This defines a tensor of rank $n+m$, since
\[
\begin{aligned}
(U \otimes V)_{i j \cdots k p q \cdots r}^{\prime} &=U_{i j \cdots k}^{\prime} V_{p q \cdots r}^{\prime} \\
&=R_{i a} R_{j b} \cdots R_{k c} U_{a b \cdots c} R_{p d} R_{q e} \cdots R_{r f} V_{d e \cdots f} \\
&=R_{i a} R_{j b} \cdots R_{k c} R_{p d} R_{q e} \cdots R_{r f}(U \otimes V)_{a b \cdots c d e \cdots f} .
\end{aligned}
\]

\begin{definition}
    Given an $n$ th rank tensor with components $T_{i j k \cdots l}$ we can define an $(n-2)$th rank tensor by \textbf{contracting} on two of the indices. For instance, contracting on $i$ and $j$ is defined by
    \[
    \delta_{i j} T_{i j k \cdots l}=T_{i i k \cdots l}.
    \]
\end{definition}

This really is a tensor of rank $n-2$, since
\[
T_{i i k \cdots l}^{\prime}=R_{i p} R_{i q} R_{k r} \cdots R_{l s} T_{p q r \cdots s}=\delta_{p q} R_{k r} \cdots R_{l s} T_{p q r \cdots s}=R_{k r} \cdots R_{l s} T_{p p r \cdots s}.
\]
We can choose to contract on any pair of indices we like, not just the first two.

\begin{definition}
    
We say a tensor $T_{i j \cdots k}$ is \textbf{symmetric} in $(i, j)$ if
$$
T_{i j \cdots k}=T_{j i \cdots k}
$$
\end{definition}
This is a well-defined property of the tensor, i.e. it doesn't matter which coordinate frame choose, since if $T_{i j \cdots k}$ is symmetric in $(i, j)$ then
\begin{align*}
    T_{i j \cdots k}^{\prime}&=R_{i p} R_{j q} \cdots R_{k r} T_{p q \cdots r}=R_{i p} R_{j q} \cdots R_{k r} T_{q p \cdots r}\\&=R_{i q} R_{j p} \cdots R_{k r} T_{p q \cdots r}=T_{j i \cdots k}^{\prime}.
\end{align*}
i.e. $T_{i j \cdots k}^{\prime}$ is also symmetric in $(i, j)$. Similarly,
\begin{definition}
    We say $A_{i j \cdots k}$ is \textbf{anti-symmetric} in $(i, j)$ if $T_{i j \cdots k}=-T_{j i \cdots k} .$
\end{definition}
We could be talking about any particular pair of indices here $-$ it doesn't necessarily need to be the first and second.
\begin{definition}
    We say a tensor is \textbf{totally symmetric} (anti-symmetric) if it is symmetric (anti-symmetric) in \textit{each} pair of indices.
\end{definition}

\begin{example}
    The tensors $\delta_{i j}$ and $a_{i} a_{j} a_{k}$ are totally symmetric tensors of rank two and three respectively. $\epsilon_{i j k}$ is totally anti-symmetric.
\end{example}
\begin{example}
    In fact, the only totally anti-symmetric tensor on $\mathbb{R}^{3}$ of rank $n=3$ is proportional to $\epsilon_{i j k},$ and there aren't any (non-zero) ones of higher rank. 
    Indeed, suppose $T_{i j \cdots k}$ is a totally anti-symmetric tensor of rank $n$. Then $T_{i j \cdots k}=0$ if any two indices are the same. If $n>3$ then there will always be at least two matching indices in $T_{i j \cdots k},$ by the pigeonhole principle. When $n=3,$ there are only only $3 !=6$ non-zero components, and if
    \[
    T_{123}=T_{231}=T_{312}=\lambda
    \]
    then $T_{213}=T_{321}=T_{132}=-\lambda$ by anti-symmetry. Hence $T_{i j k}=\lambda \epsilon_{i j k}$.
\end{example}

\subsection{Tensor calculus}
Remember that a vector field is just a function that gives you a vector $\mathbf{v}(\mathbf{x})$ at each point $\mathbf{x} \in \mathbb{R}^{3}$. Said differently, a vector field gives you a rank 1 tensor at each $\mathbf{x} \in \mathbb{R}^{3}$. Similarly, a scalar field $\varphi(\mathbf{x})$ gives a scalar, or rank 0 tensor at each point. 
\begin{definition}
    We define a tensor field of rank $n, T_{i j \cdots k}(\mathbf{x})$ so that for each $\mathbf{x}_{0} \in \mathbb{R}^{3},$ $ T_{i j \cdots k}\left(\mathbf{x}_{0}\right)$ define the components of an $n$th rank Cartesian tensor.
\end{definition}
Recall that Cartesian coordinates transform as
\[
x_{i}^{\prime}=R_{i j} x_{j} \Longleftrightarrow x_{j}=R_{i j} x_{i}^{\prime}.
\]
Differentiating both sides with respect to $x_{k}^{\prime}$ we find
\[
\frac{\partial x_{j}}{\partial x_{k}^{\prime}}=R_{i j} \frac{\partial x_{i}^{\prime}}{\partial x_{k}^{\prime}}=R_{i j} \delta_{i k}=R_{k j}.
\]
So the chain rule gives
\[
\frac{\partial}{\partial x_{i}^{\prime}}=\frac{\partial x_{j}}{\partial x_{i}^{\prime}} \frac{1}{\partial x_{j}}=R_{i j} \frac{\partial}{\partial x_{j}}
\]
This gives us a good way of understanding how tensor fields change when we differentiate them. Note that the rotation matrix is constant.

\begin{proposition}
    If $T_{i j \ldots k}(\mathbf{x})$ is a tensor field of rank $n$, then
    \[
    \left(\frac{\partial}{\partial x_{p}}\right)\left(\frac{\partial}{\partial x_{q}}\right) \cdots\left(\frac{\partial}{\partial x_{r}}\right) T_{i j \cdots k}(\mathbf{x})
    \]
    (with $m$ derivatives in total) is a tensor field of rank $m+n$.
\end{proposition}
\begin{proof}
    Denote the quantity by $A_{p q \cdots r i j \cdots k}$ so that
    \[
    \begin{aligned}
    A_{p q \cdots r i j \cdots k}^{\prime} &=\left(\frac{\partial}{\partial x_{p}^{\prime}}\right)\left(\frac{\partial}{\partial x_{q}^{\prime}}\right) \cdots\left(\frac{\partial}{\partial x_{r}^{\prime}}\right) T_{i j \cdots k}^{\prime}(\mathbf{x}) \\
    &=R_{p a} R_{q b} \cdots R_{r c}\left(\frac{\partial}{\partial x_{a}}\right)\left(\frac{\partial}{\partial x_{b}}\right) \cdots\left(\frac{\partial}{\partial x_{c}}\right) R_{i d} R_{j e} \cdots R_{k f} T_{d e \cdots f}(\mathbf{x}) \\
    &=R_{p a} R_{q b} \cdots R_{r c} R_{i d} R_{j e} \cdots R_{k f} A_{a b \cdots c d e \cdots f},
    \end{aligned}
    \]
    which is the transformation law for a rank $n+m$ tensors.
\end{proof}
\begin{example}
    For a scalar field $\varphi=\varphi(\mathbf{x}),$ consider the gradient $\nabla \varphi(\mathbf{x}),$ or in components
    \[
    [\nabla \varphi]_{i}=\frac{\partial \varphi}{\partial x_{i}}.
    \]
    By the previous result, these are the components of a rank $0+1=1$ tensor field, or simply a vector field.
\end{example}

\begin{example}
    For a vector field $\mathbf{v}=\mathbf{v}(\mathbf{x})$ we have the divergence
    \[
    \nabla \cdot \mathbf{v}=\frac{\partial v_{i}}{\partial x_{i}}.
    \]
    Not unsurprisingly, this transforms as a scalar field:
    \[
    \frac{\partial v_{i}^{\prime}}{\partial x_{i}^{\prime}}=R_{i p} \frac{\partial}{\partial x_{p}} R_{i q} v_{q}=R_{i p} R_{i q} \frac{\partial v_{q}}{\partial x_{p}}=\delta_{p q} \frac{\partial v_{q}}{\partial x_{p}}=\frac{\partial v_{p}}{\partial x_{p}}.
    \]
\end{example}
\begin{example}
    For a vector field $\mathbf{v}=\mathbf{v}(\mathbf{x})$ consider the curl $\nabla \times \mathbf{v}$. Then in components
    \[
    [\nabla \times \mathbf{v}]_{i}=\epsilon_{i j k} \frac{\partial v_{k}}{\partial x_{j}}.
    \]
    This transforms as a vector field, but it is not immediately obvious. We have
    \[
    \begin{aligned}
    \epsilon_{i j k}^{\prime} \frac{\partial v_{k}^{\prime}}{\partial x_{j}^{\prime}} &=R_{i a} R_{j b} R_{k c} \epsilon_{a b c} R_{j p} \frac{\partial}{\partial x_{p}} R_{k q} v_{q} \\
    &=R_{i a} \epsilon_{a b c} R_{j b} R_{j p} R_{k c} R_{k q} \frac{\partial v_{q}}{\partial x_{p}} \\
    &=R_{i a} \epsilon_{a b c} \delta_{b p} \delta_{c q} \frac{\partial v_{q}}{\partial x_{p}} \\
    &=R_{i a} \epsilon_{a b c} \frac{\partial v_{c}}{\partial x_{b}},
    \end{aligned}
    \]
    so $[\nabla \times \mathbf{v}]_{i}$ does indeed transform like a vector field.
\end{example}

We can integrate tensor fields just as we can integrate vector fields. Recall the divergence theorem for vector fields, written using suffix notation
\[
\int_{V} \frac{\partial v_{i}}{\partial x_{i}} \mathrm{~d} V=\int_{\partial V} v_{i} n_{i} \mathrm{~d} S
\]
where $\mathbf{n}$ is the outward normal to the surface enclosing $V$. An analogous result holds for
tensor fields.
\begin{proposition}
    For a smooth tensor field $T_{i j \cdots k \cdots l}(\mathbf{x})$ we have
    \[
    \int_{V} \frac{\partial T_{i j \cdots k \cdots l}}{\partial x_{k}} \mathrm{~d} V=\int_{\partial V} T_{i j \cdots k \cdots l} n_{k} \mathrm{~d} S,\tag{$*$}
    \]
    where $ n_k $ are components of the normal pointing outside $ \partial V $. Note that whatever $\frac{\partial}{\partial x_{k}}$ appears on the left is replaced with $n_{k}$ on the right.
\end{proposition}
\begin{proof}
    Apply the usual divergence theorem to the vector field
    \[
    v_{k}:=a_{i} b_{j} \cdots c_{l} T_{i j \cdots k \cdots l}
    \]
    where $a_{i}, b_{j}, \ldots, c_{l}$ are components of constant vectors $\mathbf{a}, \mathbf{b}, \ldots, \mathbf{c} .$ On the right hand side the only free index is $k,$ since all of the others have been contracted with the components of one of the vectors $\mathbf{a}, \mathbf{b}, \cdots, \mathbf{c}$. This gives
    \[
    \int_{V} \frac{\partial v_{k}}{\partial x_{k}} \mathrm{~d} V=a_{i} b_{j} \cdots c_{l} \int_{V} \frac{\partial T_{i j \cdots k \cdots l}}{\partial x_{k}} \mathrm{~d} V
    \]
    and
    \[
    \int_{\partial V} v_{k} n_{k} \mathrm{~d} S=a_{i} b_{j} \cdots c_{l} \int_{\partial V} T_{i j \cdots k \cdots l} n_{k} \mathrm{~d} S .
    \]
    The result follows because the $\mathbf{a}, \mathbf{b}, \ldots, \mathbf{c}$ were arbitrary. Indeed, suppose we wanted to check $(*)$ for when all the free indices were equal to $1 .$ Then we would set
    \[
    a_{i}=\delta_{i 1}, \quad b_{j}=\delta_{j 1}, \quad \cdots \quad c_{l}=\delta_{l 1}
    \]
    so that both sides would reduce to
    \[
    \int_{V} \frac{\partial T_{11 \cdots k \cdots 1}}{\partial x_{k}} \mathrm{~d} V=\int_{\partial V} T_{11 \cdots k \cdots 1} n_{k} \mathrm{~d} S .
    \]
    A similar idea can be used for any other choice of the free indices.
\end{proof}

\subsection{Rank 2 tensors}
Particularly common in applied mathematics and physics are tensors of rank two. For any rank 2 tensor $T_{i j}$ we can always write
\[
\begin{aligned}
T_{i j} &=\frac{1}{2}\left(T_{i j}+T_{j i}\right)+\frac{1}{2}\left(T_{i j}-T_{j i}\right) \\
& \equiv S_{i j}+A_{i j}.
\end{aligned}
\]
We see that $S_{i j}$ is \textbf{symmetric}
\[
S_{i j}=\frac{1}{2}\left(T_{j i}+T_{i j}\right)=S_{j i},
\]
whereas $A_{i j}$ is \textbf{anti-symmetric}
\[
A_{i j}=-\frac{1}{2}\left(T_{j i}-T_{i j}\right)=-A_{j i}.
\]
The original tensor has $3^{2}=9$ independent components. The components of a symmetric tensor $S_{i j}$ are completely determined by the entries with $i \geq j,$ of which there are
\[
1+2+3=6
\]
components. Whereas an anti-symmetric tensor $A_{i j}$ is completely determined by the components with $i>j$ (since the $i=j$ components are necessarily zero), of which there
are
\[
1+2=3.
\]
This is good, because $6+3=9$. Since the anti-symmetric tensor $A_{i j}$ is completely determined by 3 of its components, we expect that all the information contained in $A_{i j}$ can be somehow encoded into a vector $\omega_{i}$.

\begin{proposition}
    Every rank two tensor can be decomposed uniquely as
    \[
    T_{i j}=S_{i j}+\epsilon_{i j k} \omega_{k},
    \]
    where the vector $\boldsymbol{\omega}$ is defined by $\omega_{i}=\frac{1}{2} \epsilon_{i j k} T_{j k}$ and $S_{i j}$ is symmetric.
\end{proposition}
\begin{proof}
    Given our previous discussion, we can identify
    \[
    S_{i j}=\frac{1}{2}\left(T_{i j}+T_{j i}\right)
    \]
    and it remains to show that $\epsilon_{i j k} \omega_{k}=\frac{1}{2}\left(T_{i j}-T_{j i}\right) .$ We have
    \[
    \begin{aligned}
    \epsilon_{i j k} \omega_{k} &=\frac{1}{2} \epsilon_{i j k} \epsilon_{k l m} T_{l m} \\
    &=\frac{1}{2}\left(\delta_{i l} \delta_{j m}-\delta_{i m} \delta_{j l}\right) T_{l m} \\
    &=\frac{1}{2}\left(T_{i j}-T_{j i}\right).
    \end{aligned}
    \]
    If there were two such decompositions
    \[
    S_{i j}+\epsilon_{i j k} \omega_{k}=\tilde{S}_{i j}+\epsilon_{i j k} \tilde{\omega}_{k},
    \]
    then taking symmetric parts of both sides yields $S_{i j}=\tilde{S}_{i j},$ and so $\epsilon_{i j k} \omega_{k}=\epsilon_{i j k} \tilde{\omega}_{k} .$ The latter implies $\omega_{k}=\tilde{\omega}_{k},$ so the decomposition is unique.
\end{proof}

\begin{example}
    Suppose that each point $\mathbf{x}$ in an elastic body undergoes a small displacement of $\mathbf{u}(\mathbf{x})$. Then two nearby points that were initially separated by $\delta \mathbf{x}$ become separated by
    \[
    (\mathbf{x}+\delta \mathbf{x}+\mathbf{u}(\mathbf{x}+\delta \mathbf{x}))-\mathbf{x}-\mathbf{u}(\mathbf{x})=\delta \mathbf{x}+\mathbf{u}(\mathbf{x}+\delta \mathbf{x})-\mathbf{u}(\mathbf{x}).
    \]
    So the change in displacement between the two points in the body is
    \[
    \mathbf{u}(\mathbf{x}+\delta \mathbf{x})-\mathbf{u}(\mathbf{x})
    \]
    This gives a measure of how much deformation happens to the body. Using Taylor's theorem with Cartesian coordinates and suffix notation
    \[
    u_{i}(\mathbf{x}+\delta \mathbf{x})-u_{i}(\mathbf{x})=\frac{\partial u_{i}}{\partial x_{j}} \delta x_{j}+o(\delta \mathbf{x}).
    \]
    We decompose the second rank tensor $\partial u_{i} / \partial x_{j}$ as
    \[
    \frac{\partial u_{i}}{\partial x_{j}}=e_{i j}+\epsilon_{i j k} \omega_{k},
    \]
    where $e_{i j}=\frac{1}{2}\left(\frac{\partial u_{i}}{\partial x_{j}}+\frac{\partial u_{j}}{\partial x_{i}}\right)$ is called the \textbf{linear strain tensor} and
    \[
    \omega_{i}=\frac{1}{2} \epsilon_{i j k} \frac{\partial u_{j}}{\partial x_{k}}=-\frac{1}{2}(\nabla \times \mathbf{u})_{i}
    \]
    This tells us that
    \[
    u_{i}(\mathbf{x}+\delta \mathbf{x})-u_{i}(\mathbf{x})=e_{i j} \delta x_{j}+[\boldsymbol{\omega} \times \delta \mathbf{x}]_{i}+o(\delta \mathbf{x})
    \]
    The second term is just a solid body rotation\footnote{Remember from IA Vectors \& Matrices that a rotation about $\mathbf{n}$ by $\theta$ is
    \[
    R \mathbf{x}=\mathbf{x} \cos \theta+(\mathbf{n} \times \mathbf{x}) \sin \theta+\mathbf{n}(\mathbf{n} \cdot \mathbf{x})(1-\cos \theta)=\mathbf{x}+\theta(\mathbf{n} \times \mathbf{x})+o(\theta), \quad \theta \rightarrow 0.
    \]
    So the change in displacement owing to a small rotation is $R \delta \mathbf{x}-\delta \mathbf{x}=\theta(\mathbf{n} \times \delta \mathbf{x})+o(\theta),$ hence the
    identification of the second term with a solid rotation.} whereas the first term tells us about how
    the material stretches and/or compresses internally.
\end{example}

\begin{example}
    A well known symmetric rank 2 tensor is the \textbf{inertia tensor}. Suppose a body with uniform density $\rho(\mathbf{x})$ occupies a volume $V$. Suppose each point in the body is rotating with constant angular velocity $\boldsymbol{\omega}$ about the origin, so the velocity of a point at $\mathbf{x} \in V$ is $\mathbf{v}=\boldsymbol{\omega} \times \mathbf{x}$. Then the total angular momentum about the origin is
    \[
    \begin{aligned}
    \mathbf{L} &=\int_{V}(\mathbf{x} \times \mathbf{v}) \rho(\mathbf{x}) \mathrm{d} V \\
    &=\int_{V} \mathbf{x} \times(\boldsymbol{\omega} \times \mathbf{x}) \rho(\mathbf{x}) \mathrm{d} V.
    \end{aligned}
    \]
    Using suffix notation we find
    \[
    L_{i}=\int_{\mathcal{V}} \rho(\mathbf{x})\left(x_{k} x_{k} \omega_{i}-x_{i} x_{j} \omega_{j}\right) \mathrm{d} V=I_{i j} \omega_{j}
    \]
    where we have defined the inertia tensor by
    \[
    I_{i j}=\int_{\mathcal{V}} \rho(\mathbf{x})\left(x_{k} x_{k} \delta_{i j}-x_{i} x_{j}\right) \mathrm{d} V
    \]
    and the integral is taken over $\mathcal{V}=\left\{\left(x_{1}, x_{2}, x_{3}\right): \mathbf{x}=x_{i} \mathbf{e}_{i} \in V\right\} .$ If we had used a different set of Cartesian basis vectors $\left\{\mathbf{e}_{i}^{\prime}\right\}$ with $\mathbf{x}=x_{i}^{\prime} \mathbf{e}_{i}^{\prime}$ etc we would have found
    \begin{align*}
        I_{i j}^{\prime}&=\int_{\mathcal{V}^{\prime}} \rho(\mathbf{x})\left(x_{k}^{\prime} x_{k}^{\prime} \delta_{i j}-x_{i}^{\prime} x_{j}^{\prime}\right) \mathrm{d} V\\ &=R_{i p} R_{j q} \int_{\mathcal{V}} \rho(\mathbf{x})\left(x_{k} x_{k} \delta_{p q}-x_{p} x_{q}\right) \mathrm{d} V\\ &=R_{i p} R_{j q} I_{p q}
    \end{align*}
    where $\mathcal{V}^{\prime}=\left\{\left(x_{1}^{\prime}, x_{2}^{\prime}, x_{3}^{\prime}\right): \mathbf{x}=x_{i}^{\prime} \mathbf{e}_{i}^{\prime} \in V\right\} .$ So it really is a symmetric tensor of rank 2 .
\end{example}

\begin{example}
    Consider an ellipsoid given in a particular Cartesian coordinate system by
    \[
    \frac{x_{1}^{2}}{a^{2}}+\frac{x_{2}^{2}}{b^{2}}+\frac{x_{3}^{2}}{c^{2}} \le 1
    \]
    with uniform density $\rho_{0}$, so has mass $M=\frac{4 \pi a b c}{3} \rho_{0} .$ To compute the inertia tensor for this solid we use scaled spherical polar coordinates
    \[
    x_{1}=a r \cos \phi \sin \theta, \quad x_{2}=b r \sin \phi \sin \theta, \quad x_{3}=c r \cos \theta
    \]
    with $0 \le r \le 1$. Note that if $i \neq j$ then $ \int_{V} \rho_{0} x_{i} x_{j} \mathrm{~d} V=0 $ by symmetry. Also
    \[
    \begin{aligned}
    I_{11} &=\rho_{0} \int_{V}\left(x_{2}^{2}+x_{3}^{2}\right) \mathrm{d} V \\
    &=\rho_{0} a b c \int_{r=0}^{1} \int_{\theta=0}^{\pi} \int_{\phi=0}^{2 \pi} r^{2}\left(b^{2} \sin ^{2} \phi \sin ^{2} \theta+c^{2} \cos ^{2} \theta\right) r^{2} \sin \theta \mathrm{d} r \mathrm{~d} \theta \mathrm{d} \phi \\
    &=\frac{\rho_{0} a b c}{5} \int_{0}^{\pi}\left(\pi b^{2} \sin ^{2} \theta+2 \pi c^{2} \cos ^{2} \theta\right) \sin \theta \mathrm{d} \theta \\
    &=\frac{3 M}{4} \frac{1}{5} \int_{0}^{\pi}\left(b^{2} \sin \theta+\left(2 c^{2}-b^{2}\right) \cos ^{2} \theta \sin \theta\right) \mathrm{d} \theta \\
    &=\frac{3 M}{20}\left(2 b^{2}+\frac{2}{3}\left(2 c^{2}-b^{2}\right)\right) \\
    &=\frac{M}{5}\left(b^{2}+c^{2}\right)
    \end{aligned}
    \]
    and by symmetry
    \[
    I_{22}=\frac{M}{5}\left(a^{2}+c^{2}\right), \quad I_{33}=\frac{M}{5}\left(a^{2}+b^{2}\right).
    \]
    So in the coordinate system aligned with the axes of symmetry of the ellipsoid
    \[
    \left(I_{i j}\right)=\frac{M}{5}\begin{pmatrix}
        b^{2}+c^{2} & 0 & 0 \\
    0 & a^{2}+c^{2} & 0 \\
    0 & 0 & a^{2}+b^{2}
    \end{pmatrix}.
    \]
    Note that if $a=b=c$ then $I_{i j}=\frac{2}{5} M \delta_{i j}$.
\end{example}

For symmetric, second rank tensors we also have the following result.
\begin{proposition}
    If $T_{i j}$ is a symmetric, second rank tensor then there exists a choice of right-handed Cartesian coordinates axes in which
    \[
    \left(T_{i j}\right)=\begin{pmatrix}
        \alpha & 0 & 0 \\
    0 & \beta & 0 \\
    0 & 0 & \gamma
    \end{pmatrix}.
    \]
    The corresponding Cartesian axes are called the \textbf{principal axes} for $T$.
\end{proposition}
\begin{proof}
    This is a direct consequence of the fact that a real symmetric matrix can be diagonalised using orthogonal transformation $R,$ for which we can assume $\det(R)=1$ without loss of generality.
\end{proof}
In particular, for the inertia tensor $I_{i j}$ we can \textit{always} choose a set of axes so that the components form a diagonal array. These axes usually correspond to some obvious lines of symmetry on the body.

\subsection{Invariant and isotropic tensors}

\begin{definition}
    We say a tensor is \textbf{isotropic} if it is invariant under changes of Cartesian coordinates, i.e.
    \[
    T_{i j \cdots k}^{\prime}=R_{i p} R_{j q} \cdots R_{k r} T_{p q \cdots r}=T_{i j \cdots k}
    \]
    for any rotation matrix $R$.
\end{definition}

\begin{example}\
    We have already seen three types of isotropic tensor:
    \begin{enumerate}
        \item Every tensor if rank 0 is isotropic.
        \item The \textit{Kronecker delta} is isotropic
        \[
        \delta_{i j}^{\prime}=R_{i p} R_{j q} \delta_{p q}=R_{i p} R_{j p}=\delta_{i j}.
        \]
        \item The \textit{Levi-Civita tensor} is isotropic
        \[
        \epsilon_{i j k}^{\prime}=R_{i p} R_{j q} R_{k r} \epsilon_{p q r}=\operatorname{det}(R) \epsilon_{i j k}=\epsilon_{i j k}.
        \]
    \end{enumerate}
\end{example}

We can actually classify all isotropic tensors on $\mathbb{R}^{3} .$ The general result is due to Hermann Weyl, and is given in his book \textit{The Classical Groups}.

\begin{proposition}
    Isotropic tensors on $ \mathbb{R}^{3} $ are classified as follows:
    \begin{enumerate}[(a)]
        \item All rank 0 tensors are isotropic.
        \item There are no non-zero isotropic tensors of rank 1.
        \item The most general isotropic tensor of rank 2 is $ \alpha \delta_{ij} $ , with $ \alpha $ a scalar.
        \item The most general isotropic tensor of rank 3 is $ \beta \epsilon_{ijk} $, with $ \beta $ scalar.
        \item The most general isotropic tensor of rank 4 is 
        \[
            \alpha \delta_{ij}\delta_{kl}+\beta \delta_{ik}\delta_{jl}+\gamma \delta_{il}\delta_{jk},
        \]
        $ \alpha,\beta,\gamma $ scalars.
        \item The most general isotropic tensor of rank $n \ge 4$ is a linear combination of products
        of Kronecker deltas and Levi-Civita tensors.
    \end{enumerate}
\end{proposition}

The full proof of this result is a little too involved for this course. However, we can
certainly give the basic idea for rank $\le 3$.

\begin{proof}[Sketch proof]
    \begin{enumerate}[(a)]
        \item Rank 0 tensors are isotropic by definition.
        \item If $v_{i}$ are the components of an isotropic vector then
        \[
        v_{i}=R_{i j} v_{j}.
        \]
        for any rotation. We will use rotations by $\pi$ about each axis. Take
        \[
        \left(R_{i j}\right)=\begin{pmatrix}
            -1 & 0 & 0 \\
        0 & -1 & 0 \\
        0 & 0 & 1
        \end{pmatrix}.
        \]
        Then $v_{1}=T_{1 j} v_{j}=-v_{1},$ so $v_{1}=0$ and $v_{2}=T_{2 j} v_{j}=-v_{2}$ so $v_{2}=0 .$ Similarly, using
        \[
        \left(R_{i j}\right)=\begin{pmatrix}
            1 & 0 & 0 \\
        0 & -1 & 0 \\
        0 & 0 & -1
        \end{pmatrix}
        \]
        we find $v_{3}=T_{3 j} v_{j}=-v_{j}$ so $v_{3}=0,$ i.e. $v_{i}=0,$ and this holds in all frames.
        \item If $T_{i j}$ is isotropic then $T_{i j}=R_{i p} R_{j q} T_{p q}$ for any $R $. We will use rotations by $\pi / 2$ about each axis. First we take
        \[
        \left(R_{i j}\right)=\begin{pmatrix}
            0 & 1 & 0 \\
        -1 & 0 & 0 \\
        0 & 0 & 1
        \end{pmatrix}
        \]
        Then
        \begin{align*}
            T_{13}&=R_{1 p} R_{3 q} T_{p q}=R_{12} R_{33} T_{23}=T_{23}, \\
        T_{23}&=R_{2 p} R_{3 q} T_{p q}=R_{21} R_{33} T_{13}=-T_{13},
        \end{align*}
        so $T_{13}=T_{23}=0 .$ We also have
        \[
        T_{11}=R_{1 p} R_{1 q} T_{p q}=R_{12} R_{12} T_{22}=T_{22}.
        \]
        Next we take
        \[
        \left(R_{i j}\right)=\begin{pmatrix}
            1 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & -1 & 0
        \end{pmatrix}.
        \]
        Then
        \begin{align*}
            T_{32}&=R_{3 p} R_{2 q} T_{p q}=R_{32} R_{23} T_{23}=-T_{23}=0 \\
        T_{12}&=R_{1 p} R_{2 q} T_{p q}=R_{11} R_{23} T_{13}=-T_{13}=0 \\
        T_{31}&=R_{3 p} R_{1 q} T_{p q}=R_{32} R_{11} T_{21}=-T_{21} \\
        T_{21}&=R_{2 p} R_{1 q} T_{p q}=R_{23} R_{11} T_{31}=T_{31}
        \end{align*}
        so $T_{31}=T_{21}=0 .$ We also have
        \[
        T_{22}=R_{2 p} R_{2 q} T_{p q}=R_{23} R_{23} T_{33}=T_{33}.
        \]
        In conclusion $T_{i j}=0$ if $i \neq j$ and $T_{11}=T_{22}=T_{33}$. Hence $T_{i j}=\alpha \delta_{i j}$ for a scalar $\alpha$.
        \item Similar, just more indices.

    \end{enumerate}
\end{proof}

As an example of the use of isotropic tensors, consider integrals of the form
\[
T_{i j \cdots k}=\int_{|\mathbf{x}|<R} f(r) x_{i} x_{j} \cdots x_{k} \mathrm{~d} V(\mathbf{x}),
\]
where $|\mathbf{x}|=r$ and $\mathrm{d} V(\mathbf{x})=\mathrm{d} x_{1} \mathrm{~d} x_{2} \mathrm{~d} x_{3} .$ Then $f(r)$ and the domain $\{\mathbf{x}:|\mathbf{x}|<R\}$ are
invariant under rotations. We have
\[
\begin{aligned}
T_{i j \ldots k}^{\prime} &=\int_{|\mathbf{x}|<R} f(r) x_{i}^{\prime} x_{j}^{\prime} \cdots x_{k}^{\prime} \mathrm{d} V(\mathbf{x}) \\
&=\int_{|\mathbf{x}|<R} f(r) R_{i p} x_{p} R_{j q} x_{q} \cdots R_{k r} x_{r} \mathrm{~d} V(\mathbf{x}).
\end{aligned}
\]
Now make the substitution $y_{i}=R_{i j} x_{j}$ for $j=1,2,3$. Since it is a rotation the determinant of this transformation is $1 .$ Hence
\[
T_{i j \ldots k}^{\prime}=\int_{|\mathbf{y}|<R} f(r) y_{i} y_{j} \cdots y_{k} \mathrm{~d} V(\mathbf{y}),
\]
where $\mathrm{d} V(\mathbf{y})=\mathrm{d} y_{1} \mathrm{~d} y_{2} \mathrm{~d} y_{3} .$ Since $\mathbf{y}$ is just a dummy index
\[
T_{i j \ldots k}^{\prime}=\int_{|\mathbf{x}|<R} f(r) x_{i} x_{j} \cdots x_{k} \mathrm{~d} V(\mathbf{x})=T_{i j \cdots k}.
\]
So these are isotropic tensors! Taking $R \rightarrow \infty$ corresponds to integrating over all $\mathbb{R}^{3}$.

\begin{example}
    Consider the array
    \[
    T_{i j}=\int_{\mathbb{R}^{3}} e^{-r^{5}} x_{i} x_{j} \mathrm{~d} V
    \]
    By the previous discussion, we know that $T_{i j}$ is isotropic, so $T_{i j}=\alpha \delta_{i j}$ for some constant $\alpha$. Contracting on $i$ and $j$
    \[
    3 \alpha=\int_{\mathbb{R}^{3}} e^{-r^{5}} r^{2} \mathrm{~d} V=4 \pi \int_{0}^{\infty} r^{4} e^{-r^{5}} \mathrm{~d} r=\frac{4 \pi}{5}
    \]
    Hence $T_{i j}=\frac{4 \pi}{15} \delta_{i j}$
\end{example}
\begin{example}
    The inertia tensor for a ball of radius $R$ and uniform density $\rho_{0}$ is
    \[
    I_{i j}=\int_{|\mathbf{x}|<R} \rho_{0}\left(x_{k} x_{k} \delta_{i j}-x_{i} x_{j}\right) \mathrm{d} V
    \]
    This is the sum of two isotropic tensors, so $I_{i j}=\alpha \delta_{i j}$ for some appropriate constant $\alpha$. Contracting on $i$ and $j$
    \begin{align*}
        3 \alpha&=\int_{|\mathbf{x}|<R} 2 \rho_{0} r^{2} \mathrm{~d} V=2 \rho_{0} \int_{r=0}^{R} \int_{\theta=0}^{\pi} \int_{\phi=0}^{2 \pi} r^{4} \sin \theta \mathrm{d} r \mathrm{~d} \theta \mathrm{d} \phi\\ &=\frac{8 \pi R^{5} \rho_{0}}{5}=\frac{6 M R^{2}}{5}
    \end{align*}
    where $M=\frac{4 \pi}{3} R^{3} \rho_{0}$ is the mass of the ball. Hence $I_{i j}=\frac{2}{5} M R^{2} \delta_{i j}$.
\end{example}

For a tensor $T_{i j}$ in a given Cartesian coordinate system, consider the bilinear map $t: \mathbb{R}^{3} \times \mathbb{R}^{3} \mapsto \mathbb{R}$ defined by
\[
t(\mathbf{a}, \mathbf{b}):=T_{i j} a_{i} b_{j}.
\]

This map is well-defined, in the sense that it doesn't depend on what Cartesian basis we use. Indeed, the right hand side is a scalar, so invariant under change of Cartesian basis. So we can define a bilinear map from a rank two tensor.

Conversely, if $t: \mathbb{R}^{3} \times \mathbb{R}^{3} \rightarrow \mathbb{R}$ is bilinear map, then for a given Cartesian basis $\left\{\mathbf{e}_{i}\right\}$ it defines an array $T_{i j}$ via
\[
t(\mathbf{a}, \mathbf{b})=a_{i} b_{j} t\left(\mathbf{e}_{i}, \mathbf{e}_{j}\right):=T_{i j} a_{i} b_{j}
\]
If we use a different Cartesian coordinate system $\left\{\mathbf{e}_{i}^{\prime}\right\}$ related to the old one via $\mathbf{e}_{i}^{\prime}=R_{i p} \mathbf{e}_{p}$ then, using bilinearity of $t,$ we see that the components of the array transform as
\[
T_{i j}^{\prime}=t\left(\mathbf{e}_{i}^{\prime}, \mathbf{e}_{j}^{\prime}\right)=R_{i p} R_{j q} t\left(\mathbf{e}_{p}, \mathbf{e}_{q}\right)=R_{i p} R_{j q} T_{p q}
\]
So the array $T_{i j}$ transforms as a rank 2 tensor. These observations can be used to construct an explicit, one-to-one correspondence between bilinear maps and rank 2 Cartesian tensor. In particular, if the map
\[
(\mathbf{a}, \mathbf{b}) \mapsto T_{i j} a_{i} b_{j}
\]
is genuinely a bilinear map (independent of basis), then the array $T_{i j}$ define a rank 2 tensor. Exactly the same idea works with tensors of higher rank: if the map
\[
(\mathbf{a}, \mathbf{b}, \ldots, \mathbf{c}) \mapsto T_{i j \cdots k} a_{i} b_{j} \cdots c_{k}
\]
genuinely defines a multilinear map (independent of basis), then the array $T_{i j \ldots k}$ transform as a tensor of the appropriate rank.

This gives us a nice way of proving the \textbf{quotient theorem}. We saw earlier in this chapter,
when looking at the conductivity tensor, that if $\mathbf{E}$ and $\mathbf{J}$ are vectors and $J_{i}=\sigma_{i j} E_{j},$ then $\sigma_{i j}$ were necessarily the components of a rank 2 tensor. The quotient theorem is the same as this, but for arbitrary rank.

\begin{proposition}
    Let $T_{i \cdots j p \cdots q}$ is an array of numbers defined in each Cartesian coordinate system such that
    \[
    v_{i \cdots j}:=T_{i \cdots j p \cdots q} u_{p \cdots q}
    \]
    is a tensor for each tensor $u_{p \cdots q},$ then $T_{i \cdots j p \cdots q}$ is a tensor.
\end{proposition}
\begin{proof}
    Take the special case $u_{p \cdots q}=c_{p} \cdots d_{q}$ for vectors $\mathbf{c}, \ldots, \mathbf{d}$. Then
    \[
    v_{i \cdots j}:=T_{i \cdots j p \cdots q} c_{p} \cdots d_{q}
    \]
    is a tensor and in particular
    \[
    v_{i \cdots j} a_{i} \cdots b_{j}=T_{i \cdots j p \cdots q} a_{i} \cdots b_{j} c_{p} \cdots d_{q}
    \]
    is a scalar for each $\mathbf{a}, \ldots, \mathbf{b}, \mathbf{c}, \ldots, \mathbf{d}$. So the right hand side is a scalar (i.e. it is independent of Cartesian basis), and gives rise to a well-defined multi-linear map via
    \[
    t(\mathbf{a}, \ldots, \mathbf{b}, \mathbf{c}, \ldots, \mathbf{d}):=T_{i \cdots j p \cdots q} a_{i} \cdots b_{j} c_{p} \cdots d_{q}
    \]
    By our previous discussion we conclude that $T_{i \cdots j p \cdots q}$ is a tensor.
\end{proof}

\begin{example}
    We have already seen the linear strain tensor
    \[
    e_{i j}=\frac{1}{2}\left(\frac{\partial u_{i}}{\partial x_{j}}+\frac{\partial u_{j}}{\partial x_{i}}\right)
    \]
    which measures the infinitesimal deformation of a body in which each point $\mathbf{x}$ undergoes a displacement of $\mathbf{u}(\mathbf{x})$. Experiment suggests that the stresses, or internal forces, experienced by a body depends linearly on the strain at each point. Stresses are measured by the stress tensor $\sigma_{i j} .$ So there should be a collection of $3^{4}=81$ numbers $c_{i j k l}$ such that
    \[
    \sigma_{i j}=c_{i j k l} e_{k l}
    \]
    We cannot immediately apply the quotient theorem, because $e_{k l}$ is not arbitrary: it is necessarily symmetric. However, if we assume $c_{i j k l}=c_{i j l k}$ then we can apply the quotient theorem (see example sheet 4). On this assumption, the array $c_{i j k l}$ must correspond to the components of a rank 4 tensor. It is called the stiffness tensor, and is a property of the material that is under stress. If the material is isotropic then we should write, for scalars $\lambda, \beta, \gamma$
    \[
    c_{i j k l}=\lambda \delta_{i j} \delta_{k l}+\beta \delta_{i k} \delta_{j l}+\gamma \delta_{i l} \delta_{k j}
    \]
    so that
    \[
    \begin{aligned}
    \sigma_{i j} &=\lambda \delta_{i j} e_{k k}+\beta e_{i j}+\gamma e_{j i} \\
    & \equiv \lambda \delta_{i j} e_{k k}+2 \mu e_{i j}
    \end{aligned}
    \]
    where in the final line we used $e_{i j}=e_{j i}$ and defined $2 \mu=\beta+\gamma$. This is Hooke's law for an isotropic material. It is the higher dimensional analogue of the familiar equation $F=-k x$ for an elastic spring. We can use it to get the strain tensor in terms of the stress tensor. First contract on $i$ and $j$ so that
    \[
    \sigma_{i i}=(3 \lambda+2 \mu) e_{k k}, \quad \text { i.e. } \quad e_{k k}=\frac{\sigma_{k k}}{3 \lambda+2 \mu},
    \]
    assuming $3 \lambda \neq-2 \mu .$ Then we get
    \[
    2 \mu e_{i j}=\sigma_{i j}-\left(\frac{\lambda}{3 \lambda+2 \mu}\right) \sigma_{k k} \delta_{i j}.
    \]
\end{example}