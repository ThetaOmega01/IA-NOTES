\documentclass[10pt]{article}
\pdfoutput=1 
\usepackage{NotesTeX,lipsum}
\usepackage{IEEEtrantools}
\def\d{{\mathrm d}}
\def\e{{\mathrm e}}
\def\g{{\mathrm g}}
\def\h{{\mathrm h}}
\def\f{{\mathrm f}}
\def\p{{\mathrm p}}
\def\s{{\mathrm s}}
\def\t{{\mathrm t}}
\def\i{{\mathrm i}}

\def\A{{\mathrm A}}
\def\B{{\mathrm B}}
\def\E{{\mathrm E}}
\def\F{{\mathrm F}}
\def\G{{\mathrm G}}
\def\H{{\mathrm H}}
\def\P{{\mathrm P}}


\def\bb{\mathbf b}
\def \bc{\mathbf c}
\def\bx {\mathbf x}
\def\bn {\mathbf n}
\def\le{\leqslant}
\def\ge{\geqslant}
\def\arcosh{{\rm arcosh}\,}

\newcommand{\bluecomment}[1]{{\color{blue}#1}}
%\renewcommand{\comment}[1]{}
\newcommand{\redcomment}[1]{{\color{red}#1}}
\newcommand{\tm}{\times}
%\usepackage{showframe}

\title{\begin{center}{\Huge \textit{Vectors and Matrices}}\\{{\itshape Based on Lectures and "Intro to Linear Algebra"}}\end{center}}
\author{$\theta\omega\theta$}
\affiliation{
Not in University of Cambridge\\
skipped some talks irrelevant to contents\\
}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\nullity}{null}
\emailAdd{not telling you}

\begin{document}
	\maketitle
	\flushbottom
	\newpage
	\pagestyle{fancynotes}
	\part{Complex Numbers}
    \section{Definition}
    \begin{definition}
        Construct $\mathbb{C} $ from $ \mathbb{R}  $ by adding $i$ that $i^2 = -1$. Any $z\in \mathbb{C}$ is in the form
        \[
            z = x+iy, x = \Re z, y= \Im z, x,y\in \mathbb{R} . 
        \] 
        Addition and multiplication are defined by
        \[
            z_1+z_2 = (x_1+x_2) + i(y_1+y_2), z_1z_2 = (x_1+iy_1)(x_2+iy_2)
        .\]
        The \textit{conjugate} is defined by
        \[
            \bar{z} = z* = x-iy
        .\]
        The \textit{modulus} is defined by
        \[
            r = |z|, r\ge 0, r^2=|z|^2=z\bar{z}=x^2+y^2
        .\]
        The \textit{argument} is defined by
        \[
            z\neq 0: \theta=\arg(z)\in \mathbb{R}, z=r(\cos \theta+i\sin \theta)
        .\]
        The values of $ \theta $ in $ (-\pi, \pi] $ are called the \textit{principal values}.

        Complex numbers can be plotted on an \textit{Argand diagram}.
    \end{definition}
    \section{Basic Properties \& Consequences}
    \begin{enumerate}[(1)]
        \item $ +, \times $ are commutative and associative,
        
            $ \mathbb{C} $ under $+$ is an abelian group,
            
            $ \mathbb{C} $ under $\times$ is an abelian group,

            $ \mathbb{C} $ is a field.
        \item \textbf{Fundamental Theorem of Algebra}: A polynomial with deg $n$ with coefficients in $ \mathbb{C}  $ can be written as a product of $n$ linear factors, has at least one solution in $ \mathbb{C}  $ and has n solutions connected with multiplicity.
        \item Parallelogram constructions.
        \item \[
            \left| z_1 \right| \left| z_2 \right| = \left| z_1z_2 \right|, \left| z_1+z_2 \right| \le \left| z_1 \right| +\left| z_2 \right|
        .\]

        Alternative forms:
        \[
            \left| z_2-z_1 \right| \ge \left| z_2 \right| -\left| z_1 \right|, \left| z_2-z_1 \right| \ge \left| |z_2|-|z_1| \right| 
        .\]
        \item \textbf{De Moivre's Theorem}: $ z^n = r^n(\cos n\theta+i\sin n\theta) $.
    \end{enumerate}
    \section{Exponential and Trigs in $\mathbb{C}$}
    \begin{definition}
        Define $ \exp, \cos, \sin $ on $ \mathbb{C} $ by
        \[
            \begin{aligned}
                 \exp(z)&= e^z = \sum_{n=0}^{\infty} \frac{1}{n!}z^n,\\
                 \cos(z)&= \frac{1}{2}(e^{iz}+e^{-iz}) = 1-\frac{1}{2!}z^2+\frac{1}{4!}z^4+\cdots,\\
                 \sin(z)&= \frac{1}{2i}(e^{iz}-e^{-iz}) = z-\frac{1}{3!}z^3+\frac{1}{5!}z^5+\cdots.\\
            \end{aligned}
        \]
        These series converge for all $ z\in \mathbb{C} $. Can be multiplied, rearranged, etc. Definitions reduce to familiar ones in the reals.
    \end{definition}
    \begin{proposition}\label{prop:exp multi}
        $ \forall z,w\in \mathbb{C}, e^ze^w=e^{z+w}; e^ze^{-z}=1, (e^z)^n =e^{nz}, n\in \mathbb{Z}$.
    \end{proposition}
    \begin{lemma}\label{lma:exp_arth}
        For $z=x+iy$:
        \begin{enumerate}[(1)]
            \item $ e^z = e^x(\cos y+i\sin y) $.
            \item $ \exp(z)\in \mathbb{C} \setminus \left\{ 0\right\} $.
            \item $ e^z = 1 \Leftrightarrow z = 2\pi n i, n\in \mathbb{Z} $.
        \end{enumerate}
    \end{lemma}
    \begin{definition}[Roots of unity]
        $z$ is an $ N $th root of unity if $ z^N=1 $.
    \end{definition}
    We have 
    \[
        z^N=r^Ne^{iN\theta} =1 \Longleftrightarrow r=1, N\theta = 2n\pi \Longleftrightarrow \theta = \frac{2n\pi}{N}
    ,\]
    which gives $N$ distinct solutions
    \[
        z = \frac{2n\pi}{N} = \omega^n, \quad, n=0,1,\dots, N-1
    .\]
    $\omega^n$ lie one the vertices of a regular $n$-gon on the unit circle.
    \section{Logarithms and Complex powers}
    \begin{definition}
        Define $ w = \log z, z\in \mathbb{C} \land z\neq 0 $ by $ e^w = e^{\log z}=z $. Note that since exp is many-to-one, $\log$ is multi-valued.
        \[
            \begin{aligned}
                 & z = re^{i\theta} =e^{\log r}e^{i\theta}=e^{\log r+ i\theta}\\
                 \Longrightarrow & \boxed{\log z = \log r+i\theta = \log |z|+i\arg(z)}
            \end{aligned}
        \]
        To make it single-valued, simply take the principal value.
    \end{definition}
    \begin{definition}
        Define \textit{complex power} by 
        \[
            z^\alpha = e^{\alpha\log z}, \quad z,\alpha\in \mathbb{C} , z\neq 0
        .\]
        Note that since $ \arg z \to \arg z + 2n\pi \Rightarrow z^\alpha \to z^\alpha e^{2n\pi} $, it is generally multi-valued. This also reduces to common powers when $ z,\alpha\in \mathbb{R}. $
    \end{definition}
    \begin{example}
        \[
            i^i = e^{i\log i} = e^{i(0+i(\frac{\pi}{2}+2n\pi))} = e^{-(\frac{\pi}{2}+2n\pi)}
        .\]
    \end{example}
    \section{Transformations, Lines, and Circles}
    \begin{itemize}
        \item We have five elementary transformations:
        \begin{enumerate}[(1)]
            \item $ z \mapsto z+a $,
            \item $ z \mapsto \lambda z $,
            \item $ z \mapsto e^{i\alpha} z $,
            \item $ z \mapsto \bar{z} $,
            \item $ z \mapsto \frac{1}{z} $.
        \end{enumerate}
        \item General point of a line in $\mathbb{C}$ through $z_0$ and parallel to $w$:
        \[
            z = z +\lambda w, \lambda\in \mathbb{R} \text{ or } \bar{w}z-w\bar{z}=\bar{w}z_0-w\bar{z_0}
        .\]
        \item General point of a circle in $\mathbb{C}$ with centre $ c $ and radius $ \rho $:
        \[
            z = c+ \rho e^{i\theta} \text{ or } \left| z-c \right| = \rho \text{ or } |z|^2-\bar{c}z-c\bar{z}=\rho^2-|c|^2
        .\]
        \item Stereographic projection.
    \end{itemize}
    \part{Vectors in 3 Dimensions}
    \section{Vector addition and scalar multiplication}
    \begin{definition}[scalar multiplication]
        Given $ \mathbf{a} $, and scalar $ \lambda\in \mathbb{R} $, define $ \lambda \mathbf{a} $ to be the position vector of $ A' $ on the line $OA$ with length $ |\lambda \mathbf{a}|=\left| \lambda \right| \left| \mathbf{a} \right|  $. Direction depends on the sign of $ \lambda $.
    \end{definition}
    Define $ \spn\left\{ \mathbf{a}\right\} = \left\{ \lambda \mathbf{a}: \lambda\in \mathbb{R} \right\} $. If $ \mathbf{a}\neq 0 $, then $ \spn \{\mathbf{a}\} $ is the entire line through $O$ and $A$.

    Define $ \mathbf{a} \parallel \mathbf{b} $ if and only if either $ \mathbf{a} = \lambda \mathbf{b} $ or $ \mathbf{b} = \lambda \mathbf{a} $. Allow $ \lambda=0 $, so $\forall \mathbf{a}, \mathbf{0} \parallel \mathbf{a} $. Also allow $\lambda<0$.
    \begin{definition}[vector addition]
        Give $ \mathbf{a}, \mathbf{b} $, if $ \mathbf{a} \nparallel \mathbf{b} $, construct a parallelogram $OACB$ and define $ \mathbf{c} = \mathbf{a} + \mathbf{b} $.
    \end{definition}
    If $ a \parallel b $, then $ \mathbf{a} = \alpha \mathbf{u}, \mathbf{b}=\beta \mathbf{u} $, where $\mathbf{u}$ is a unit vector and $ \mathbf{a}+\mathbf{b}=(\alpha+\beta)\mathbf{u} $.

    Given $ \mathbf{a},\mathbf{b},\dots,\mathbf{c} $, we have a linear combination
    \[
        \alpha \mathbf{a}+ \beta \mathbf{b}+\cdots+\gamma \mathbf{c}
    \]
    for any $ \alpha,\beta,\dots, \gamma\in \mathbb{R}  $.

    Define $ \spn \left\{ \mathbf{a},\mathbf{b},\dots,\mathbf{c}\right\} = \left\{ \alpha \mathbf{a}+ \beta \mathbf{b}+\cdots+\gamma \mathbf{c}:\alpha,\beta,\dots, \gamma\in \mathbb{R} \right\} $. In 3d case, if $ \mathbf{a} \nparallel \mathbf{b} $, then $ \spn \{ \mathbf{a}, \mathbf{b} \} $ is a plane through $O,A,B$.
    
    Here are some properties:
    \begin{itemize}
        \item $ \forall \mathbf{a}, \mathbf{b}, \mathbf{c}, \mathbf{a}+\mathbf{0}=\mathbf{0}+\mathbf{a}=\mathbf{a} $, this says that $ \mathbf{0} $ is the identity for addition.
        \item $ \exists -\mathbf{a}, \mathbf{a}+(-\mathbf{a})=(-\mathbf{a})+\mathbf{a}=\mathbf{0} $. This says $ -\mathbf{a} $ is the inverse of $ \mathbf{a} $ under addition.
        \item $ \forall \mathbf{a},\mathbf{b}, \mathbf{a}+\mathbf{b}=\mathbf{b}+\mathbf{a} $, this says that vector addition is commutative.
        \item $ \forall \mathbf{a},\mathbf{b},\mathbf{c}, (\mathbf{a}+\mathbf{b})+\mathbf{c}=\mathbf{a}+(\mathbf{b}+\mathbf{c}) $, this says that vector addition is associative.
    \end{itemize}
    Hence, the set of vectors with addition form an abelian group.

    Relation with scalars:
    \begin{itemize}
        \item $ \lambda(\mathbf{a}+\mathbf{b})=\lambda \mathbf{a}+\lambda \mathbf{b} $.
        \item $ (\lambda+\mu)\mathbf{a}=\lambda \mathbf{a}+ \mu \mathbf{a} $.
        \item $ (\lambda \mu)\mathbf{a}=\lambda(\mu \mathbf{a}) $.
    \end{itemize}
    \section{Dot product}
    \begin{definition}[dot product]
        Give $ \mathbf{a}, \mathbf{b} $, let $ \theta $ be the angle between them, define $ \mathbf{a}\cdot\mathbf{b}=|\mathbf{a}||\mathbf{b}|\cos \theta $. Note that $ \theta $ is defined unless $ \mathbf{a}=\mathbf{0} $, in which case we define $ \mathbf{a}\cdot \mathbf{b}=0 $.

        $ \mathbf{a}\perp\mathbf{b} \Leftrightarrow \mathbf{a}\cdot \mathbf{b}=0 \Leftrightarrow \theta=\frac{\pi}{2}\bmod \pi $ when $ \theta $ is defined. Allow $ \mathbf{a} $ or $ \mathbf{b}=0 $, so $ \mathbf{a} \parallel \mathbf{0} \land \mathbf{a} \perp \mathbf{0} $.
    \end{definition}
    For $ \mathbf{a}\neq \mathbf{0} $, $ |\mathbf{b}|\cos \theta $ is the component of $ \mathbf{b} $ along $ \mathbf{a} $.
    \[
        \left| \mathbf{b} \right| \cos \theta = \frac{\mathbf{a}\cdot \mathbf{b}}{|\mathbf{a}|}=\mathbf{u}\cdot \mathbf{b}
    .\]
    By resolving $ \mathbf{b} $ along and perpendicular to $ \mathbf{a} $, we get 
    \[
        \mathbf{b} = \mathbf{b}_{\parallel} + \mathbf{b}_{\perp }
    .\]
    Properties:
    \begin{itemize}
        \item $ \mathbf{a}\cdot \mathbf{b}=\mathbf{b}\cdot \mathbf{a}, \mathbf{a}\cdot \mathbf{a}=|\mathbf{a}|^2\ge 0 $, $ =0 $ iff $ \mathbf{a}=\mathbf{0} $.
        \item $ (\lambda \mathbf{a})\cdot \mathbf{b}=\lambda (\mathbf{a}\cdot \mathbf{b})=\mathbf{a}\cdot (\lambda \mathbf{b}) $.
        \item $ \mathbf{a}\cdot (\mathbf{b}+\mathbf{c})=\mathbf{a}\cdot \mathbf{b}+\mathbf{a}\cdot \mathbf{c} $.
    \end{itemize}
    \section{Vector cross product}
    \begin{definition}
        Given $ \mathbf{a},\mathbf{b} $, let $ \theta $ be the angle between them, wrt a unit vector $ \mathbf{n} $ normal to the plane they span. Define $ \mathbf{a} \wedge \mathbf{b} $ or $ \mathbf{a} \times \mathbf{b} $ as $ |\mathbf{a}||\mathbf{b}|\sin \theta \mathbf{n} $. $ \mathbf{0} $ case is similar.
    \end{definition}
    This is the \textit{vector area} of the parallelogram generated by $ \mathbf{a}, \mathbf{b} $. Note that $ \mathbf{a} \wedge \mathbf{b} = \mathbf{a} \wedge \mathbf{b}_{\perp } $.

    Properties:
    \begin{itemize}
        \item $ \mathbf{a} \wedge \mathbf{b}=\mathbf{b}\wedge \mathbf{a} $.
        \item $ (\lambda \mathbf{a})\wedge \mathbf{b}=\lambda(\mathbf{a}\wedge \mathbf{b})=\mathbf{a} \wedge (\lambda\mathbf{b}) $.
        \item $ \mathbf{a}\wedge (\mathbf{b}+\mathbf{c})=\mathbf{a}\wedge \mathbf{b}+\mathbf{a}\wedge \mathbf{c} $.
        \item $ \mathbf{a} \wedge \mathbf{b}=\mathbf{0} $ if and only if $ \mathbf{a} \parallel \mathbf{b} $.
        \item $ \mathbf{a}\wedge \mathbf{b} \perp \mathbf{a} \land \perp \mathbf{b} $.
    \end{itemize}
    %Lecture 4
    \section{Orthonormal Bases and Components}\marginnote{Lecture 4.}
    Choose $ \mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3 $ that are \textit{orthonormal}. That is, they are of unit lengths and $ \mathbf{e}_i\cdot \mathbf{e}_j =0, i\neq j\in \{1,2,3\}$, which is equivalent to choose cartesian axes along the directions. Then $ \left\{ \mathbf{e}_i\right\} $ is a basis and $ \forall \mathbf{a}\in \mathbb{R}^3 $, 
    \[
        \mathbf{a}=\sum_{i=1}^{3}a_i \mathbf{e}_i \marginnote{Each component $ a_i $ is uniquely determined by $ a_i = \mathbf{e}_i \cdot \mathbf{a} $.}
    .\]
    By this spirite, we can write 
    \[
        \mathbf{a} = (a_1,a_2,a_3) = \begin{pmatrix}
            a_1\\ 
            a_2\\ 
            a_3\\
            \end{pmatrix}
    .\]
    Scalar product in this form can be written as 
    \[
        \begin{pmatrix}
            a_1\\ 
            a_2\\ 
            a_3\\
            \end{pmatrix} \cdot \begin{pmatrix}
                b_1\\ 
                b_2\\ 
                b_3\\
                \end{pmatrix}
                =a_1b_1+a_2b_2+a_3b_3,\quad \left| \mathbf{a} \right| = a_1^2+a_2^2+a_3^2.
    .\]
    For vector products, choose this basis that it is also \textit{right-handed}:
    \[
        \mathbf{e}_i \times \mathbf{e}_{i+1}=\mathbf{e}_{i+2}
    .\]
    Then
    \[
        \begin{aligned}
            \mathbf{a}\times \mathbf{b} &= (a_1 \mathbf{e_1}+a_2 \mathbf{e}_2+a_3 \mathbf{e}_3)(b_1 \mathbf{e_1}+b_2 \mathbf{e}_2+b_3 \mathbf{e}_3)\\
            &= (a_3b_2-a_2b_3)\mathbf{e}_1+(a_3b_1-a_1b_3)\mathbf{e}_2+(a_1b_2-a_2b_1)\mathbf{e}_3.
        \end{aligned}
    \]
    \section{Triple products}
    \subsection{Scalar triple product}
    \begin{definition}
        Define scalar triple product by
        \[
            [\mathbf{a},\mathbf{b},\mathbf{c}]=\mathbf{a}\cdot (\mathbf{b} \times \mathbf{c})=\mathbf{b}\cdot (\mathbf{c} \times \mathbf{a})=\mathbf{c}\cdot (\mathbf{a}\times \mathbf{b})
        .\]
        This is the volumn of the parallelepiped with bases $ \mathbf{b}, \mathbf{c} $ and side $ \mathbf{a} .$
    \end{definition}
    \begin{remark}
        $ \mathbf{c}\cdot (\mathbf{a}\times \mathbf{b}) $ is a "signed" volumn. If $ \mathbf{c}\cdot (\mathbf{a}\times \mathbf{b})>0 $ then $ \left\{ \mathbf{a},\mathbf{b},\mathbf{c}\right\} $ is called a \textit{right-handed set}. $ \mathbf{c}\cdot (\mathbf{a}\times \mathbf{b})=0 $ if and only if $ \mathbf{a},\mathbf{b},\mathbf{c} $ are coplanar, e.g., $ \mathbf{c}=\alpha \mathbf{a}+\beta \mathbf{b} \in \spn\left\{ \mathbf{a},\mathbf{b}\right\}$.
    \end{remark}
    In components, 
    \[
        \begin{aligned}
            \mathbf{a}\cdot (\mathbf{b}\times \mathbf{c})&=a_1b_2c_3-a_1b_3c_2 \\
            &+a_2b_3c_1-a_2b_1c_3\\
            &+a_3b_1c_2-a_3b_2c_1\\
        \end{aligned} =\begin{vmatrix}
            a_{1} & a_{2} & a_{3} \\
            b_{1} & b_{2} & b_{3} \\
            c_{1} & c_{2} & c_{3}
        \end{vmatrix}.
    \]
    \subsection{Vector triple product}
    \begin{definition}
        Define the vector triple product by $ \mathbf{a}\times (\mathbf{b}\times \mathbf{c}) $. Note that $ \mathbf{a}\times (\mathbf{b}\times \mathbf{c}) $ does not necessarily give the same result as $ (\mathbf{a}\times \mathbf{b})\times \mathbf{c} $.
        \begin{equation}\label{eq:vector triple prod identity}
            \mathbf{a}\times (\mathbf{b}\times \mathbf{c})=(\mathbf{a}\cdot \mathbf{c})\mathbf{b}-(\mathbf{a}\cdot \mathbf{b})\mathbf{c}.
        \end{equation}
    \end{definition}
    We have the following identities:
    \begin{proposition}\label{prop:triple}
        \[
            \begin{array}{l}
                \mathbf{a} \times(\mathbf{b} \times \mathbf{c})+\mathbf{b} \times(\mathbf{c} \times \mathbf{a})+\mathbf{c} \times(\mathbf{a} \times \mathbf{b})=\mathbf{0} \\
                (\mathbf{a} \times \mathbf{b}) \times(\mathbf{c} \times \mathbf{d})=[\mathbf{a}, \mathbf{b}, \mathbf{d}] \mathbf{c}-[\mathbf{a}, \mathbf{b}, \mathbf{c}] \mathbf{d} \\
                (\mathbf{a} \times \mathbf{b}) \cdot((\mathbf{c} \times \mathbf{d}) \times(\mathbf{e} \times \mathbf{f}))=[\mathbf{a}, \mathbf{b}, \mathbf{d}][\mathbf{c}, \mathbf{e}, \mathbf{f}]-[\mathbf{a}, \mathbf{b}, \mathbf{c}][\mathbf{d}, \mathbf{e}, \mathbf{f}] \\
                (\mathbf{b} \times \mathbf{c}) \cdot(\mathbf{a} \times \mathbf{d})+(\mathbf{c} \times \mathbf{a}) \cdot(\mathbf{b} \times \mathbf{d})+(\mathbf{a} \times \mathbf{b}) \cdot(\mathbf{c} \times \mathbf{d})=0
                \end{array}
        \]
    \end{proposition}
    \section{Lines, Planes, and Vector equations}
    Vectors are defined as position vectors from $O$. But the definition of addition enables us to use them to describe displacements between points.
    \subsection{Lines}
    General point on a line through $ \mathbf{a} $ through $ \mathbf{u} $:
    \[
        \begin{aligned}
            &\mathbf{r} = \mathbf{a}+\lambda\mathbf{u}, &\lambda\in \mathbb{R}\quad &\text{The parametric form.}\\
            &\mathbf{u} \times \mathbf{r} = \mathbf{u} \times \mathbf{a},&  &\text{Cross form.}
        \end{aligned}
    \]
    \begin{proposition}\label{prop:line_equation_vec}
        Any vector equation of the form $ \mathbf{u}\times \mathbf{r}=\mathbf{c} $ represents a line.
    \end{proposition}
    \begin{proof}
        $ \mathbf{u}\times \mathbf{r}=\mathbf{c} \Rightarrow \mathbf{u}\cdot (\mathbf{u}\times \mathbf{r})=\mathbf{u}\cdot \mathbf{c} \Leftrightarrow \mathbf{u} \cdot \mathbf{c}=0 $. If $ \mathbf{u} \cdot \mathbf{c}\neq 0 $ then the equation is inconsistent. If $ \mathbf{u}\cdot \mathbf{c} =0$, then note that 
        \[
            \mathbf{u} \times (\mathbf{u} \times \mathbf{c})=(\mathbf{u}\cdot \mathbf{c})\mathbf{u}-(\mathbf{u}\cdot \mathbf{u})\mathbf{c} = -\left| \mathbf{u} \right|^2 \mathbf{c}
        .\]
        Hence $ \mathbf{a} = -(\mathbf{u} \times \mathbf{c})/|\mathbf{u}|^2 $ is a solution, and thus it represents a line.
    \end{proof}
    \subsection{Planes}
    General point on a plane through $ \mathbf{a} $ with directions $ \mathbf{u},\mathbf{v} $ in the plane($ \mathbf{u}\nparallel \mathbf{v} $):
    \[
        \begin{aligned}
             &\mathbf{r} = \mathbf{a}+\lambda \mathbf{u}+\mu \mathbf{v}, &\lambda, \mu\in \mathbb{R} &\quad \text{Parametric form,}\\
             & \mathbf{n} \cdot \mathbf{r} = k = \mathbf{n} \cdot \mathbf{a}, & \mathbf{n} = \mathbf{u}\times \mathbf{v} &\quad \text{Dot form.}
        \end{aligned}
    \]
    The component of $\mathbf{r}$ along $ \mathbf{n} $ is 
    \[
        \frac{\mathbf{n}\cdot \mathbf{r}}{|\mathbf{n}|}=\frac{k}{|\mathbf{n}|}
    .\]
    \subsection{Other vector equations}
    \begin{enumerate}[(1)]
        \item $ |\mathbf{r}|^2+\mathbf{r}\cdot \mathbf{a}=k \Leftrightarrow \left| \mathbf{r}+\frac{1}{2}\mathbf{a} \right|^2=k+\frac{1}{4}|\mathbf{a}|^2  $, a sphere with centre $ -\frac{1}{2}\mathbf{a} $ and radius $ \sqrt{k+\frac{1}{4}|\mathbf{a}|^2} $, provided $ k>-\frac{1}{4}|\mathbf{a}|^2 $.
        \item $ \mathbf{r}+\mathbf{a} \times (\mathbf{b}\times \mathbf{r}) =\mathbf{c} \Leftrightarrow \mathbf{r}+(\mathbf{a}\cdot \mathbf{r})\mathbf{b}-(\mathbf{a}\cdot \mathbf{b})\mathbf{r}=\mathbf{c}$. Dot with $\mathbf{a}$:
        \[
            \mathbf{a}\cdot \mathbf{r}= \mathbf{a}\cdot \mathbf{c} \Longrightarrow (1-\mathbf{a}\cdot \mathbf{b})\mathbf{r}=\mathbf{c}-(\mathbf{a}\cdot \mathbf{c})\mathbf{b}
        .\]
        If $ \mathbf{a}\cdot \mathbf{b}\neq 1 $, then there is a unique solution
        \[
            \mathbf{r} = \frac{1}{1-\mathbf{a}\cdot \mathbf{b}}(\mathbf{c}-(\mathbf{a}\cdot \mathbf{c})\mathbf{b})
        ,\]
        which is a point.

        If $ \mathbf{a}\cdot \mathbf{b}=1 $ and RHS$\neq 0$, then it is inconsistent.

        If $ \mathbf{a}\cdot \mathbf{b} $ and $\mathbf{c}-(\mathbf{a}\cdot \mathbf{c})\mathbf{b}=\mathbf{0}$, then 
        \[
            (\mathbf{a}\cdot \mathbf{r}-\mathbf{a}\cdot \mathbf{c})\mathbf{b}=\mathbf{0}
        .\] 
        Hence it is a plane.
    \end{enumerate}
    \section{Index notation and the summation convention}
    \subsection{Components, $\delta\& \epsilon$}
    Write vectors $ \mathbf{a},\mathbf{b},\dots $ in terms of components $ a_i,b_i,\dots $ wrt an orthonormal right-handed basis $ \left\{ \mathbf{e}_i\right\} $. Indices $i,j,\dots$ take values $1,2,3$.

    For example, if $ \mathbf{c}=\alpha \mathbf{a}+\beta \mathbf{b} $, then $ c_i=[\alpha \mathbf{a}+\beta \mathbf{b}]_i=\alpha a_i+\beta b_i $, for $i=1,2,3$. $i$ is called a \textit{free index}.

    Hence
    \begin{itemize}
        \item $\displaystyle \mathbf{a}\cdot \mathbf{b} = \sum_{i=1}^{3}a_ib_i$.
        \item $\displaystyle \mathbf{x}=\mathbf{a}+(\mathbf{b}\cdot \mathbf{c})\mathbf{d} \Leftrightarrow x_j=a_j+\left( \sum_{k=1}^{3} b_kc_k \right)d_j $. 
    \end{itemize}
    \begin{definition}[Kr\"{o}necker delta]
        \[
            \delta_{ij} = \begin{cases}
            1 &\text{ if }i=j\\
            0 &\text{ if }i\neq j\\
            \end{cases}
        \] 
    \end{definition}
    We see that $ \delta_{ij}=\delta_{ji} $ and also 
    \[
        \left(\begin{array}{lll}
            \delta_{11} & \delta_{12} & \delta_{13} \\
            \delta_{21} & \delta_{22} & \delta_{23} \\
            \delta_{31} & \delta_{32} & \delta_{33}
            \end{array}\right)=\left(\begin{array}{lll}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
            \end{array}\right)
    \]
    Using this, $ \mathbf{e}_i\cdot \mathbf{e}_j=\delta_{ij} $
    \begin{definition}[Levi-Civita epsilon]
        \[
            \epsilon_{ijk}=\begin{cases}
            1 &\text{ if $ (i,j,k) $ is an even permutation of $ (1,2,3) $,}\\
            -1 &\text{ if $ (i,j,k) $ is an odd permutation of $ (1,2,3) $,}\\
            0 &\text{ else.}\\
            \end{cases} 
        \]
    \end{definition}
    We have $ \epsilon_{123}=\epsilon_{231}=\epsilon_{312}=1, \epsilon_{321}=\epsilon_{213}=\epsilon_{132}=-1 $.
    $ \epsilon_{ijk} $ is totally anti-symmetric: exchanging any pair of indices produces a change in sign.

    Then 
    \[
        \mathbf{e}_i \times \mathbf{e}_j = \sum_{k}\epsilon_{ijk}\mathbf{e}_k
    \]
    and 
    \[
        \begin{aligned}
            \mathbf{a} \times \mathbf{b}&= \left( \sum_{i} a_i \mathbf{e}_i \right)\times \left( \sum_{j} b_j \mathbf{e}_j \right)\\
            &= \sum_{ij}a_ib_j \mathbf{e}_i \times \mathbf{e}_j\\
            &= \sum_{ij}a_ib_j\sum_{k}\epsilon_{ijk}\mathbf{e}_k=\sum_{ijk}a_ib_j\epsilon_{ijk}\mathbf{e}_k
        \end{aligned}
    \]
    so 
    \[
        \boxed{(\mathbf{a}\times \mathbf{b})_k=\sum_{ij}\epsilon_{ijk}a_ib_j}
    .\]
    \subsection{Summation convention}
    With components and index notation, indices that appear twice in a given term are usually summed over. In the summation convention, we omit the sum signs for repeated indices. i.e., the sum is understood.
    \begin{example}
        \begin{enumerate}[(i)]
            \item In $ a_i \delta_{ij} = a_1\delta_{1j}+a_2\delta_{2j}+a_3 \delta_{3j}=a_j $, since $ \Sigma_{i} $ is understood.
            \item $ \mathbf{a}\cdot \mathbf{b}=\delta_{ij}a_{i}b_{j}=a_ib_i $. $ \Sigma_{ij}, \Sigma_{i} $ are understood.
            \item $ (\mathbf{a}\times \mathbf{b})_i=\epsilon_{ijk}a_jb_k $, $ \Sigma_{jk} $ is understood.
            \item $ [\mathbf{a},\mathbf{b},\mathbf{c}]=\epsilon_{ijk}a_ib_jc_k $, $ \Sigma_{ijk} $ is understood.
            \item $ \delta_{ii}=\delta_{11}+\delta_{22}+\delta_{33}=3 $.
            \item 
            \[
                \begin{aligned}
                    [(\mathbf{a}\cdot \mathbf{c})\mathbf{b}-(\mathbf{a}\cdot \mathbf{b})\mathbf{c}]_i&=(\mathbf{a}\cdot \mathbf{c})b_i-(\mathbf{a}\cdot \mathbf{b})c_i\\
                    &= a_jc_jb_i-a_jb_jc_i.
                \end{aligned}
            \]
            $ \Sigma_j $ is understood.
        \end{enumerate}
    \end{example}
    Here are the rules of summation convention.
    \begin{enumerate}[(1)]
        \item An index occuring exactly once in any given term must appear once in every term in an equation, and it can take any value in $ 1,2,3 $, a \textit{free} index.
        \item An index occuring exactly twice in a given term is summed over. A \textit{repeated}, \textit{contracted}, or \textit{dummy} index.
        \item No index can occur more than twice in any given term.
    \end{enumerate}
    \subsection{Applications}
    We can use this to prove the vector triple product identity.

    \begin{proof}
        Write the huge sum in summation convention:
        \begin{IEEEeqnarray*}{rCl}
            [\mathbf{a}\times (\mathbf{b}\times \mathbf{c})]_i & = & \epsilon_{ijk}a_j(\mathbf{b}\times \mathbf{c})_k
        \\
            & = & \epsilon_{ijk}a_j\epsilon_{kpq}b_pc_q
        \\
            & = & (\epsilon_{ijk}\epsilon_{kpq})a_jb_pc_q.
        \end{IEEEeqnarray*}
        Notice that
        \begin{equation}
            \epsilon_{ijk}\epsilon_{kpq}=\delta_{ip}\delta_{jq}-\delta_{iq}\delta_{jp} \tag{*}
        \end{equation}
        see next subsection. So 
        \[
            [\mathbf{a}\times (\mathbf{b}\times \mathbf{c})]_i=\delta_{ip}\delta_{jq}a_jb_pc_q-\delta_{iq}\delta_{jp}a_jb_pc_q
        .\]
        Notice also that $ a_i \delta_{ij}=a_j $, so
        \[
            [\mathbf{a}\times (\mathbf{b}\times \mathbf{c})]_i = a_qb_ic_q-a_jb_jc_i = (\mathbf{a}\cdot \mathbf{c})b_i-(\mathbf{a}\cdot \mathbf{b})c_i
        .\]
        Hence the equation \ref{eq:vector triple prod identity} is proved.
    \end{proof}
    \subsection{$\epsilon\, \epsilon$ identity}
    \begin{proposition}\label{prop:eeidentity1}
        $ \epsilon_{ijk}\epsilon_{pqk}= \delta_{ip}\delta_{jq}-\delta_{iq}\delta_{jp}=\epsilon_{kij}\epsilon_{kpq}$.
    \end{proposition}
    \begin{proof}
        Notice that LHS and RHS are both anti-symmetric, so both vanish when $ i,j $ or $ p,q $ take the same value. Inspection shows that\mn{Think carefully here.} it suffices to show the cases $ i=p=1 \land  j=q=2 $ or $ i=q=1, j=p=2 $ and all other index changings that give non-zero results.
    \end{proof}

    \begin{proposition}\label{prop:eeidentity2}
        $ \epsilon_{ijk}\epsilon_{pjk}=2\delta_{ip}. $
    \end{proposition}
    \begin{proof}
        Take $q=j$ in the above equation:
        \[
            \epsilon_{ijk}\epsilon_{pjk}=\delta_{ip}\delta_{jj}-\delta_{ij}\delta_{jp}=3\delta_{ip}-\delta_{ip}=2\delta_{ip}
        .\]
    \end{proof}
    \begin{proposition}\label{prop:eeidentity3}
        $  \epsilon_{ijk}\epsilon_{ijk}=6 $.
    \end{proposition}
    \begin{proposition}\label{prop:eeidentity4}
        \[
            \begin{aligned}
                \epsilon_{ijk}\epsilon_{pqr}&=\delta_{ip}\delta_{jq}\delta_{kr}-\delta_{jp}\delta_{iq}\delta_{kr}\\
                &+\delta_{jp}\delta_{kq}\delta_{ir}-\delta_{kp}\delta_{jq}\delta_{ir}\\
                &+\delta_{kp}\delta_{iq}\delta_{jr}-\delta_{ip}\delta_{kq}\delta_{jr}.
            \end{aligned}
        \]
    \end{proposition}
    \begin{proof}
        Total anti-symmetry\mn{This simplifies most of the process and leaves only one case to check.} in $i,j,k$ and independently in $p,q,r$ implies LHR, RHS agree up to an overall factor. To check the factor is 1, consider $i=p=1, j=q=2, k=r=3$.
    \end{proof}
    \part{Vectors in General}
    \section{Vectors in $ \mathbb{R}^n $}
    \subsection{Definition and basic properties}\marginnote{Lecture 6}
    \begin{definition}
        Regard vectors as sets of components, and let 
        \[
            \mathbb{R}^n = \left\{ \mathbf{x}=(x_1,\dots, x_n): x_i\in \mathbb{R} \right\}
        .\]
        Define:
        \begin{itemize}
            \item Addition: $ \mathbf{x}+\mathbf{y}=(x_1+y_1,\dots,x_n+y_n) $,
            \item Scalar multiplication: $ \lambda \mathbf{x}=(\lambda x_1,\dots,\lambda x_n) $.
            \item Linear combinations: $ \lambda \mathbf{x}+\mu \mathbf{y} $,
            \item Parallel: $ \mathbf{x} \parallel \mathbf{y} \Leftrightarrow \mathbf{x}=\lambda \mathbf{y} \lor \mathbf{y}=\lambda \mathbf{x} $.
            \item Inner Product(Scalar product): $ \mathbf{x}\cdot \mathbf{y}= \sum_{i=1}^{n} x_iy_i $.
        \end{itemize}
    \end{definition}
    Properites of inner product:
    \begin{enumerate}[(1).]
        \item Symmetric: $ \mathbf{x}\cdot \mathbf{y}=\mathbf{y}\cdot \mathbf{x} $.
        \item Bilinear: 
        \[
            \begin{aligned}
                (\lambda \mathbf{x}+\lambda' \mathbf{x}')\cdot \mathbf{y}&=\lambda \mathbf{x}\cdot \mathbf{y}+\lambda' \mathbf{x}'\cdot \mathbf{y},\\
                \mathbf{x}\cdot (\mu\mathbf{y}+\mu' \mathbf{y}')&=\mu \mathbf{x}\cdot \mathbf{y}+\mu' \mathbf{x}' \cdot \mathbf{y}.
            \end{aligned}
        \]
        \item Positive definite: $ \mathbf{x}\cdot \mathbf{x}\ge 0 $, with $=$ holds if and only if $ \mathbf{x}=\mathbf{0} $.
    \end{enumerate}
    \subsection{Norm of a vector}
    \begin{definition}
        The \textit{norm} of a vector $ \mathbf{x} $ is denoted as $ |\mathbf{x}| $ with $ |\mathbf{x}|^2=\mathbf{x}\cdot \mathbf{x} $.

        $ \mathbf{x},\mathbf{y} $ are called \textit{orthogonal} if $ \mathbf{x}\cdot \mathbf{y}=0 $, denote as $ \mathbf{x} \perp \mathbf{y} $.
    \end{definition}
    The \textit{standard basis} of $ \mathbb{R}^n $ is
    \[
        e_i = (0,\dots,1,\dots,0)
    \]
    with 1 on the $i$th position. So that 
    \[
        \mathbf{x}=\sum_{i=1}^{n}x_i \mathbf{e}_i
    \]
    and $ \mathbf{e}_i \cdot \mathbf{e}_j = \delta_{ij} $. i.e., standard basis is orthogonal.
    \subsection{Cauchy-Schwarz and Triangle inequalities}
    \begin{proposition}[Cauchy-Schwarz]\label{prop:cauchy-schwarz}
        \[
            \forall \mathbf{x}, \mathbf{y}\in \mathbb{R}^n,\quad |\mathbf{x}\cdot \mathbf{y}|\le |\mathbf{x}| | \mathbf{y}|
        \]
        with equality if and only if $ \mathbf{x}\parallel \mathbf{y} $.
    \end{proposition}
    General deductions:
    \begin{enumerate}[(i).]
        \item Setting $ \mathbf{x}\cdot \mathbf{y}=|\mathbf{x}||\mathbf{y}|\cos \theta $, we can define angle $ \theta $ between $ \mathbf{x},\mathbf{y}\in \mathbb{R}^n $.
        \item We have the \textit{triangle inequality}:
        \[
            |\mathbf{x}+\mathbf{y}|\le|\mathbf{x}|+|\mathbf{y}|
        .\]
    \end{enumerate}
    \begin{proof}
        If $ \mathbf{y}=\mathbf{0} $, then the result is immediate. If not, consider
        \[
            \begin{aligned}
                 \left| \mathbf{x}-\lambda \mathbf{y} \right|&= (\mathbf{x}-\lambda \mathbf{y})(\mathbf{x}-\lambda \mathbf{y})\\
                 &= \left| \mathbf{x} \right|^2-2\lambda \mathbf{x}\cdot \mathbf{y}+\lambda^2 |\mathbf{y}|^2\ge 0.
            \end{aligned}
        \]
        This is a real equation of $\lambda$ with at most one root, so 
        \[
            (-2 \mathbf{x}\cdot \mathbf{y})^2-4|\mathbf{x}|^2|\mathbf{y}|^2\le 0 \Longleftrightarrow |\mathbf{x}\cdot \mathbf{y}|\le |\mathbf{x}| | \mathbf{y}|
        .\]
        Equality holds if and only if $ \mathbf{x}=\lambda \mathbf{y} $.

        Note also that for triangle inequality:
        \[
            \begin{aligned}
                 |\mathbf{x}+\mathbf{y}|^2&= |\mathbf{x}|^2+2 \mathbf{x}\cdot \mathbf{y}+|\mathbf{y}|^2\\
                 &\le |\mathbf{x}|^2+2 |\mathbf{x}| | \mathbf{y}|+|\mathbf{y}|^2\\
                 &=(|\mathbf{x}|+|\mathbf{y}|)^2,
            \end{aligned}
        \]
        as required.
    \end{proof}
    \subsection{Inner Products and Cross products}
    Inner product in $ \mathbb{R}^n $ can be written as
    \[
        \mathbf{a}\cdot \mathbf{b}=\delta_{ij}a_{i}b_{j},\quad \text{by summation convention}
    .\]
    For $n=3,$ it matches geometrical definition.

    We can also define cross product in component definition. In 3d we have
    \[
        (\mathbf{a}\times \mathbf{b})_i=\epsilon_{ijk}a_jb_k
    ,\]
    and in $n$ dimensions we have $\epsilon_{ij\cdots l}$ which is totally anti-symmetric. But there are only two $a_ib_j$ so we cannot use this to define vector product in general.

    However, in $ \mathbb{R}^2 $ we have $ \epsilon_{ij} $ with $ \epsilon_{12}=-\epsilon_{21}=1 $, so can use this to define a new scalar product 
    \[
        [\mathbf{a},\mathbf{b}]=\epsilon_{ij}a_ib_j=a_1b_2-a_2b_1
    .\]
    Geometrically, this the (signed) area of parallelogram formed by $ \mathbf{a},\mathbf{b} $ and 
    \[
        |[\mathbf{a},\mathbf{b}]|=|\mathbf{a}||\mathbf{b}|\sin \theta
    .\]
    Compare with $ [\mathbf{a},\mathbf{b},\mathbf{c}]=\epsilon_{ijk}a_ib_jc_k $.
    \section{Vector Spaces}
    \subsection{Axioms, span, and subspaces}
    \begin{definition}
        Let $V$ be a set of objects called \textit{vectors} with operation
        \[
            \begin{aligned}
                 \mathbf{v}+\mathbf{w}\in V&\quad \forall \mathbf{v},\mathbf{w}\in V\\
                 \lambda \mathbf{v}\in V&\quad \forall \mathbf{v}\in V, \lambda\in \mathbb{R}.
            \end{aligned}
        \]
        Then $V$ is called a \textit{real vector space} if 
        \begin{enumerate}[(i).]
            \item $V$ with $+$ is an abelian group.
            \item $ \lambda(\mathbf{v}+\mathbf{w})=\lambda \mathbf{v}+\lambda \mathbf{w} $
            \item $ (\lambda+\mu)\mathbf{v}=\lambda \mathbf{v}+\mu \mathbf{v} $
            \item $ \lambda(\mu \mathbf{v})=(\lambda \mu) \mathbf{v} $
            \item $ 1 \mathbf{v}=\mathbf{v} $.
        \end{enumerate}
    \end{definition}
    \begin{example}
        Let $ V=\left\{ f:[0,1] \to \mathbb{R} : f \land f(0)=f(1)=0\right\} $. By smooth we mean $f$ is differentiable infinitely many times. Then $V$ is a real vector space with $+$ defined as $ (f+g)(x)=f(x)+g(x) $ and $ (\lambda f)(x)=\lambda(f(x)) $. Then all axioms apply.
    \end{example}
    \begin{definition}\marginnote{Lecture 7}
        A \textit{subspace} of a real vector space $V$ is a subset $U \subseteq V$ that is also a vector space. 
    \end{definition}
    \begin{remark}
        A non-empty subset is a subspace if and only if $ \forall v,w\in U, \lambda v+ \mu u\in U $.
    \end{remark}
    For any vectors $ v_1,\dots,v_r\in V $, their \textit{span} $ \spn\left\{ v_1,\dots,v_r\right\}=\left\{ \lambda_1 v_1+\cdots+\lambda_r v_r:v_i\in \mathbb{R} \right\} $ is a subspace. $V$ and $ \left\{ 0\right\} $ are subspaces of $V$.
    \begin{example}
        A line or plane through $O$ is a subspace in $ \mathbb{R}^3 $, but a line or plane that does not contain $ \mathbf{0} $ is not a subspace.
    \end{example}
    \subsection{Linear dependence and independence}
    For $ v_1,\dots,v_r\in V $, a real vector space, consider a linear relation
    \[
        \lambda_1 v_1+\cdots+\lambda_r v_r=0\tag{*}
    .\]
    If $ (*)\Rightarrow \lambda_i=0 $, then the vectors form a \textit{linearly independent set}. They obey only the trivial linear relation.

    If $ (*) $ holds with at least $ \lambda_k\neq 0 $, then the vectors form a \textit{linearly dependent} set. They obey a non-trivial linear relation.
    \begin{example}
        In $ \mathbb{R}^2 $, $ \left\{ (1,0),(0,1),(0,2)\right\} $ is linearly dependent.

        \bluecomment{We cannot express $(1,0)$ in terms of the others.}
    \end{example}
    Several facts:
    \begin{itemize}
        \item Any set containing $0$ is linearly dependent.
        \item In $ \mathbb{R}^3 $, $ \left\{ \mathbf{a}\right\} $ is linearly independent if and only if $ \mathbf{a}\neq \mathbf{0} $.
        \item $ \left\{ \mathbf{a},\mathbf{b}\mathbf{c}\right\} $ is linearly independent if $ [\mathbf{a},\mathbf{b},\mathbf{c}]\neq 0 $. Since if 
        \[
            \alpha \mathbf{a}+\beta \mathbf{b}+\gamma \mathbf{c}=0
        ,\]
        then dotting with $ \mathbf{b}\times \mathbf{c} $ we get $ \alpha[\mathbf{a},\mathbf{b},\mathbf{c}]=0 \Rightarrow \alpha=0 $. Similarly $ \beta=0, \gamma=0 $.
    \end{itemize}
    \subsection{Inner products}
    This is an additional structure on a real vector space $V$, that can also be characterised by axioms or key properties.

    For $v,w\in V$, denote inner product by 
    \[
        v\cdot w \text{  or  } (v,w)\in \mathbb{R} 
    .\]
    Require this satisfies 1. it's symmetric, 2. it's bilinear, 3. it is positive definite.

    Definition of length or norm and deductions such as Cauchy-Schwarz inequality depend just on these properties.

    \begin{example}
        Consider space of functions 
        $$ V=\left\{ f:[0,1]\to \mathbb{R} : f \text{ smooth}\land f(0)=f(1)=0\right\} .$$
        Define an inner product by 
        \[
            (f,g)=\int_{0}^{1} f(x)g(x) \,\mathrm{d}x
        .\]
        which has properties 123. Cauchy-Schwarz holds:
        \[
            |(f,g)|\le \left\| f \right\| \left\| g \right\| 
        \]
        with $ \left\| f \right\|^2 =(f,f) $. i.e.
        \[
            \left| \int_{0}^{1} f(x)g(x) \,\mathrm{d}x \right| \le \left( \int_{0}^{1} f(x) \,\mathrm{d}x \right)^{1/2}\left( \int_{0}^{1} g(x) \,\mathrm{d}x \right)^{1/2}
        .\]
    \end{example}
    \begin{lemma}\label{lma:innerprod}
        In any real vector space $V$ with an inner product, if $v_1,v_2,\dots,v_r$ are non-zero and orthogonal vectors, then they are linearly independent.
    \end{lemma}
    \begin{proof}
        If 
        \[
            \sum_{i}\alpha_i v_i=0
        ,\]
        then
        \[
            (v_j,\sum_{i}\alpha_i v_i)=0 \Longleftrightarrow \alpha_j=0
        .\]
    \end{proof}
    \section{Bases and dimension}
    \begin{definition}
        For a vector space $V$, a \textit{basis} is a set 
        \[
            \mathfrak{B}= \left\{ e_1,\dots, e_n\right\}
        \]
        such that 
        \begin{enumerate}[(i)]
            \item $ \mathfrak{B} $ spans $V$. i.e., $ \forall v\in V $, 
            \[
                v = \sum_{i=1}^{n}v_i e_i
            .\]
            \item $ \mathfrak{B} $ is linearly independent.
        \end{enumerate}
        Given (ii), the coefficients $v_i$ in (i) are unique, since 
        \[
            \sum_{i}v_ie_i=\sum_{i}v_i'e_i \Longleftrightarrow v_i-v_i'=0 \Longleftrightarrow v_i=v_i'
        .\]
    \end{definition}
    \begin{example}
        Standard basis for $ \mathbb{R}^n $ consists of 
        \[
            e_1=\begin{pmatrix}
                1\\
                0\\
                \vdots\\
                0
            \end{pmatrix},
            e_2 = \begin{pmatrix}
                0\\
                1\\
                \vdots\\
                0
            \end{pmatrix},
            \dots,
            e_n=\begin{pmatrix}
                0\\
                0\\
                \vdots\\
                1
            \end{pmatrix}
        .\]
        Many other bases can be chosen.
    \end{example}
    \begin{theorem}\label{thm:dimension}
        If $ \left\{ e_1,\dots,e_n\right\} $, $ \left\{ f_1,\dots,f_m\right\} $ are bases for a real vector space $V$, then $m=n$.
    \end{theorem}
    \begin{proof}
        We have 
        \[
            \begin{aligned}
                 f_a&= \sum_{i}A_{ai}e_i,\\ 
                 e_i&= \sum_{a}B_{ia}f_a
            \end{aligned}
        \]
        for $ A_{ai},B_{ia}\in \mathbb{R} $.
        Hence
        \[
            \begin{aligned}
                f_a&=\sum_{i}A_{ai}\sum_{b}B_{ib}f_{b}\\
                &= \sum_{b}\sum_{i}A_{ai}B_{ib}f_b
            \end{aligned}
        \]
        But the coefficients are unique, so 
        \[
            \sum_{i}A_{ai}B_{ib}=\delta_{ab}
        .\]
        Similarly,
        \[
            \sum_{a}B_{ia}A_{aj}=\delta_{ij}
        .\]
        Now, 
        \[
            \sum_{i,a}A_{ai}B_{ia}=\sum_{a} \delta_{aa}=m = \sum_{i}\delta_{ii}=n,\\
        .\]
    \end{proof}
    \begin{definition}
        The number of vectors in any basis is the \textit{dimension} of the vector space.
    \end{definition}
    \begin{remark}\marginnote{Lecture 8}
        $\{0\}$ is called the \textit{trivial} vector space and has dimension 0.

        The steps in the proof of basis theorem are within scope of this course, but the proof without prompts non-examinable. The same applies to the following:
    \end{remark}
    \begin{proposition}\label{prop:reduce_span}
        Let $V$ be a vector space with finite subsets $ Y=\left\{ w_1,\dots,w_m\right\}$ and $ X=\left\{ u_1,\dots,u_k\right\} $, with $Y$ spans $V$ and $X$ linearly independent. Then 
        \[
            k\le \dim V\le m
        .\]
        And 
        \begin{enumerate}[(1)]
            \item A basis can be found as a subset of $Y$ be discarding vectors as necessary.
            \item $X$ can be extended to a basis by adding vectors from $Y$ as necessary.
        \end{enumerate}
    \end{proposition}
    \begin{proof}
        (1) If $Y$ is linearly independent, then $Y$ is a basis, and $m=n=\dim V$. If $Y$ is not, then 
        \[
            \sum_{i=1}^{m}\lambda_i w_i=0
        ,\]
        where $ \lambda_i $ are not all zero. wlog, can take $\lambda_m\neq 0.$ Then 
        \[
            w_m=-\frac{1}{\lambda_m}\sum_{i=1}^{m-1}\lambda_i w_i
        ,\]
        so $ \spn Y=\spn (Y\setminus \left\{ w_m\right\})=\spn Y' $. We repeat, until a basis is obtained.

        (2) If $X$ spans $V$ then $X$ is a basis $k=n=\dim V$. If not $ \exists u_{k+1} $ not in $\spn X$. Consider 
        \[
            \sum_{i=1}^{k+1}\mu_i u_i=0
        \]
        and thus $\mu_{i}=0, \forall i\in \left\{ 1,2,\dots,k+1\right\}$. Hence 
        \[
            X'=X \cup \left\{ u_{k+1}\right\}
        \]
        is linearly independent.

        Furthermore, we can choose $u_{k+1}\in Y$ since otherwise $ \spn X=V $, \#. Repeat this until a basis is achieved. The process stops since $Y$ is finite.
    \end{proof}
    In this course, we will deal only with finite-dimensional spaces, except examples mentioned.
    \begin{example}
        $ V=\left\{ f:[0,1] \to \mathbb{R} : f\text{ smooth } \land f(0)=f(1)=0\right\} $. Note that 
        \[s_n(x)=\sqrt{2}\sin(n\pi x)\]
        belong to $V$ and 
        \[
            (s_n,s_m)=2 \int_{0}^{1} \sin(n\pi x)\sin(m\pi x) \,\mathrm{d}x=\delta_{nm},
        \]
        so these functions are orthonormal and thus linearly independent. So $V$ is infinite-dimensional.
    \end{example}
    \section{Vectors in $\mathbb{C}^n$}
    \subsection{Introduction and definitions}
    Let $ \mathbb{C}^n=\left\{ \mathbf{z}=(z_1,\dots,z_n):z_j\in \mathbb{C}\right\} $ and define 
    \begin{itemize}
        \item Addition: $ \mathbf{z}+\mathbf{w}=(z_1+w_1,\dots,z_n+w_n) $,
        \item Scalar multiplication: $ \lambda \mathbf{z}=(\lambda z_1,\dots, \lambda z_n) $.
    \end{itemize}
    If scalars $ \lambda,\mu\in \mathbb{R} $, then $ \mathbb{C}^n $ is a real vector space, and axioms apply.

    If $ \lambda, \mu\in \mathbb{C}  $, $ \mathbb{C}^n $ is a complex vector space. The same axioms hold, and definitions of linear combinations, linear dependence/independence, bases, dimension are generalised to $\mathbb{C}$.

    The distinction between real and complex scalars is important.
    \begin{example}
        $ \mathbf{z}=(z_1,\dots,z_n)\in \mathbb{C}^{n} $ with $ z_j=x_j+iy_j, x_j, y_j \in \mathbb{R} $. Then 
        \[
            \mathbf{z}=\sum_{j} x_j \mathbf{e}_j + \sum_{j} y_j \mathbf{f}_j,
        \]
        a linear combination of $ \mathbf{e} $, the usual standard basis in $\mathbb{R}^{n}$, and $ \mathbf{f}_j=(0,\dots,i,\dots,0) $.
    \end{example}
    We can see that $ \left\{ \mathbf{e}_1,\dots,\mathbf{e}_n,\mathbf{f}_1,\dots,\mathbf{f}_n\right\} $ is a basis for $\mathbb{C}^{n}$ as a \textit{real} vector space, so it has dimension $2n$.
    
    However,
    \[
        \mathbf{z}=\sum_{j}z_j \mathbf{e}_j
    \]
    is a \textit{complex} linearl combination, so the basis of $ \mathbb{C}^{n} $ as a \textit{complex} vector space is simply $ \left\{ \mathbf{e}_1,\dots,\mathbf{e}_n\right\} $ and the dimension is $n$ over $ \mathbb{C} $.

    From now on we will view $ \mathbb{C}^{n} $ as a complex vector space unless mentioned otherwise.
    \subsection{Inner product}
    The inner product on $ \mathbb{C}^{n} $ is defined by 
    \[
        (\mathbf{z},\mathbf{w})=\sum_{j=1}^n \bar{z}_j w_j
    .\]
    It has the followin properties:
    \begin{enumerate}[(1)]
        \item It is \textit{hermitian}: $ (\mathbf{w},\mathbf{z})=\overline{(\mathbf{z},\mathbf{w})} $.
        \item It is linear/anti-linear: $ (\mathbf{z}, \lambda \mathbf{w} +\lambda' \mathbf{w}')=\lambda(\mathbf{z},\mathbf{w})+\lambda'(\mathbf{z},\mathbf{w}') $. But $ (\lambda \mathbf{z}+\lambda' \mathbf{z}',w)=\bar{\lambda}(\mathbf{z},\mathbf{w})+\bar{\lambda'}(\mathbf{z},\mathbf{w}') $.
        \item Positive definite: $ (\mathbf{z},\mathbf{z})\in \mathbb{R} \land \ge 0 $. $ =0 $ if and only if $ \mathbf{z}=\mathbf{0} $.
    \end{enumerate}
    Define the norm of $ \mathbf{z} $ to be 
    \[
        \left| \mathbf{z} \right| \ge 0, |\mathbf{z}|^2=(\mathbf{z},\mathbf{z})
    .\]
    Also define $ \mathbf{z}, \mathbf{w}\in \mathbb{C}^{n} $ to be \textit{orthogonal} if $ (\mathbf{z},\mathbf{w})=0. $

    Note that the standard basis for $ \mathbb{C}^{n} $ is orthonormal. That is,
    \[
        (\mathbf{e}_j,\mathbf{e}_k)=\delta_{jk}
    .\]
    \begin{example}
        Complex inner product of $ \mathbb{C}^{1} $ is 
        \[
            (z,w)=\bar{z}w 
        .\]
        Let $ z=a_1+ia_2,w=b_1+ib_2 $, and considers $ \mathbf{a}=(a_1,a_2),\mathbf{b}=(b_1,b_2)\in \mathbb{R}^{2} $. Then 
        \[
            \bar{z}w=a_1b_1+a_2b_2+i(a_1b_2-a_2b_1)=\mathbf{a}\cdot \mathbf{b}+i[\mathbf{a},\mathbf{b}]
        ,\]
        recover 2 scalar products in $ \mathbb{R}^2 $.
    \end{example}\newpage
    \part{Matrices and Linear Maps}
    \section{Introduction}\marginnote{Lecture 9}
    \subsection{Definitions}
    \begin{definition}
        A \textit{linear map} or \textit{linear transformation} is a function $ T:V\to W $ between $V$ with $\dim V=n$ and $W$ with $\dim W=m$ such that 
        \[
            T(\lambda x+ \mu y)=\lambda T(x)+\mu T(y)
        \]
        where $ x,y\in V $ and $ \lambda,\mu\in \mathbb{R}$ or $ \mathbb{C} $, depending on whether $V,W$ are complex vector spaces.

        $ x'=T(x)\in W $ is called the \textit{image} of $x$ under $T$. Define
        \[
            \begin{aligned}
                 \im(T)&=\left\{ x'\in W: \exists x\in V, x'=T(x)\right\},\\
                \ker(T)&=\left\{ x\in V:T(x)=0\in W\right\}.
            \end{aligned}    
        \]
    \end{definition}
    \begin{remark}
        A linear map is determined by its action on a basis. We have 
        \[
            T\left( \sum_{i}x_i e_i \right)=\sum_{i}x_i T(e_i)
        .\]
    \end{remark}
    \begin{lemma}\label{lma:ker,im subspace}
        $ \im(T),\ker(T) $ are subspace of $ W,V $ respectively.
    \end{lemma}
    \begin{proof}
        Note that $ 0\in \im(T) $ and $ \forall x',y'\in\im(T)  $, let $ T(x)=x',T(y)=y' $, then $  \lambda x'+\mu y'=\lambda T(x)+\mu T(y)=T(\lambda x+\mu y)\in \im(T) $, so it is a subspace. $\ker(T)$ is proved similarly.
    \end{proof}
    \begin{example}
        \begin{enumerate}[(1)]
            \item Zero linear map: $ T:V\to W $ that $ T(v)=0 $. We have $ \im(T)=\left\{ 0\right\} $ and $ \ker(T)=V $.
            \item Identity map: $ T:V\to V $ that $ T(v)=v $. $ \im(T)=V, \ker(T)=\left\{ 0\right\} $.
            \item Let $ V=W=\mathbb{R}^3 $ and $ T(x)=x' $ where 
            \[
                \begin{array}{l}
                    x_{1}^{\prime}=3 x_{1}+x_{2}+5 x_{3}, \\
                    x_{2}^{x}=-x_{1}-2 x_{3}, \\
                    x_{3}^{\prime}=2 x_{1}+x_{2}+3 x_{3}.
                \end{array}
            \]
            Then $T$ is indeed a linear map and
            \[
                \im(T)=\left\{ \lambda \begin{pmatrix}
                    3\\-1\\2
                \end{pmatrix}+\mu \begin{pmatrix}
                    1\\0\\-1
                \end{pmatrix} \right\},\quad \ker(T)=\left\{ \lambda \begin{pmatrix}
                    2\\-1\\-1
                \end{pmatrix}\right\}
            .\]
            Note that $\im(T)$ is a plane and $\ker(T)$ is a line.
        \end{enumerate}
    \end{example}
    \subsection{Rank and Nullity}
    Define $ \rank(T)=\dim \im(T) $ and $ \nullity(T)=\dim \ker(T) $.
    \begin{theorem}\label{thm:r-n thm}
        For $ T:V\to W $ a linear map, then 
        \[
            \rank(T)+\nullity(T)=\dim V
        .\]
    \end{theorem}
    \begin{proof}
        Let $ \left\{ e_1,\dots,e_k\right\} $ be a basis of $\ker(T)$. Extend this by $ e_{k+1},\dots,e_n $ to a basis of $V$. Claim that $ \mathcal{B}=\left\{ T(e_{k+1}),\dots,T(e_n)\right\} $ is a basis of $\im(T)$. The result clearly follows.

        Indeed, $ \mathcal{B} $ spans $\im(T)$ since $ \forall x\in V, x=\sum_{i}x_ie_i $ and 
        \[
            T(x)=\sum_{i=1}^{n}x_iT(e_i)=\sum_{i=k+1}^{n}x_iT(e_i)
        .\]
        Suppose 
        \[
            \sum_{i=k+1}^{n}\lambda T(e_i)=0
        .\]
        Thus 
        \[
            \begin{aligned}
                 &T\left( \sum_{i=k+1}^{n}x_ie_i \right)=0\\
                 \Longrightarrow & \sum_{i=k+1}^{n}x_ie_i\in \ker(T)\\
                 \Longrightarrow & \sum_{i=k+1}^{n}x_ie_i=\sum_{i=1}^{k}x_ie_i\\
                 \Longrightarrow & -\sum_{i=1}^{k}x_i e_i+\sum_{i=k+1}^{n}x_ie_i=0\\
                 \Longrightarrow &x_i=0 \text{ for } i=1,\dots,n.
            \end{aligned}
        \]
        Hence $ \mathcal{B} $ is linearly independent and thus it is a base.
    \end{proof}
    \section{Geometrical Examples}
    \subsection{Rotations}
    In $ \mathbb{R}^{2} $, rotations about $\mathbf{0}$ through $ \theta $ is defined by 
    \[\begin{aligned}
         \mathbf{e}_1\mapsto \mathbf{e}_1'&=\cos(\theta)\mathbf{e}_1+\sin(\theta)\mathbf{e}_2,\\
         \mathbf{e}_2\mapsto \mathbf{e}_2'&=-\sin(\theta)\mathbf{e}_1+\cos(\theta)\mathbf{e}_2.
    \end{aligned}\]
    In $ \mathbb{R}^{3} $, can extend so that $ \mathbf{e}_3\mapsto \mathbf{e}_3 $

    To generalise to arbitrary rotations of $\theta $ along axis $ \mathbf{n} $, where $ \mathbf{n} $ is a unit vector, resolve horizontally and vertically:
    \[
        \mathbf{x}=\mathbf{x}_{\parallel}+\mathbf{x}_{\perp}
    ,\]
    where $ \mathbf{x}_{\parallel}=(\mathbf{x}\cdot \mathbf{n})\mathbf{n} $.
    Then, 
    \[
        \begin{aligned}
             \mathbf{x}_{\parallel}&\mapsto \mathbf{x}_{\parallel },\\
            \mathbf{x}_{\perp }&\mapsto \cos(\theta)\mathbf{x}_{\perp }+\sin(\theta)\mathbf{n}\times \mathbf{x},
        \end{aligned}
    \]
    by considering the plane perpendicular to $ \mathbf{n} $. Note that $ |\mathbf{x}_{\perp }|=|\mathbf{x}\times \mathbf{n}| $, so the result follows by comparing with $ \mathbb{R}^{2} $ by regarding $ \mathbf{e}_1 $ as $ \mathbf{x}_{\perp } $ and $ \mathbf{e}_2 $ as $ \mathbf{n}\times \mathbf{x} $.

    Hence,
    \[
        \begin{aligned}
            \mathbf{x} &\mapsto (\mathbf{n}\cdot \mathbf{x})\mathbf{x}+\cos \theta (\mathbf{x}-(\mathbf{n}\cdot \mathbf{x})\mathbf{x})+\sin \theta \mathbf{n}\times \mathbf{x}\\
            &= \boxed{\cos \theta \mathbf{x}+(1-\cos \theta)(\mathbf{n}\cdot \mathbf{x})\mathbf{n}+\sin \theta \mathbf{n}\times \mathbf{x}.}
        \end{aligned}
    \]
    \subsection{Reflections and Projections}
    For a plane with unit normal vector $ \mathbf{n} $, define projectin of $ \mathbf{x} $ on the plane as 
    \[
        \begin{aligned}
             \mathbf{x}_{\parallel}& \mapsto \mathbf{0},\\
             \mathbf{x}_{\perp}& \mapsto \mathbf{x}_{\perp}.
        \end{aligned}
    \]
    i.e.,
    \[
        \mathbf{x}\mapsto \mathbf{x}'=\mathbf{x}-(\mathbf{n}\cdot \mathbf{x})\mathbf{n}
    .\]
    Define reflection of $ \mathbf{x} $ wrt the plane by 
    \[
        \begin{aligned}
             \mathbf{x}_{\parallel}& \mapsto -\mathbf{x}_{\parallel},\\
             \mathbf{x}_{\perp}& \mapsto \mathbf{x}_{\perp}.
        \end{aligned}
    \]
    i.e.,
    \[
        \mathbf{x} \mapsto \mathbf{x}'=\mathbf{x}-2(\mathbf{n}\cdot \mathbf{x})\mathbf{n}
    .\]
    Same applies to $ \mathbb{R}^{2} $ by replacing plane by line.
    \subsection{Dilations}
    Given scale factors $ \alpha,\beta,\gamma>0 $. Define a \textit{dilation} along axes by 
    \[
        \begin{aligned}
             \mathbf{e}_1 &\mapsto \mathbf{e}_1'=\alpha \mathbf{e}_1,\\
             \mathbf{e}_2 &\mapsto \mathbf{e}_2'=\beta \mathbf{e}_2,\\
             \mathbf{e}_3 &\mapsto \mathbf{e}_3'=\gamma \mathbf{e}_3.
        \end{aligned}
    \]
    Then $ \mathbf{x}=x_1 \mathbf{e}_1+x_2 \mathbf{e}_2 +x_3 \mathbf{e}_3 \mapsto \mathbf{x}'=\alpha x_1 \mathbf{e}_1+\beta x_2 \mathbf{e}_2 +\gamma x_3 \mathbf{e}_3 $.
    \subsection{Shears}
    Let $ \mathbf{a},\mathbf{b} $ be orthogonal unit vectors in $ \mathbb{R}^{3} $, and $ \lambda $ a real parameter. Define a \textit{shear}
    \[
        \mathbf{x} \mapsto \mathbf{x}'=\mathbf{x}+\lambda \mathbf{a}(\mathbf{x}\cdot \mathbf{b})
    .\]
    Notice that $ \mathbf{a}\mapsto \mathbf{a} $ and $ \mathbf{b} \mapsto \mathbf{b}+\lambda \mathbf{a} $. Definition holds the same way in $ \mathbb{R}^{2} $.
    \section{Matrices as linear maps}\marginnote{Lecture 10}
    \subsection{Definitions}
    Consider a linear map $ T:\mathbb{R}^{n}\to \mathbb{R}^{m} $, with bases $ \left\{ \mathbf{e}_i\right\}_{i=1}^n, \left\{ \mathbf{f}_a\right\}_{a=1}^m $, of the form 
    \[
        T(\mathbf{x})=\mathbf{x}',\quad \mathbf{x}=\sum_{i=1}^{n}x_i \mathbf{e}_i, \mathbf{x}'=\sum_{a=1}^{m} x_a' f_a 
    .\]
    Linearity of $T$ implies we can specify $T$ using $ T(\mathbf{e}_i)=\mathbf{e}_i'=\mathbf{C}_i\in \mathbb{R}^{m} $. Take these $ \mathbf{C}_i $ as \textit{columns} of an $ m \times n $ array or \textit{matrix} $M$ with rows $ \mathbf{R}_a\in \mathbb{R}^{n} $.
    \[
        M=\begin{pmatrix}
            \uparrow & & \uparrow \\
            \mathbf{C}_1 & \cdots & \mathbf{C}_n\\
            \downarrow & & \downarrow 
        \end{pmatrix}=\begin{pmatrix}
            \leftarrow & \mathbf{R}_1 & \rightarrow \\
            &\cdots &\\
            \leftarrow & \mathbf{R}_{m} & \rightarrow 
        \end{pmatrix}
    .\]
    $M$ has \textit{entries} $ M_{ai}\in \mathbb{R} $ where $a$ labels rows and $i$ labels columns. Thus we have 
    \[
        M_{ai} = (\mathbf{C}_i)_a=(\mathbf{R}_a)_i
    .\]
    The action of $T$ is then given by
    \[
        \mathbf{x}'=M \mathbf{x}
    ,\]
    defined by $ x_a'=\sum_{i=1}^{n}M_{ai}x_i = M_{ai}x_i $, by summation convention. In column vector:
    \[
        \begin{pmatrix}
            x_1'\\x_2'\\\vdots\\x_m'
        \end{pmatrix}=\begin{pmatrix}
            M_{11}&M_{12}&\cdots&M_{1n}\\
            M_{21}&M_{22}&\cdots&M_{2n}\\
            \vdots&\vdots&&\vdots\\
            M_{m1}&M_{m2}&\cdots&M_{mn}
        \end{pmatrix}\begin{pmatrix}
            x_1\\x_2\\\vdots\\x_m
        \end{pmatrix}=\begin{pmatrix}
            M_{1i}x_i\\M_{2i}x_i\\\vdots\\M_{mi}x_i
        \end{pmatrix}
    .\]
    Indeed, this matrix does represent $T$, for 
    \[
        \begin{aligned}
            \mathbf{x}'&=T(x_i \mathbf{e}_i)=x_iT(\mathbf{e}_i)=x_i \mathbf{C}_i \\
            \Longrightarrow & x_a'=x_i (\mathbf{C}_i)_a=M_{ai}x_i.
        \end{aligned}
    \]
    Now we can regard properties of $T$ as properties of $M$. For example, 
    \[
        \begin{aligned}
             & \im(T)=\im(M)=\spn\left\{ \mathbf{C}_1,\dots, \mathbf{C}_n\right\};\\
             & x_a'=M_{ai}x_i=(\mathbf{R}_{a})_ix_i= \mathbf{R}_a\cdot \mathbf{x};\\
             & \ker T = \ker M=\left\{ \mathbf{x}:\mathbf{R}_a\cdot \mathbf{x}=0 \text{ for all }a.\right\}.\\
        \end{aligned}
    \]
    \subsection{Examples}
    \begin{example}
        \begin{enumerate}[(1)]
            \item The \textit{zero map} $ \mathbb{R}^{n}\to \mathbb{R}^{m} $ corresponds to the \textit{zero matrix}.
            \item Identity map corresponds to $ I $ where $ I_{ij}=\delta_{ij} $, called the \textit{unit matrix}.
            \item Let $ T:\mathbb{R}^{3}\to \mathbb{R}^{3} $ with $ \mathbf{x}'=T(\mathbf{x})=M \mathbf{x} $ where 
            \[
                M=\begin{pmatrix}
                    3&1&5\\-1&0&-2\\2&1&3
                \end{pmatrix}
            .\]
            We get 
            \[
                \begin{pmatrix}
                    x_1'\\x_2'\\x_3'
                \end{pmatrix}=\begin{pmatrix}
                    3x_1+x_2+5x_3\\-x_1-2x_3\\2x_1+x_2+3x_3
                \end{pmatrix}
            .\]
            Hence if we let 
            \[
                \mathbf{C}_1=\begin{pmatrix}
                    3\\-1\\2
                \end{pmatrix},
                \mathbf{C}_2=\begin{pmatrix}
                    1\\0\\1
                \end{pmatrix},
                \begin{pmatrix}
                    5\\-2\\3
                \end{pmatrix}
            ,\]
            we get 
            \[
                \im T=\im M = \spn\left\{ \mathbf{C}_1,\mathbf{C}_2,\mathbf{C}_3\right\}=\spn\left\{ \mathbf{C}_1, \mathbf{C}_2\right\}
            .\]
            Here we have
            \[
                \mathbf{R}_1=\begin{pmatrix}
                    3&1&5
                \end{pmatrix},\mathbf{R}_2=\begin{pmatrix}
                    -1&0&2
                \end{pmatrix},\mathbf{R}_3=\begin{pmatrix}
                    2&1&3
                \end{pmatrix}
            ,\]
            hence $ \mathbf{R}_2 \times \mathbf{R}_3 = \begin{pmatrix}
                2&-1&-1
            \end{pmatrix}=\mathbf{u} $, where infact $ \mathbf{u} \perp \mathbf{R}_1 $. Hence 
            \[
                \ker T=\ker M=\left\{ \lambda \mathbf{u}:\lambda\in \mathbb{R}\right\}
            .\]
            \item Now we turn to study rotations in $ \mathbb{R}^{2} $ and $ \mathbb{R}^{3} $. The matrix wrt rotation of angle $ \theta $ in $ \mathbb{R}^{2} $ is
            \[
                R=\begin{pmatrix}
                    \cos \theta&-\sin \theta\\
                    \sin \theta&\cos \theta
                \end{pmatrix}
            .\]
            For $ \mathbb{R}^{3} $, note that 
            \[
                \begin{aligned}
                     &\mathbf{x}'=\cos \theta \mathbf{x}+(1-\cos \theta)(\mathbf{n}\cdot \mathbf{x})\mathbf{n}+\sin \theta \mathbf{n}\times \mathbf{x}\\
                     \Longrightarrow &x'_i= \cos \theta x_i+(1-\cos \theta)n_jx_jn_i-\sin \theta \epsilon_{ijk}x_j n_k=R_{ij}\\
                     \Longrightarrow & R_{ij}=\cos \theta \delta_{ij}+(1-\cos \theta)n_in_j-\sin \theta \epsilon_{ijk} n_k.
                \end{aligned}
            \]
            \item Dilations. We have 
            \[
                M=\begin{pmatrix}
                    \alpha&0&0\\
                    0&\beta&0\\
                    0&0&\gamma
                \end{pmatrix}
            .\]
            \item Reflections. To find the matrix $H$ wrt the reflection in plane with normal vector $ \mathbf{n} $, consider 
            \[
                \begin{aligned}
                     x_i'&=x_i-2(x_jn_j)n_i=H_{ij}x_j\\
                     \Longrightarrow & H_{ij}=\delta_{ij}-2n_in_j.
                \end{aligned}
            \]
            \item Shear. We have 
            \[
                \begin{aligned}
                     &\mathbf{x}'=\mathbf{x}+\lambda(\mathbf{b}\cdot \mathbf{x})\mathbf{a}\\
                    \Longrightarrow & x_i'=x_i+\lambda(b_jx_j)a_i=S_{ij}x_j\\
                    \Longrightarrow & S_{ij}=\delta_{ij}+\lambda a_i b_j.
                \end{aligned}
            \]
        \end{enumerate}
    \end{example}
    \subsection{Matrix of a General Linear Map $ V\to W $}
    \begin{definition}
        Consider $ T:V\to W $, between general (real or complex) vector spaces, with dim $n,m$ respectively. Choose $ \left\{ e_i\right\}_{i=1}^n, \left\{ f_a\right\}_{a=1}^m $ as bases of $ V,W $. Define the matrix of $T$ wrt these bases is defined as an $m\times n$ array with entries $ M_{ai}\in \mathbb{R}\text{ or }\mathbb{C}  $ defined by 
        \[
            T(e_i)=\sum_{a=1}^{m}f_a M_{ai}
        .\]
    \end{definition}
    Then $ x'=T(x) \Leftrightarrow x_a'=M_{ai}x_i $.
    \marginnote{Entries in column $i$ of $M$ are components of $T(e_i)$ wrt basis $f_a$.}
    \begin{remark}
        Given choices of bases $ \left\{ e_i\right\},\left\{ f_a\right\} $, $V$ is identified with $ \mathbb{R}^{n} $ and $W$ is identified with $ \mathbb{R}^{m} $, and $T$ is identified with an $m \times n$ matrix $M$. 
    \end{remark}
    \section{Matrix Algebra}\marginnote{Lecture 11.}
    \subsection{Linear Combinations}
    If $ T,S:V\to W $ are two linear maps where $ V,W $ are linear or complex, of dimensions $ n,m $ respectively, then the map 
    \[
        \alpha T+\beta S: V\to W
    \]
    where $ \alpha,\beta\in \mathbb{R} $ or $C$, is also a linear map where 
    \[
        (\alpha T+\beta S)(x)=\alpha T(x)+\beta S(x)
    .\]
    If $ M,N $ are matrices for $ T,S $, then $ \alpha M+\beta N $ is the matrix of $\alpha T+\beta S$ where 
    \[
        (\alpha M+\beta N)_{ai}=\alpha M_{ai}+\beta N_{ai},\quad a=1,\dots,m, i=1,\dots,n
    .\]
    All are with respect to same bases, e.g., standard bases for $ \mathbb{R}^{n},\mathbb{R}^{m} $ or $ \mathbb{C}^{n},\mathbb{C}^{m} $.
    \subsection{Matrix multiplication}
    If $A$ is an $m\times n$ matrix with entries $A_{ai}$ and $B$ is a $n\times p$ matrix with entries $B_{ir}$, then $AB$ is an $m\times p$ matrix with entries 
    \[
        (AB)_{ar}=A_{ai}B_{ir}, \quad a=1,\dots,m,i=1,\dots,n,r=1,\dots p,
    \]
    where summation convention applies to $i$. The product is not defined unless \# of columns of $A=$ \# of rows of $B$.

    Matrix multiplication corresponds to composition of linear maps.
    \begin{example}
        Let $ S:\mathbb{R}^{p}\to \mathbb{R}^{n} $ be $ S(\mathbf{x})=B \mathbf{x}, \mathbf{x}\in \mathbb{R}^{p} $, and $ T:\mathbb{R}^{n}\to \mathbb{R}^{m} $ be $ T(\mathbf{y})=A \mathbf{y}, y\in \mathbb{R}^{n} $, then $ TS:\mathbb{R}^{p}\to \mathbb{R}^{m} $ with 
        \[
            (TS)(\mathbf{x})=(AB) \mathbf{x}
        ,\]
        since 
        \[
            [(AB)\mathbf{x}]_{a}=(AB)_{ar}x_r
        \]
        and
        \[
            \begin{aligned}
                [A(B \mathbf{x})]_{a}&=A_{ai}(B \mathbf{x})_{i}\\
                &= A_{ai}B_{ir}x_r =(AB)_{ar}x_r.
            \end{aligned}
        \]
    \end{example}
    \begin{remark}
        Matrix multiplication does not commute!
    \end{remark}
    \begin{proposition}[Properties of matrix multiplication]\label{prop:Properties of matrix multiplication}
        \[\begin{aligned}
             (\lambda M+\mu N)P&=\lambda M P+\mu N P,\\
             P((\lambda M+\mu N))&=\lambda PM+ \mu PN,\\
            (MN)P&=M(NP),
        \end{aligned}\]
        whenever the products are defined.
    \end{proposition}
    The \textit{identity} matrix $I$ is defined by $ I_{ij}=\delta_{ij} $ and we have $ IM=MI=M $.
    \subsection{Helpful points of view}
    \begin{enumerate}[(i)]
        \item Regarding vectors $ \mathbf{x}\in \mathbb{R}^{n}$ as column vectors, or $n\times 1$ matrices, definitions of matrix-vector and matrix-matrix multiplication agree.
        \item For product $ AB $, where $ A $ is $m\times n$ and $B$ is $ n \times p $, with columns of $ B $ be $ \mathbf{C}_r(B)\in \mathbb{R}^{n} $, then the columns of $ AB $ are $ \mathbf{C}_{r}(AB)\in \mathbb{R}^{m} $, where $r=1,\dots p$. Furthermore, $ \mathbf{C}_r(B) $ and $ \mathbf{C}_r(AB) $ are related by 
        \[
            \mathbf{C}_r(AB)=A \mathbf{C}_r(B)
        .\]
        \item In terms of rows and columns, 
        \[
            AB=\begin{pmatrix}
                &\vdots&\\
                \leftarrow&\mathbf{R}_a(A)&\rightarrow\\
                &\vdots&\\
            \end{pmatrix}\begin{pmatrix}
                &\uparrow&\\
                \cdots&\mathbf{C}_r(B)&\cdots\\
                &\downarrow&\\
            \end{pmatrix}
        .\]
        And 
        \[
            \begin{aligned}
                (AB)_{ar}&=\mathbf{R}_{a}(A)_i \mathbf{C}_r(B)_i\\
                &= \mathbf{R}_a(A) \cdot \mathbf{C}_r(B) \text{ dot product in } \mathbb{R}^{n}.
            \end{aligned}
        \]
    \end{enumerate}

    \subsection{Matrix Inverses}

    \begin{definition}
        If $ A\in \mathcal{M}_{m\times n} $ then $ B\in \mathcal{M}_{n\times m} $ is a \textit{left inverse} of $A$ if $ BA=I\in \mathcal{M}_{n \times n} $. $ C\in \mathcal{M}_{n \times m} $ is a \textit{right inverse} of $A$ if $ AC=I\in \mathcal{M}_{m \times m} $. If $m=n$ then these implies $ B=C=A^{-1} $, \textit{the }inverse of $A$. We have 
        \[
            AA^{-1}=A^{-1}A=I
        .\]
        Not every matrix has an inverse. If it does, $A$ is called \textit{invertible} or \textit{non-singular}.
    \end{definition}
    Consider $ x,x'\in \mathbb{R}^{n} $ or $\mathbb{C}^{n}$ and $ M\in \mathcal{M}_{n \times n} $. If $ M^{-1} $ exists, we can solve 
    \[
        x'=Mx
    \]
    for $x$, since $ M^{-1}x'=(M^{-1}M)x=x $.
    \begin{example}
        For $n=2$, consider a matrix
        \[
            M=\begin{pmatrix}
                M_{11}&M_{12}\\
                M_{21}&M_{22}
            \end{pmatrix}
        .\]
        We try to solve the system of linear equations of $x_1$ and $x_2$:
        \[
            \left\{\begin{aligned}
                 x_{1}'&=M_{11}x_1+M_{12}x_2\\
                 x_2'&=M_{21}x_1+M_{22}x_2.
            \end{aligned}\right.
        \]
        Doing some algebras gives 
        \[
            \left\{\begin{aligned}
                M_{22}x_1'-M_{12}x_2'&=(\det M)x_1\\
                -M_{21}x_1'+M_{11}x_2'&=(\det M)x_2,
           \end{aligned}\right.
        \]
        where $ \det M=M_{11}M_{22}-M_{12}M_{21} $ is the \textit{determinant} of $M$ and for $ \det M\neq 0 $ we have 
        \[
            M^{-1}=\frac{1}{\det M}\begin{pmatrix}
                M_{22}&-M_{12}\\
                -M_{21}&M_{11}
            \end{pmatrix}
        \]
        exists.
    \end{example}
    \begin{remark}
        Relation between $\det$ and the \textit{alternative} scalar product in $ \mathbb{R}^{2} $\marginnote{Recall the alternative scalar product of $ \mathbf{a},\mathbf{b}\in \mathbb{R}^{2} $ is defined as $ [\mathbf{a},\mathbf{b}]=\epsilon_{ij}a_ib_j=a1b_2-a_2b_1 $, applying the summation convention.}: Consider 
        \[
            \mathbf{C}_1=M \mathbf{e}_1=\begin{pmatrix}
                M_{11}\\M_{21}
            \end{pmatrix}, \quad \mathbf{C}_2=M \mathbf{e}_2=\begin{pmatrix}
                M_{12}\\M_{22}
            \end{pmatrix}
        .\] 
        So $ \det M =[\mathbf{C}_1,\mathbf{C}_2]= [M \mathbf{e}_1,M \mathbf{e}_2] $ in $ \mathbb{R}^{2} $. This gives the factor(with sign) by which the areas are scaled under action of $M$. Therefore
        \[
            \begin{aligned}
                 \det M\neq 0 & \Longleftrightarrow M \mathbf{e}_1, M \mathbf{e}_2 \text{ are linearly independent}\\
                 &\Longleftrightarrow \dim \im M=2.
            \end{aligned}
        \]
    \end{remark}
    \begin{example}
        Consider shears in $ \mathbb{R}^{2} $ given by 
        \[
            S(\lambda)=\begin{pmatrix}
                1&\lambda\\
                0&1
            \end{pmatrix}
        .\]
        Then $ \det S(\lambda)=1 $, so the area is preserved and 
        \[
            S^{-1}(\lambda)=\begin{pmatrix}
                1&-\lambda\\
                0&1
            \end{pmatrix}=S(-\lambda)
        .\]
    \end{example}
    \begin{example}
        Recall the rotation of angle $ \theta $ about axis $ \mathbf{n} $ in $ \mathbb{R}^{3} $:
        \[
            R_{ij}(\theta)=\cos \theta \delta_{ij}+(1-\cos \theta)n_in_j-\sin \theta \epsilon_{ijk} n_k
        .\]
        Obviously the inverse of $R(\theta)$ should be $ R(-\theta) $:
        \begin{IEEEeqnarray*}{rClCl}
            &&R_{ij}(\theta)R_{jk}(-\theta)&=&(\cos \theta \delta_{ij}+(1-\cos \theta)n_in_j-\sin \theta \epsilon_{ijp} n_p)\\ 
            &&&\times & (\cos \theta \delta_{jk}+(1-\cos \theta)n_jn_k+\sin \theta \epsilon_{jkq} n_q)\\
            &=&\delta_{ik} \cos^2 \theta &+& 2 \cos \theta(1-\cos \theta)n_in_k\\
            &&&+&(1-\cos \theta)^2n_in_j^2n_k\\
            &&&-&\epsilon_{ijp}\epsilon_{jkq}n_pn_q \sin^2 \theta\marginnote{Do check that other terms cancel.}\\
            &=&\delta_{ik} \cos^2 \theta &+& 2 \cos \theta(1-\cos \theta)n_in_k+(1-\cos \theta)^2n_in_k-\epsilon_{ijp}\epsilon_{jkq}n_pn_q \sin^2 \theta\\
            &=&\delta_{ik} \cos^2 \theta &+& (1-\cos^2 \theta)n_in_k-\epsilon_{ijp}\epsilon_{jkq}n_pn_q \sin^2 \theta\\
            &=&\delta_{ik} \cos^2 \theta &+& (1-\cos^2 \theta)n_in_k+\delta_{ik}\sin^2 \theta-\sin^2 \theta n_in_k\marginnote{By $ \epsilon $-$ \epsilon $ identity and that $n_p^2=1$.}\\
            &=&\boxed{\delta_{ik}}.&&
        \end{IEEEeqnarray*}
        Hence $ R(\theta)R(-\theta)=I $ or $ R(-\theta)=R^{-1}(\theta) $.
    \end{example}
    \subsection{Transpose and Hermitian conjugate}\marginnote{Lecture 12}
    \begin{definition}
        If $M$ is an $ m\times n $ (real or complex) matrix, the transpose $ M^T $ is an $ n\times m $ matrix 
    \[
        (M^T)_{ia}=M_{ai}\quad \text{exchange rows and columns}
    ,\]
    where $ a=1,\dots,m,i=1,\dots,n $.
    \end{definition}

    \begin{proposition}[Properties]
        Let $ A\in \mathcal{M}_{m\times n}, B\in \mathcal{M}_{n\times p} $,
        \begin{enumerate}
            \item $ (\alpha A+\beta B)^T=\alpha A^T+\beta B^T $.
            \item $ (AB)^T=B^TA^T $.
        \end{enumerate}
    \end{proposition}
    \begin{remark}
        Write 
        \[
            \mathbf{x}=\begin{pmatrix}
                x_1\\\vdots\\x_n
            \end{pmatrix}
        ,\]
        then $ \mathbf{x}^T=(x_1,\dots,x_n) $. Inner products in $ \mathbb{R}^{n} $ is 
        \[
            \mathbf{x}\cdot \mathbf{y}=\mathbf{x}^T \cdot \mathbf{y}\quad 1 \times 1 \text{ matrix}
        .\]
        Note that $ \mathbf{y}\mathbf{x}^T $ is an $ n \times n $ matrix with $ M_{ij}=x_iy_j $
    \end{remark}
    \begin{definition}
        If $ M\in \mathcal{M}_{n\times n} $ then $M$ is:
        \begin{enumerate}
            \item \textit{symmetric} if $ M^T=M $ or $ M_{ij}=M_{ji} $.
            \item \textit{anti-symmetric} if $ M^T=-M $ or $ M_{ij}=-M_{ji} $.
        \end{enumerate}
    \end{definition}
    \begin{proposition}
        Any $ M $ can be written as $ M=S+A $, where $S$ is symmetric and $A$ is anti-symmetric.
    \end{proposition}
    \begin{proof}
        Take $ S=\frac{1}{2}(M+M^T) $ and $ \frac{1}{2}(M-M^T) $.
    \end{proof}
    \begin{remark}
        If $ A\in \mathcal{M}_{3\times 3} $ is anti-symmetric, we can write $ A_{ij}=\epsilon_{ijk}a_k $, or 
        \[
            A=\begin{pmatrix}
                0&a_3&-a_2\\
                -a_3&0&a_1\\
                a_2&-a_1&0
            \end{pmatrix}
        .\]
        Then $ (A \mathbf{x})_{i}=\epsilon_{ijk}a_kx_j=(x \times a)_{i} $.
    \end{remark}
    \begin{definition}
        If $ M\in \mathcal{M}_{m\times n} $, the \textit{Hermitian conjugate} $ M^\dagger  $ is an $n\times n$ matrix defined by 
        \[
            (M^\dagger )_{ia}=\overline{M}_{ai}, \text{ or } M^\dagger =\overline{M}^T=\overline{M^T}
        .\]

        If $M$ is square, then $M$ is \textit{hermitian} if $ M^\dagger =M $ or $ M_{ij}=\overline{M}_{ji} $, and is \textit{anti-hermitian} if $ M_{ij}=-\overline{M}_{ji} $.
    \end{definition}
    Similarly above, if 
    \[
        \mathbf{z}=\begin{pmatrix}
            z_1\\\vdots\\z_n
        \end{pmatrix}\in \mathbb{C}^{n},
    \]
    then $ \mathbf{z}^\dagger = (\bar{z}_1,\dots, \bar{z}_n)\in \mathbb{C}^{n} $. Also the complex inner product is
    \[
        (\mathbf{z},\mathbf{w})=\bar{z}_i w_i=\mathbf{z}^\dagger \mathbf{w}
    .\]
    \subsection{Trace}
    \begin{definition}
        For a complex $ n \times n $ matrix $M$, the \textit{trace} is defined by 
        \[
            \trace(M)=M_{ii}
        .\]
    \end{definition}
    Properties:
    \begin{enumerate}
        \item $ \tr(\alpha M+ \beta N)=\alpha \tr(M)+\beta \tr(N) $.
        \item $ \tr(MN)=\tr(NM) $ for $ M\in \mathcal{M}_{m\times n}, N\in \mathcal{M}_{n\times m} $.
        \item $ \tr(M^T)=\tr(M) $.
        \item $ \tr(I)=\delta_{ii}=n $ if $ I\in \mathcal{M}_{n\times n} $.
    \end{enumerate}
    If $ S\in \mathcal{M}_{n\times n} $ is symmetric, let 
    \[
        T=S-\frac{1}{n}\tr(S)I \Longleftrightarrow T_{ij}=S_{ij}-\frac{1}{n}\tr(S)\delta_{ij}
    .\]
    Then
    \[
        \tr(T)=S_{ii}-\frac{1}{n}\tr(S)\delta_{ii}=0
    .\]
    Then 
    \[
        S=T+\frac{1}{n}\tr(S)I
    .\]
    $T$ is often referred to as \textit{traceless} and $ \frac{1}{n}\tr(S)I $ is referred to as \textit{pure trace}.
    \begin{remark}
        If $A$ is $ n \times n $ anti-symmetric, then $ \tr(A)=A_{ii}=0 $.
    \end{remark}
    \section{Orthogonal and unitary matrices}
    \begin{definition}
        A real $ n\times n $ matrix $U$ is \textit{orthogonal} if and only if $ U^T=U^{-1} $, which can be written as 
        \[
            U_{ki}U_{kj}=U_{ik}U_{jk}=\delta_{ij}
        .\]
        This means that columns of $U$ are orthonormal and rows of $U$ are orthonormal. In matrix,
        \[
            \begin{pmatrix}
                &\vdots&\\
                \leftarrow & \mathbf{C}_i& \rightarrow \\
                &\vdots&
            \end{pmatrix}
            \begin{pmatrix}
                &\uparrow&\\
                \cdots&\mathbf{C}_j&\cdots\\
                &\downarrow &
            \end{pmatrix}
            =
            \begin{pmatrix}
                &\vdots&\\
                \leftarrow & \mathbf{R}_i& \rightarrow \\
                &\vdots&
            \end{pmatrix}
            \begin{pmatrix}
                &\uparrow&\\
                \cdots&\mathbf{R}_j&\cdots\\
                &\downarrow &
            \end{pmatrix}
            =
            \begin{pmatrix}
                1&\cdots&0\\
                \vdots&\ddots &\vdots \\
                0&\cdots &1
            \end{pmatrix}
        .\]
    \end{definition}
    For example, if $U=R(\theta)$ is the rotation through $ \theta $ about axis $\mathbf{n}$, then $ U^T=R(\theta)^T=R(-\theta)=R^{-1}(\theta)=U^{-1} $, so the rotation matrix is orthogonal.

    \begin{definition}[Equivalent definition]
        $U$ is orthogonal if and only if it preserves inner product on $ \mathbb{R}^{n} $.
        \[
            (U \mathbf{x})\cdot (U \mathbf{y})=\mathbf{x}\cdot \mathbf{y}, \quad \forall \bfx,\bfy\in \mathbb{R}^{n}
        .\]
    \end{definition}
    To check equivalence, note that 
    \[
        (U \mathbf{x})^T(U \mathbf{y})=(\mathbf{x}^T U^T)(U \mathbf{y})=\mathbf{x}^T(U^TU)\mathbf{y}=\mathbf{x}^T \mathbf{y}
    .\]
    The last equality holds if and only if $ U^TU=I $, so they are indeed equivalent.

    Moreover, in $ \mathbb{R}^{n} $, the columns of $U$ are $ U \mathbf{e}_i $, so inner product is preserved when $U$ acts on standard basis. That is $ (U \mathbf{e}_i)\cdot(U\mathbf{e}_j) =\delta_{ij}$, so the columns of $U$ are orthonormal.
    \begin{example}
        To find general $2\times 2$ orthogonal matrices, note that 
        \[
            U\begin{pmatrix}
                1\\0
            \end{pmatrix}=\begin{pmatrix}
                \cos \theta\\ \sin \theta
            \end{pmatrix},
            U\begin{pmatrix}
                0\\1
            \end{pmatrix}=\pm \begin{pmatrix}
                -\sin \theta\\ \cos \theta
            \end{pmatrix}
        \]
        by the fact that $ |U \mathbf{x}|^2=|\bfx|^2 \Leftrightarrow |U \mathbf{x}|=| \mathbf{x}| $. Therefore, two cases of $U$ are 
        \[
            U=R=\begin{pmatrix}
                \cos \theta & \sin \theta\\
                \sin \theta & -\cos \theta
            \end{pmatrix} \text{ or } U=H=\begin{pmatrix}
                \cos \theta & -\sin \theta\\
                \sin \theta & \cos \theta
            \end{pmatrix}
        .\]
        Obviously the first one is rotaton. To show the second is reflection, let $ \mathbf{n}=(-\sin \theta/2, \cos \theta/2) $ Then by the example of reflection in section 19.2, we have 
        \[
            H_{ij}=\delta_{ij}-2n_in_j
        .\]
        Hence 
        \[
            H=\begin{pmatrix}
                1-2 \sin ^{2} \theta / 2 & 2 \sin \theta / 2 \cos \theta / 2 \\
                2 \sin \theta / 2 \cos \theta / 2 & 1-2 \cos ^{2} \theta / 2
            \end{pmatrix}=\begin{pmatrix}
                \cos \theta & -\sin \theta\\
                \sin \theta & \cos \theta
            \end{pmatrix}
        .\]
    \end{example}
    \begin{remark}
        Cases $ U=R, U=H $ are distinguished by 
        \[
            \det R=1, \det H=-1
        .\]
    \end{remark}
    \begin{definition}
        A complex $n\times n$ matrix $U$ is called \textit{unitary} if and only if 
        \[
            U^\dagger =U^{-1}
        .\]
        Equivalently, $U$ is unitary if and only if it preserves inner product on $ \mathbb{C}^{n} $:
        \[
            (U \mathbf{z}, U \mathbf{w})=(\mathbf{z},\mathbf{w}), \forall \mathbf{z},\mathbf{w}\in \mathbb{C}^{n}
        .\]
    \end{definition}
    To check equivalence, 
    \[
            (U \mathbf{z}, U \mathbf{w})=(U \mathbf{z})^\dagger (U \mathbf{w})=(\bfz^\dagger U^\dagger )(U \bfw )=\bfz^\dagger (U^\dagger U)\bfw=\bfz^\dagger \bfw
    \]
    iff $ U^\dagger U=I $.
    \section{Determinants and Inverses}
    \subsection{Introduction}
    Consider a linear map $ T: \mathbb{R}^{n}\to \mathbb{R}^{n} $. If $T$ is invertible, then $ \ker T=\left\{ 0\right\} $, since $T$ is 1-1. Also $ \im T=\mathbb{R}^{n} $ since $T$ is onto. But these conditions are actually equivalent by rank-nullity theorem.

    Conversely if the conditions hold, then
    \[
        T(\mathbf{e}_1), T(\mathbf{e}_2),\dots,T(\bfe_n)
    \]
    is a basis, so we can define $ T^{-1} $ as a linear map by 
    \[
        T^{-1}(T(\bfe_i))=\bfe_i
    .\]

    To test if the conditions hold for a matrix $M$ representing $T$ and find its inverse $M^{-1}$ explicitly, consider a related matrix $ \tilde{M} $ and a scalar $ \det M $, called the \textit{determinant}, such that 
    \begin{equation}\label{eq:inverse1}
        \tilde{M}M=(\det M)I
    \end{equation}
    Then if $ \det M\neq 0 $, $M$ is invertible with
    \[
        M^{-1}=\frac{1}{\det M}\tilde{M}
    .\]
    For $n=2$, recall that \ref{eq:inverse1} holds with 
    \[
        M=\begin{pmatrix}
            M_{11}&M_{12}\\
            M_{21}&M_{22}
        \end{pmatrix},\quad \tilde{M}=\begin{pmatrix}
            M_{22}&-M_{21}\\
            -M_{12}&M_{11}
        \end{pmatrix},
    \]
    and 
    \[
        \det M=[M \bfe_1, M\bfe_2]=\epsilon_{ij}M_{i1}M_{j2}
    .\]
    This is the factor by which areas are scaled under $M$. Hence 
    \[
        \det M \neq 0 \Longleftrightarrow \left\{ M\bfe_1, M\bfe_2\right\}\text{ is linearly independent}
    .\]

    For $n=3$, consider similarly 
    \[
        \begin{aligned}
            [M\bfe_1, M\bfe_2, M\bfe_3]&= \epsilon_{ijk}M_{i1}M_{j2}M_{k3}\\
            &= \det M,
        \end{aligned}
    \]
    defined for $ \det M $ in $ 3 \times 3 $case. This is the factor by which volumns are scaled under $M$, and 
    \[
        \begin{aligned}
            \det M\neq 0 &\Longleftrightarrow \left\{ M \mathbf{e}_1, M\bfe_2,M\bfe_3\right\} \text{ are linearly independent}\\
            & \Longleftrightarrow \im M=\mathbb{R}^{3}.
        \end{aligned}
    \]
    Now define $ \tilde{M} $ from $M$ using row/column notation:
    \[
        \begin{aligned}
            &\mathbf{R}_1(\tilde{M})=\bfC_2(M)\times \bfC_3(M),
            \\
            &\mathbf{R}_2(\tilde{M})=\bfC_3(M)\times \bfC_1(M),
            \\
            &\mathbf{R}_3(\tilde{M})=\bfC_1(M)\times \bfC_2(M).
        \end{aligned}
    \]
    Claim that this is indeed $ \tilde{M} $. Note that 
    \[
        \begin{aligned}
            (\tilde{M}M)_{ij}&=\mathbf{R}_i(\tilde{M}) \cdot \mathbf{C}_j(M)\\
            &=[\bfC_1(M),\bfC_2(M),\bfC_3(M)]\delta_{ij}\\
            &=\det M \delta_{ij}.
        \end{aligned}
    \]
    \begin{example}
        Take 
        \[
            M=\begin{pmatrix}
                1&3&0\\
                0&-1&-2\\
                4&1&-1
            \end{pmatrix},
            \begin{aligned}
                &\mathbf{C}_{2} \times \mathbf{C}_{3}=\begin{pmatrix}
                3 \\
                -1 \\
                1
                \end{pmatrix} \times\begin{pmatrix}
                0 \\
                2 \\
                -1
                \end{pmatrix}=\begin{pmatrix}
                -1 \\
                3 \\
                6
                \end{pmatrix}\\
                &\mathbf{C}_{3} \times \mathbf{C}_{1}=\begin{pmatrix}
                0 \\
                2 \\
                -1
                \end{pmatrix} \times\begin{pmatrix}
                1 \\
                0 \\
                4
                \end{pmatrix}=\begin{pmatrix}
                8 \\
                -1 \\
                -2
                \end{pmatrix}\\
                &\bfC_{1} \times \mathbf{C}_{2}=\begin{pmatrix}
                1 \\
                0 \\
                4
                \end{pmatrix} \times\begin{pmatrix}
                3 \\
                -1 \\
                1
                \end{pmatrix}=\begin{pmatrix}
                4 \\
                11\\
                -1
                \end{pmatrix}
                \end{aligned}
        \]
        Thus we can take 
        \[
            \tilde{M}=\begin{pmatrix}
                -1&3&6\\
                8&-1&-2\\
                4&11&-1
            \end{pmatrix}, \det M = \bfC_1 \cdot (\bfC_2 \times \bfC_3 )=23
        .\]
        Can check that $ \tilde{M}M=23I $.
    \end{example}
    \subsection{Epsilon and alternating forms}
    Recall that a permutation $ \sigma $ on the set $\{1,2,\dots,n\}$ is a bijection to itself, specified by an ordered list $ (\sigma(1),\dots,\sigma(n)) $. Permutations form a group $ S_n $ which has order $ n! $. A transposition $ \tau=(p,q) $ is a permutation exchanging $p$ and $q$.

    Any permutation is a product of $k$ transpositions say 
    \[
        \sigma=\tau_k \cdots \tau_2 \tau_1
    ,\]
    with $k$ being always even or odd. Denote $ \epsilon(\sigma)=(-1)^k $, which is well defined, as the sign or signature of $ \sigma $, and say $ \sigma $ is even or odd concerning the sign. 

    The \textit{alternating} or $ \epsilon $ symbol in $ \mathbb{R}^{n} $ or $ \mathbb{C}^{n} $ is an $n$-index object(tensor) defined by 
    \[
        \epsilon_{\underbrace{ij\cdots l}_{n\text{ indices}}}=\begin{cases}
        +1 &\text{if $ (i,j,\dots,l) $ is an even permutation of $ (1,2,\dots,n) $}\\
        -1 &\text{if $ (i,j,\dots,l) $ is an odd permutation of $ (1,2,\dots,n) $}\\
        0 &\text{else, i.e. any indices take same values}
        \end{cases} 
    \]
    Then if $ \sigma $ is any permutation, then
    \[
        \epsilon_{\sigma(1)\cdots \sigma(n)}=\epsilon(\sigma)
    .\]
    $ \epsilon_{ij\cdots l} $ is \textit{totally antisymmetric} and changes sign when a pair of indices are exchanged.

    \begin{definition}
        Given vectors $ \bfv_1,\dots,\bfv_n \in \mathbb{R}^{n}\lor \mathbb{C}^{n} $, the \textit{alternating form} combines them to give the scalar 
        \[
            \begin{aligned}
                [\bfv_1,\bfv_2,\dots,\bfv_n]&= \epsilon_{ij\cdots l}(\bfv_1)_{i}(\bfv_2)_j\cdots (\bfv_n)_l\\
                &= \sum_{\sigma\in S_n}\epsilon(\sigma)(\bfv_1)_{\sigma(1)}(\bfv_2)_{\sigma(2)}\cdots (\bfv_n)_{\sigma(n)}.
            \end{aligned}
        \]
    \end{definition}
    Properties:
    \begin{enumerate}
        \item \textit{Multilinear}. We have 
        \[
            \begin{aligned}
                [\bfv_1,\dots,\bfv_{p-1},\alpha\bfu+\beta\bfw,\bfv_{p+1},\dots,\bfv_n]=&\alpha[\bfv_1,\dots,\bfv_{p-1},\bfu,\bfv_{p+1},\dots,\bfv_n]\\
                &+\beta[\bfv_1,\dots,\bfv_{p-1},\bfw,\bfv_{p+1},\dots,\bfv_n].
            \end{aligned}
        \]
        \item Totally antisymmetric.
        \[
            [\bfv_{\sigma(1)},\bfv_{\sigma(2)},\dots,\bfv_{\sigma(n)}]=\epsilon(\sigma)[\bfv_1,\bfv_2,\dots,\bfv_n]
        .\]
        \item $ [\bfe_1,\dots,\bfe_n]=1 $.
        \item If $ \mathbf{v}_p=\mathbf{v}_q (p\neq q) $ then 
        \[
            [\bfv_1,\dots,\bfv_p,\dots,\bfv_q,\dots,\bfv_n]=0
        .\]
        \item If $ \bfv_p $ is a linear combination of the rest vectors, then 
        \[
            [\bfv_1,\dots,\bfv_p,\dots,\bfv_n]=0
        .\]
    \end{enumerate}
    The first three properites fix the alternating form completely, and they imply the rest properties.

    To justify 2., check for a transposition $ \tau=(p,q), p<q$.
    \[
        \begin{aligned}
            [\bfv_1,\dots,\bfv_q,\dots,\bfv_p,\bfv_{n}]&=\sum_{\sigma}\epsilon(\sigma)(\bfv_1)_{\sigma(1)}\cdots (\bfv_q)_{\sigma(p)}\cdots(\bfv_p)_{\sigma(q)}\cdots (\bfv_n)_{\sigma(n)}\\
            &=\sum_{\sigma}\epsilon(\sigma)(\bfv_1)_{\sigma'(1)}\cdots (\bfv_q)_{\sigma'(q)}\cdots(\bfv_p)_{\sigma'(p)}\cdots (\bfv_n)_{\sigma'(n)},
        \end{aligned}
    \]
    where $ \sigma'=\sigma \tau $. Therefore, 
    \[
        [\bfv_1,\dots,\bfv_q,\dots,\bfv_p,\bfv_{n}] = -[\bfv_1,\dots,\bfv_p,\dots,\bfv_q,\bfv_{n}]
    ,\]
    since $ \epsilon(\sigma')=-\epsilon(\sigma) $ and $ \sum_{\sigma}=\sum_{\sigma'} $.
    \begin{proposition}
        $ [\bfv_1,\bfv_2,\dots,\bfv_n]\neq 0 \Leftrightarrow \bfv_1,\bfv_2,\dots,\bfv_n $ are linearly independent.
    \end{proposition}
    \begin{proof}
        If $ \bfv_1,\bfv_2,\dots,\bfv_n $ are linearly dependent, then by 5. $ [\bfv_1,\bfv_2,\dots,\bfv_n]=0 $, \#. Conversely, linear independence of $ \bfv_1,\bfv_2,\dots,\bfv_n $ implies that they span $ \mathbb{R}^{n} $ or $ \mathbb{C}^{n} $. Hence $ \exists u_{ai}\in \mathbb{R} \text{ or }\mathbb{C}  $ such that
        \[
            \mathbf{e}_i = u_{ai}\bfv_a,\text{ by summation convention}
        .\]
        Now 
        \[
            \begin{aligned}
                [\bfe_1,\dots,\bfe_n]&=[u_{a1}\bfv_a,u_{b2}\bfv_a,\dots, u_{cn}\bfv_a]\\
                &= u_{a1}u_{b2}\cdots u_{cn}[\bfv_a,\bfv_b,\dots,\bfv_c]\\
                &=u_{a1}u_{b2}\cdots u_{cn} \epsilon_{ab\dots c} [\bfv_1,\bfv_2,\dots,\bfv_n]\text{ by 2.}
            \end{aligned}
        \]
        But LHS is 1, so $ [\bfv_1,\bfv_2,\dots,\bfv_n]\neq 0 $.
    \end{proof}
    \begin{example}
        Cosider 
        \[
            \bfv_1=\begin{pmatrix}
                i\\0\\0\\2
            \end{pmatrix},
            \bfv_2=\begin{pmatrix}
                0\\0\\5i\\0
            \end{pmatrix},
            \bfv_3=\begin{pmatrix}
                3\\2i\\0\\0
            \end{pmatrix},
            \bfv_4=\begin{pmatrix}
                0\\0\\-i\\1
            \end{pmatrix}\in \mathbb{C}^{4}
        .\]
        We have 
        \[
            \begin{aligned}
                 [\bfv_1,\bfv_2,\bfv_3,\bfv_4]&=5i[\bfv_1,\bfe_3,\bfv_3,\bfv_4]\\
                 &= 5i[i\bfe_1+2\bfe_4, \bfe_3,3\bfe_1+2i\bfe_2,-i\bfe_3+\bfe_4]\\
                 &= 5i[i\bfe_1,\bfe_3,2i\bfe_2,\bfe_4]\\
                 &= 5i(i)(2i)(-1)=10i.
            \end{aligned}
        \]
        Hence they are linearly independent.
    \end{example}
    \subsection{Determinants in $ \mathbb{R}^{n} $ and $ \mathbb{C}^{n} $}
    \subsubsection{Definitions}
    \begin{definition}
        For $ M\in \mathcal{M}_{n\times n} $ with columns $ \bfC_a=M\bfe_a $, the \textit{determinant} $ \det M $ or $ |M|\in \mathbb{R} \text{ or }\mathbb{C}  $ is defined by 
    \[
        \begin{aligned}
            \det M&= [\bfC_1,\dots,\bfC_n]\\
             &=[M\bfe_1,\dots,M\bfe_n]\\
             &= \epsilon_{ij\dots,l}M_{i1}M_{j2}\cdots M_{ln}\\
             &= \sum_{\sigma}\epsilon(\sigma)M_{\sigma(1)1}\cdots M_{\sigma(n)n}.
        \end{aligned}
    \]
    Each of the expressions can be taken as the defintion.
    \end{definition}
    \begin{example}
        \begin{enumerate}
            \item $n=2$, $ \det M= \sum_{\sigma}\epsilon(\sigma)M_{\sigma(1)1}M_{\sigma(2)2}=M_{11}M_{22}-M_{12}M_{21} $.
            \item Consider any diagonal matrix $M$ such that $ M_{ij}=0 $ whenever $ i\neq j $. We have 
            \[
                \det M = M_{11}M_{22}\cdots M_{nn}
            .\]
            \item Consider $ M\in \mcM_{n\times n} $ defined by 
            \[
                M= \begin{pmatrix}
                    A&0\\
                    0&1
                \end{pmatrix}
            ,\]
            where $A\in \mcM_{(n-1) \times (n-1)}$, so that $ M_{ni}=M_{in}=0 $ if $i\neq n$. Hence we can restrict permutation $ \sigma $ in definition to have $ \sigma(n)=n $. Therefore $ \det M=\det A $.
        \end{enumerate}
    \end{example}
    \begin{proposition}[Alternative Definitions]
        If $ \bfR_a $ are rows of $M$, then 
        \[
            \begin{aligned}
                \det M&=[\bfR_1,\bfR_2,\dots,\bfR_n]\\
                &= \epsilon_{ij\dots l}M_{1i}M_{2j}\cdots M_{nl}\\
                &= \sum_{\sigma}M_{1\sigma(1)}M_{2\sigma(2)}\cdots M_{n\sigma(n)}.
            \end{aligned}
        \]
        i.e., $ \det M = \det M^T $.
    \end{proposition}
    Recall that $ (\bfC_{a})_i=M_{ia}=(\bfR_{i})_a $.
    \begin{proof}
        We show directly the $ \sum_\sigma $ agree by considering 
        \[
            M_{\sigma(1)1}\cdots M_{\sigma(n)n} = M_{1\rho(1)}M_{2\rho(2)}\cdots M_{n\rho(n)},\quad \rho=\sigma^{-1}.
        \]
        But then $ \epsilon(\sigma)=\epsilon(\rho) $ and $ \sum_\sigma=\sum_\rho $, so this completes the proof.
    \end{proof}
    \subsubsection{Evaluating Determinants, Expanding by Rows or Columns}
    \begin{definition}
        For $ M\in \mcM_{n\times n} $ with entries $M_{ia}$, define the \textit{minor} $ M^{ia} $ to be the $ (n-1)\times (n-1) $ determinant of matrix obtained by deleting row $i$ and column $a$ from $M$.
    \end{definition}
    \begin{proposition}
        \[
            \det M = \sum_{i}(-1)^{i+a}M_{ia}M^{ia} = \sum_{a}(-1)^{i+a}M_{ia}M^{ia}
        .\]
    \end{proposition}
    \begin{example}
        Consider 
        \[
            M=\begin{pmatrix}
                i&0&3&0\\
                0&0&2i&0\\
                0&5i&0&-i\\
                2&0&0&1
            \end{pmatrix}
        .\]
        Expand by row 3:
        \[
            \det M=-5i\begin{vmatrix}
                i&3&0\\
                0&2i&0\\
                2&0&1
            \end{vmatrix}+i\begin{vmatrix}
                i&0&3\\
                0&0&2i\\
                2&0&0
            \end{vmatrix}
            = 10i.
        \]
    \end{example}
    \subsection{Simplifying Determinants: Row and Column operation}\marginnote{Lecture 15.}
    Let $ M\in \mcM_{n\times n} $. Summary: $ \det M $ is a function of the rows $ \bfR_i $ or columns $ \bfC_i $ that is multilinear, totally antisymmetric, and $ \det I = 1 $.
    \subsubsection{Row and Column Scalings}
    If $ \bfR_i \mapsto \lambda\bfR_i $ for $i$ fixed, \textit{or} $ \bfC_a\mapsto \lambda\bfC_a $ for $a$ fixed, then 
    \[
        \det M \mapsto \lambda \det M
    .\]
    If we scale all rows or columns of $M$, then $ \det M \mapsto \lambda^n \det M  $.
    \subsubsection{Rows and Column Operations}
    If $ \bfR_i\mapsto\bfR_i+\lambda\bfR_j $ for $j\neq i$, or $ \bfC_a\mapsto\bfC_a+\lambda\bfC_b $ for $ b\neq a, a,b,c,d $ are fixed, then by multilinearity, we have 
    \[
        \det M \mapsto \det M
    .\]
    \subsubsection{Row and Column Exchanges}
    If $ \bfR_i\leftrightarrow \bfR_j $ or $ \bfC_a \leftrightarrow \bfC_b $, then 
    \[
        \det M \mapsto - \det M
    .\]
    \begin{example}
        Consider 
        \[
            A=\begin{pmatrix}
                1 & 1 & a \\
                a & 1 & 1 \\
                1 & a & 1
            \end{pmatrix}, \quad a \in \mathbb{C}
        .\]
        \begin{IEEEeqnarray*}{rCl}
            \bfC_1 \mapsto \bfC_1-\bfC_3 & : & \det A =\begin{vmatrix}
                1-a & 1 & a \\
                a-1 & 1 & 1 \\
                0 & a & 1
            \end{vmatrix}=(1-a)\begin{vmatrix}
                1 & 1 & a \\
                -1 & 1 & 1 \\
                0 & a & 1
            \end{vmatrix}.
        \\
          \bfC_2 \mapsto \bfC_2-\bfC_3  & : & \det A = (1-a)\begin{vmatrix}
            1 & 1-a & a \\
            -1 & 0 & 1 \\
            0 & a-1 & 1
        \end{vmatrix}=(1-a)^2\begin{vmatrix}
            1 & 1 & a \\
            -1 & 0 & 1 \\
            0 & -1 & 1
        \end{vmatrix}.
        \\
           \bfR_1\mapsto \sum \bfR_i & : & \det A = (1-a)^2\begin{vmatrix}
            0 & 0 & a+2 \\
            -1 & 0 & 1 \\
            0 & -1 & 1
        \end{vmatrix} = \underline{(1-a)^2(a+2)}.
        \end{IEEEeqnarray*}
    \end{example}
    \subsection{Multiplicative Property}
    \begin{theorem}\label{thm:multi_prop}
        For $ M,N\in \mcM_{n\times n} $, $ \det MN=\det M \det N $.
    \end{theorem}
    \begin{proof}
        Can prove this using the following elaboration on $ \det M $.
    \begin{lemma}\label{lma:multi_prop}
        $ \epsilon_{i_1i_2\cdots i_n}M_{i_1a_1}M_{i_2a_2}\cdots M_{i_na_n}=\epsilon_{a_1a_2\cdots a_n}\det M $.
    \end{lemma}
    \begin{proof}
        LHS and RHS are each totally antisymmetric in $a_1,\dots,a_n$, so must be related by a constant of proportionality. To fix the constant, simply take $(a_1,\dots,a_n)=(1,\dots,n)$ and indeed it works.
    \end{proof}
    We have 
    \begin{IEEEeqnarray*}{rCl}
        \det MN & = & \epsilon_{i_1\cdots i_n}(MN)_{i_11}\cdots(MN)_{i_nn}
    \\
        & = & \epsilon_{i_1\cdots i_n}M_{i_1 k_1}N_{k_1 1}\cdots M_{i_nk_n}N_{k_nn}
    \\
        & = & (\det M)\epsilon_{k_1\cdots k_n} N_{k_11}\cdots N_{k_nn}
    \\
        & = & \det M \det N.
    \end{IEEEeqnarray*}
    \end{proof}
    Consequences:
    \begin{enumerate}
        \item $ M^{-1}M=I \Rightarrow |M^{-1}||M|=1 $, so $ |M^{-1}|=|M|^{-1} $.
        \item For $R$ real and orthogonal,
        \[
            R^TR=I \Longrightarrow |R|^2=1 \Longrightarrow |R|=\pm 1
        .\]
        \item For $U$ complex and unitary, 
        \[
            U^\dagger U=I \Longrightarrow |U^\dagger ||U|=1 \Longrightarrow |\overline{U}||U|=|U|^2=1 \Longrightarrow |U|=\pm 1
        .\]
        Here we use facts that $ |U^T|=|U| $ and $ |\overline{U}|=\overline{|U|} $. The second one is immediate from definition.
    \end{enumerate}
    \section{Minors, Cofactors, and Inverses}
    \subsection{Cofactors and Determinants}
    Consider a particular column of a matrix $ M\in \mcM_{n\times n} $ and write it 
    \[
        \bfC_a=\sum_{i}M_{ia}\bfe_i
    .\]
    Then 
    \[
        |M|=[\bfC_1,\dots,\bfC_n]=\sum_{i}M_{ia}[\bfC_1,\dots,\bfe_i,\bfC_{a+1},\dots,\bfC_n]:= \sum_i M_{ia}\Delta_{ia}
    .\]
    Hence
    \[
        \Delta_{ia}=\begin{vmatrix}
            A & \begin{matrix}
                0\\ \vdots \\ 0
            \end{matrix} & B\\
            \begin{matrix}
                0 & \cdots & 0
            \end{matrix} & 1 &\begin{matrix}
                0 & \cdots & 0
            \end{matrix}\\
            C & \begin{matrix}
                0\\ \vdots \\ 0
            \end{matrix} & D
        \end{vmatrix}
    ,\]
    where 1 appears at row $i$ column $a$. Swap it to bottom-right:
    \[
        \Delta_{ia}=(-1)^{n-a}(-1)^{n-i}\begin{vmatrix}
            \begin{matrix}
                A&B\\C&D
            \end{matrix}&\begin{matrix}
                0\\ \vdots \\ 0
            \end{matrix}\\
            \begin{matrix}
                0 & \cdots & 0
            \end{matrix}&1
        \end{vmatrix}=(-1)^{i+a}M^{ia}
    ,\]
    where $ M^{ia} $ is the minor of $M$. We deduce that 
    \[
        \det M = \sum_{i} M_{ia}\Delta_{ia}=\sum_{i}(-1)^{i+a}M_{ia}M^{ia}
    .\]
    Here $\Delta_{ia}$ is the \textit{cofactor}.

    Similarly, by considering rows, 
    \[
        \det M = \sum_{a}M_{ia}\Delta_{ia}=\sum_{a}(-1)^{i+a}M_{ia}M^{ia}
    .\]
    \subsection{Adjugates and Inverses}
    Reasoning as above, consider $ \bfC_b=\sum_i M_{ib}\bfe_i $. Then 
    \[
       \begin{aligned}
            &[\bfC_1,\dots,\bfC_{a-1},\bfC_b,\bfC_{a+1},\dots,m\bfC_n]\\
            =&\sum_{i}M_{ib}\Delta_{ia}=\left.\begin{cases}
            \det M &\text{if }a=b,\\
            0 &\text{otherwise.}\\
            \end{cases}\right\rbrace  = (\det M )\delta_{ab}.
       \end{aligned} 
    \]
    Similarly, expanding a row we get 
    \[
        \sum_{a}M_{ja}\Delta_{ia} = (\det M)\delta_{ij}
    .\]
    Let $ \Delta $ be the matrix of cofactors(entries of $\Delta_{ia}$), and define the \textit{adjugate} $ \tilde{M}=\adj(M) = \Delta^T $. Then 
    \[
        \Delta_{ia}M_{ib}=\Delta^T_{ai}M_{ib}=(\Delta^T M)_{ab}=|M| \delta_{ab} \Longrightarrow \tilde{M}M=|M|I
    .\] 
    Similarly, $ M \tilde{M}=|M|I $. Hence if $ \det M\neq 0 $, then 
    \[
        M^{-1}=\frac{1}{\det M}\tilde{M}
    .\]
    \begin{example}
        Return to 
        \[
            A=\begin{pmatrix}
                1&1&a\\
                a&1&1\\
                1&a&1
            \end{pmatrix}
        .\]
        By previous, $ \det A=(1-a)^2(a+2) $. To find the inverse, note that $ \Delta_{12}=1-a $, etc. The matrix of cofactors is 
        \[
            \Delta=\begin{pmatrix}
                1-a&1-a&a^2-1\\
                a^2-1&1-a&1-a\\
                1-a&a^2-1&1-a
            \end{pmatrix}
        .\]
        Hence 
        \[
            A^{-1}=\frac{1}{(1-a)(a+2)}\begin{pmatrix}
                    1&-(1+a) & 1 \\
                    1 & 1&-(1+a) \\
                    -(1+a)& 1 & 1
            \end{pmatrix}
        .\]
    \end{example}
    \section{Systems of Linear Equations}
    \subsection{Introduction \& Nature of Solutions}\marginnote{Lecture 16.}
    Consider a system of $n$ linear equations in $n$ unknowns $x_i$. Write in matrix-vector form
    \[
        A\bfx=\bfb,\quad \bfx,\bfb\in \mathbb{R}^{n}, A\in \mcM_{n\times n}
    .\]
    i.e.,
    \[
        \left\lbrace\begin{aligned}
            A_{11} x_{1}+&\ldots+A_{1 n} x_{n}=b_{1} \\
            &\vdots\\
            A_{n 1} x_{1}+&\ldots+A_{n n} x_{n}=b_{n}.
        \end{aligned}\right.
    \]
    There are three possibilities:
    \begin{enumerate}[(i)]
        \item $ \det A\neq 0 $, then there is a unique solution $ \bfx=A^{-1}\bfb $.
        \item $ \det A=0 $ and $ \bfb\notin \im(A) $. Then there is no solution.
        \item $ \det A=0  $ and $ \bfb\in \im(A) $. Then there are infinitely many solutions. In this case, the general solution is 
        \[
            \bfx=\bfx_0+\bfu,
        ,\]
        where $\bfx_0$ is a particular solution and $\bfu\in \ker A$.
    \end{enumerate}
    Elaboration: A solution exists if and only if $ A\bfx_0=\bfb $ for some $ \bfx_0 \Leftrightarrow \bfb\in \im(A) $. Then $\bfx$ is a solution if and only if $\bfu=\bfx-\bfx_0\in \ker A$. That is 
    \[
        A\bfu=\mathbf{0}
    .\]
    This is called the corresponding \textit{homogeneous} problem.
    
    Now $ \det A\neq 0 \Leftrightarrow \im(A)=\mathbb{R}^{n} \Leftrightarrow \ker A=\{\mathbf{0}\} $. So in case (i) there is always a unique solution for any $\bfb$. But $ \det A=0 \Leftrightarrow \rank(A)<n \Leftrightarrow \nullity A>0 $, and either $\bfb\in \im(A)$ or $\bfb\notin\im(A)$.

    If $ \bfu_1,\dots,\bfu_k $ is a basis of $ \ker A $, then the general solution to the homogeneous problem is 
    \[
        \bfu= \sum_{i=1}^{k}\lambda_i \bfu_i, \quad k=\nullity A
    .\]
    \begin{remark}
        Compare to differential equations on functions $y(x)$. Consider a linear differential operator 
        \[
            \mcL= p_2(x)\frac{\mathrm{d}^2}{\mathrm{d}x^2}+p_1 \frac{\mathrm{d}}{\mathrm{d}x} +p_0(x),\quad \mcL y=f 
        .\]
        It has a particular solution $y_0(x)$ with $ \mcL y_0=f $, and the general solution 
        \[
            y=y_0+(\lambda_1 y_1+\lambda_2 y_2)
        \]
        where $\mcL y_1=\mcL y_2=0$.
    \end{remark}
    \begin{example}
        Consider $A\bfx=\bfb$ where
        \[
            A=\begin{pmatrix}
                1&1&a\\
                a&1&1\\
                1&a&1
            \end{pmatrix}, \bfb=\begin{pmatrix}
                1\\c\\1
            \end{pmatrix},\quad a,c\in \mathbb{R}.
        \]
        Previously found $ \det A=(a-1)^2(a+2) $. We have three cases:
        \begin{itemize}
            \item \underline{$a\neq 1,-2$}. Then $A^{-1}$ exists, as shown previously. Hence the unique solution is 
            \[
                \frac{1}{(1-a)(a+2)}
            \begin{pmatrix}
                    1\\c\\1
                \end{pmatrix}
               \begin{pmatrix}
                    1&-(1+a) & 1 \\
                    1 & 1&-(1+a) \\
                    -(1+a)& 1 & 1
            \end{pmatrix}
            =\begin{pmatrix}
                2-c-ca\\c-a\\c-a
            \end{pmatrix}
            .\]
            Geometrically, solution is a point.
            \item \underline{$a=1$}. Then $A_{ia}\equiv 1$ and 
            \[
                \im A=\left\{ \lambda\begin{pmatrix}
                    1\\1\\1
                \end{pmatrix} : \lambda\in \mathbb{R} \right\},\quad \ker A = \spn\left\{ \begin{pmatrix}
                    -1\\1\\0
                \end{pmatrix},\begin{pmatrix}
                    -1\\0\\1
                \end{pmatrix} \right\}
            .\]
            $ \bfb\in \im A \Leftrightarrow c=1 $, and a particular solution is $ \bfx_0^T=(1,0,0) $. General solution is 
            \[
                \bfx=\begin{pmatrix}
                    1\\0\\0
                \end{pmatrix}+\lambda\begin{pmatrix}
                    -1\\1\\0
                \end{pmatrix}+\mu\begin{pmatrix}
                    -1\\0\\1
                \end{pmatrix}=\begin{pmatrix}
                    1-\lambda-\mu\\
                    \lambda\\
                    \mu
                \end{pmatrix}
            .\]
            Hence for $a=c=1$, we have (iii), which is a plane. If $a=1,c\neq 1$ there is no solution.
            \item \underline{$a=-2$}. In this case 
            \[
                A=\begin{pmatrix}
                    1&1&-2\\
                    -2&1&1\\
                    1&-2&1
                \end{pmatrix} \Rightarrow \im A=\spn\left\{ \begin{pmatrix}
                    1\\-2\\1
                \end{pmatrix},\begin{pmatrix}
                    1\\1\\-2
                \end{pmatrix} \right\}, \ker A = \left\{ \lambda\begin{pmatrix}
                    1\\1\\1
                \end{pmatrix} \right\}
            .\]
            Claim that $ \bfb\in \im A \Leftrightarrow c=-2$, and thus a particular solution is $ \bfx_0^T=(1,0,0) $.

            Indeed, $\bfb\in \im A$ if and only if it is coplanar with the basis. i.e., 
            \[
                \left[ \begin{pmatrix}
                    1\\c\\1
                \end{pmatrix},\begin{pmatrix}
                    1\\-2\\1
                \end{pmatrix},\begin{pmatrix}
                    1\\1\\-2
                \end{pmatrix} \right]=3c+6=0 \Leftrightarrow c=-2
            .\]
            Therefore, general solution is 
            \[
                \bfx=\begin{pmatrix}
                    1+\lambda\\\lambda\\\lambda
                \end{pmatrix}
            .\]
            For $a=c=-2$, we have case (iii) which is a line. If $ a=-2,c\neq -2 $ then there is no solution.
        \end{itemize}
    \end{example}
    \subsection{Geometrical Interpretation in $ \mathbb{R}^{3} $}
    Let $ \bfR_1,\bfR_2,\bfR_3 $ be rows of $A$. Then 
    \[
        A\bfu=\mathbf{0}\Longleftrightarrow \bfR_i\cdot \bfu=0, r=1,2,3
    .\]
    So solution of homogeneous problem is given by intersection of these planes.
    \begin{enumerate}[align=hang]
        \item[\underline{$ \rank A=3 $}.] Normals are independent so the planes intersect only at $O$.
        \item[\underline{$ \rank A=2 $}.] Normals span a plane and planes intersect in a line.
        \item[\underline{$ \rank A=1 $}.] Normals are parallel and planes coincide.
    \end{enumerate}

    Now consider instead $ A\bfx=\mathbf{b}\Leftrightarrow \bfR_i\cdot \bfx=b_i, r=1,2,3 $. Still planes with normals $ \bfR_i $, but not through $O$ unless one of $b_i=0$.
    \begin{enumerate}[align=hang]
        \item[\underline{$ \rank A=3 $}.] $ \det A\neq 0 $ and normals are independent. Planes intersect at a point, which is the unique solution.
        \item[\underline{$ \rank A<3 $}.] $ \det A=0 $. Existence of solutions depends on $\bfb$.
        \item[\underline{$ \rank A=2 $}.] Planes may intersect in a line but they may not. e.g., pairwise intersect in lines, or two planes are parallel and the third intersects them. In latter cases there is no solution.
        \item[\underline{$ \rank A=1 $}.] Planes may coincide but may not. e.g., at least two are parallel but do not coincide.
    \end{enumerate}
\end{document}