\documentclass[10pt]{article}
\pdfoutput=1 
\usepackage{NotesTeX,lipsum}
\usepackage{IEEEtrantools}
\def\d{{\mathrm d}}
\def\e{{\mathrm e}}
\def\g{{\mathrm g}}
\def\h{{\mathrm h}}
\def\f{{\mathrm f}}
\def\p{{\mathrm p}}
\def\s{{\mathrm s}}
\def\t{{\mathrm t}}
\def\i{{\mathrm i}}

\def\A{{\mathrm A}}
\def\B{{\mathrm B}}
\def\E{{\mathrm E}}
\def\F{{\mathrm F}}
\def\G{{\mathrm G}}
\def\H{{\mathrm H}}
\def\P{{\mathrm P}}


\def\bb{\mathbf b}
\def \bc{\mathbf c}
\def\bx {\mathbf x}
\def\bn {\mathbf n}
\def\le{\leqslant}
\def\ge{\geqslant}
\def\arcosh{{\rm arcosh}\,}

\newcommand{\bluecomment}[1]{{\color{blue}#1}}
%\renewcommand{\comment}[1]{}
\newcommand{\redcomment}[1]{{\color{red}#1}}
\newcommand{\tm}{\times}
%\usepackage{showframe}

\title{\begin{center}{\Huge \textit{Vectors and Matrices}}\\{{\itshape Based on Lectures and "Intro to Linear Algebra"}}\end{center}}
\author{$\theta\omega\theta$}
\affiliation{
Not in University of Cambridge\\
skipped some talks irrelevant to contents\\
}
\DeclareMathOperator{\spn}{span}

\emailAdd{not telling you}

\begin{document}
	\maketitle
	\flushbottom
	\newpage
	\pagestyle{fancynotes}
	\part{Complex Numbers}
    \section{Definition}
    \begin{definition}
        Construct $\mathbb{C} $ from $ \mathbb{R}  $ by adding $i$ that $i^2 = -1$. Any $z\in \mathbb{C}$ is in the form
        \[
            z = x+iy, x = \Re z, y= \Im z, x,y\in \mathbb{R} . 
        \] 
        Addition and multiplication are defined by
        \[
            z_1+z_2 = (x_1+x_2) + i(y_1+y_2), z_1z_2 = (x_1+iy_1)(x_2+iy_2)
        .\]
        The \textit{conjugate} is defined by
        \[
            \bar{z} = z* = x-iy
        .\]
        The \textit{modulus} is defined by
        \[
            r = |z|, r\ge 0, r^2=|z|^2=z\bar{z}=x^2+y^2
        .\]
        The \textit{argument} is defined by
        \[
            z\neq 0: \theta=\arg(z)\in \mathbb{R}, z=r(\cos \theta+i\sin \theta)
        .\]
        The values of $ \theta $ in $ (-\pi, \pi] $ are called the \textit{principal values}.

        Complex numbers can be plotted on an \textit{Argand diagram}.
    \end{definition}
    \section{Basic Properties \& Consequences}
    \begin{enumerate}[(1)]
        \item $ +, \times $ are commutative and associative,
        
            $ \mathbb{C} $ under $+$ is an abelian group,
            
            $ \mathbb{C} $ under $\times$ is an abelian group,

            $ \mathbb{C} $ is a field.
        \item \textbf{Fundamental Theorem of Algebra}: A polynomial with deg $n$ with coefficients in $ \mathbb{C}  $ can be written as a product of $n$ linear factors, has at least one solution in $ \mathbb{C}  $ and has n solutions connected with multiplicity.
        \item Parallelogram constructions.
        \item \[
            \left| z_1 \right| \left| z_2 \right| = \left| z_1z_2 \right|, \left| z_1+z_2 \right| \le \left| z_1 \right| +\left| z_2 \right|
        .\]

        Alternative forms:
        \[
            \left| z_2-z_1 \right| \ge \left| z_2 \right| -\left| z_1 \right|, \left| z_2-z_1 \right| \ge \left| |z_2|-|z_1| \right| 
        .\]
        \item \textbf{De Moivre's Theorem}: $ z^n = r^n(\cos n\theta+i\sin n\theta) $.
    \end{enumerate}
    \section{Exponential and Trigs in $\mathbb{C}$}
    \begin{definition}
        Define $ \exp, \cos, \sin $ on $ \mathbb{C} $ by
        \[
            \begin{aligned}
                 \exp(z)&= e^z = \sum_{n=0}^{\infty} \frac{1}{n!}z^n,\\
                 \cos(z)&= \frac{1}{2}(e^{iz}+e^{-iz}) = 1-\frac{1}{2!}z^2+\frac{1}{4!}z^4+\cdots,\\
                 \sin(z)&= \frac{1}{2i}(e^{iz}-e^{-iz}) = z-\frac{1}{3!}z^3+\frac{1}{5!}z^5+\cdots.\\
            \end{aligned}
        \]
        These series converge for all $ z\in \mathbb{C} $. Can be multiplied, rearranged, etc. Definitions reduce to familiar ones in the reals.
    \end{definition}
    \begin{proposition}\label{prop:exp multi}
        $ \forall z,w\in \mathbb{C}, e^ze^w=e^{z+w}; e^ze^{-z}=1, (e^z)^n =e^{nz}, n\in \mathbb{Z}$.
    \end{proposition}
    \begin{lemma}\label{lma:exp_arth}
        For $z=x+iy$:
        \begin{enumerate}[(1)]
            \item $ e^z = e^x(\cos y+i\sin y) $.
            \item $ \exp(z)\in \mathbb{C} \setminus \left\{ 0\right\} $.
            \item $ e^z = 1 \Leftrightarrow z = 2\pi n i, n\in \mathbb{Z} $.
        \end{enumerate}
    \end{lemma}
    \begin{definition}[Roots of unity]
        $z$ is an $ N $th root of unity if $ z^N=1 $.
    \end{definition}
    We have 
    \[
        z^N=r^Ne^{iN\theta} =1 \Longleftrightarrow r=1, N\theta = 2n\pi \Longleftrightarrow \theta = \frac{2n\pi}{N}
    ,\]
    which gives $N$ distinct solutions
    \[
        z = \frac{2n\pi}{N} = \omega^n, \quad, n=0,1,\dots, N-1
    .\]
    $\omega^n$ lie one the vertices of a regular $n$-gon on the unit circle.
    \section{Logarithms and Complex powers}
    \begin{definition}
        Define $ w = \log z, z\in \mathbb{C} \land z\neq 0 $ by $ e^w = e^{\log z}=z $. Note that since exp is many-to-one, $\log$ is multi-valued.
        \[
            \begin{aligned}
                 & z = re^{i\theta} =e^{\log r}e^{i\theta}=e^{\log r+ i\theta}\\
                 \Longrightarrow & \boxed{\log z = \log r+i\theta = \log |z|+i\arg(z)}
            \end{aligned}
        \]
        To make it single-valued, simply take the principal value.
    \end{definition}
    \begin{definition}
        Define \textit{complex power} by 
        \[
            z^\alpha = e^{\alpha\log z}, \quad z,\alpha\in \mathbb{C} , z\neq 0
        .\]
        Note that since $ \arg z \to \arg z + 2n\pi \Rightarrow z^\alpha \to z^\alpha e^{2n\pi} $, it is generally multi-valued. This also reduces to common powers when $ z,\alpha\in \mathbb{R}. $
    \end{definition}
    \begin{example}
        \[
            i^i = e^{i\log i} = e^{i(0+i(\frac{\pi}{2}+2n\pi))} = e^{-(\frac{\pi}{2}+2n\pi)}
        .\]
    \end{example}
    \section{Transformations, Lines, and Circles}
    \begin{itemize}
        \item We have five elementary transformations:
        \begin{enumerate}[(1)]
            \item $ z \mapsto z+a $,
            \item $ z \mapsto \lambda z $,
            \item $ z \mapsto e^{i\alpha} z $,
            \item $ z \mapsto \bar{z} $,
            \item $ z \mapsto \frac{1}{z} $.
        \end{enumerate}
        \item General point of a line in $\mathbb{C}$ through $z_0$ and parallel to $w$:
        \[
            z = z +\lambda w, \lambda\in \mathbb{R} \text{ or } \bar{w}z-w\bar{z}=\bar{w}z_0-w\bar{z_0}
        .\]
        \item General point of a circle in $\mathbb{C}$ with centre $ c $ and radius $ \rho $:
        \[
            z = c+ \rho e^{i\theta} \text{ or } \left| z-c \right| = \rho \text{ or } |z|^2-\bar{c}z-c\bar{z}=\rho^2-|c|^2
        .\]
        \item Stereographic projection.
    \end{itemize}
    \part{Vectors in 3 Dimensions}
    \section{Vector addition and scalar multiplication}
    \begin{definition}[scalar multiplication]
        Given $ \mathbf{a} $, and scalar $ \lambda\in \mathbb{R} $, define $ \lambda \mathbf{a} $ to be the position vector of $ A' $ on the line $OA$ with length $ |\lambda \mathbf{a}|=\left| \lambda \right| \left| \mathbf{a} \right|  $. Direction depends on the sign of $ \lambda $.
    \end{definition}
    Define $ \spn\left\{ \mathbf{a}\right\} = \left\{ \lambda \mathbf{a}: \lambda\in \mathbb{R} \right\} $. If $ \mathbf{a}\neq 0 $, then $ \spn \{\mathbf{a}\} $ is the entire line through $O$ and $A$.

    Define $ \mathbf{a} \parallel \mathbf{b} $ if and only if either $ \mathbf{a} = \lambda \mathbf{b} $ or $ \mathbf{b} = \lambda \mathbf{a} $. Allow $ \lambda=0 $, so $\forall \mathbf{a}, \mathbf{0} \parallel \mathbf{a} $. Also allow $\lambda<0$.
    \begin{definition}[vector addition]
        Give $ \mathbf{a}, \mathbf{b} $, if $ \mathbf{a} \nparallel \mathbf{b} $, construct a parallelogram $OACB$ and define $ \mathbf{c} = \mathbf{a} + \mathbf{b} $.
    \end{definition}
    If $ a \parallel b $, then $ \mathbf{a} = \alpha \mathbf{u}, \mathbf{b}=\beta \mathbf{u} $, where $\mathbf{u}$ is a unit vector and $ \mathbf{a}+\mathbf{b}=(\alpha+\beta)\mathbf{u} $.

    Given $ \mathbf{a},\mathbf{b},\dots,\mathbf{c} $, we have a linear combination
    \[
        \alpha \mathbf{a}+ \beta \mathbf{b}+\cdots+\gamma \mathbf{c}
    \]
    for any $ \alpha,\beta,\dots, \gamma\in \mathbb{R}  $.

    Define $ \spn \left\{ \mathbf{a},\mathbf{b},\dots,\mathbf{c}\right\} = \left\{ \alpha \mathbf{a}+ \beta \mathbf{b}+\cdots+\gamma \mathbf{c}:\alpha,\beta,\dots, \gamma\in \mathbb{R} \right\} $. In 3d case, if $ \mathbf{a} \nparallel \mathbf{b} $, then $ \spn \{ \mathbf{a}, \mathbf{b} \} $ is a plane through $O,A,B$.
    
    Here are some properties:
    \begin{itemize}
        \item $ \forall \mathbf{a}, \mathbf{b}, \mathbf{c}, \mathbf{a}+\mathbf{0}=\mathbf{0}+\mathbf{a}=\mathbf{a} $, this says that $ \mathbf{0} $ is the identity for addition.
        \item $ \exists -\mathbf{a}, \mathbf{a}+(-\mathbf{a})=(-\mathbf{a})+\mathbf{a}=\mathbf{0} $. This says $ -\mathbf{a} $ is the inverse of $ \mathbf{a} $ under addition.
        \item $ \forall \mathbf{a},\mathbf{b}, \mathbf{a}+\mathbf{b}=\mathbf{b}+\mathbf{a} $, this says that vector addition is commutative.
        \item $ \forall \mathbf{a},\mathbf{b},\mathbf{c}, (\mathbf{a}+\mathbf{b})+\mathbf{c}=\mathbf{a}+(\mathbf{b}+\mathbf{c}) $, this says that vector addition is associative.
    \end{itemize}
    Hence, the set of vectors with addition form an abelian group.

    Relation with scalars:
    \begin{itemize}
        \item $ \lambda(\mathbf{a}+\mathbf{b})=\lambda \mathbf{a}+\lambda \mathbf{b} $.
        \item $ (\lambda+\mu)\mathbf{a}=\lambda \mathbf{a}+ \mu \mathbf{a} $.
        \item $ (\lambda \mu)\mathbf{a}=\lambda(\mu \mathbf{a}) $.
    \end{itemize}
    \section{Dot product}
    \begin{definition}[dot product]
        Give $ \mathbf{a}, \mathbf{b} $, let $ \theta $ be the angle between them, define $ \mathbf{a}\cdot\mathbf{b}=|\mathbf{a}||\mathbf{b}|\cos \theta $. Note that $ \theta $ is defined unless $ \mathbf{a}=\mathbf{0} $, in which case we define $ \mathbf{a}\cdot \mathbf{b}=0 $.

        $ \mathbf{a}\perp\mathbf{b} \Leftrightarrow \mathbf{a}\cdot \mathbf{b}=0 \Leftrightarrow \theta=\frac{\pi}{2}\bmod \pi $ when $ \theta $ is defined. Allow $ \mathbf{a} $ or $ \mathbf{b}=0 $, so $ \mathbf{a} \parallel \mathbf{0} \land \mathbf{a} \perp \mathbf{0} $.
    \end{definition}
    For $ \mathbf{a}\neq \mathbf{0} $, $ |\mathbf{b}|\cos \theta $ is the component of $ \mathbf{b} $ along $ \mathbf{a} $.
    \[
        \left| \mathbf{b} \right| \cos \theta = \frac{\mathbf{a}\cdot \mathbf{b}}{|\mathbf{a}|}=\mathbf{u}\cdot \mathbf{b}
    .\]
    By resolving $ \mathbf{b} $ along and perpendicular to $ \mathbf{a} $, we get 
    \[
        \mathbf{b} = \mathbf{b}_{\parallel} + \mathbf{b}_{\perp }
    .\]
    Properties:
    \begin{itemize}
        \item $ \mathbf{a}\cdot \mathbf{b}=\mathbf{b}\cdot \mathbf{a}, \mathbf{a}\cdot \mathbf{a}=|\mathbf{a}|^2\ge 0 $, $ =0 $ iff $ \mathbf{a}=\mathbf{0} $.
        \item $ (\lambda \mathbf{a})\cdot \mathbf{b}=\lambda (\mathbf{a}\cdot \mathbf{b})=\mathbf{a}\cdot (\lambda \mathbf{b}) $.
        \item $ \mathbf{a}\cdot (\mathbf{b}+\mathbf{c})=\mathbf{a}\cdot \mathbf{b}+\mathbf{a}\cdot \mathbf{c} $.
    \end{itemize}
    \section{Vector cross product}
    \begin{definition}
        Given $ \mathbf{a},\mathbf{b} $, let $ \theta $ be the angle between them, wrt a unit vector $ \mathbf{n} $ normal to the plane they span. Define $ \mathbf{a} \wedge \mathbf{b} $ or $ \mathbf{a} \times \mathbf{b} $ as $ |\mathbf{a}||\mathbf{b}|\sin \theta \mathbf{n} $. $ \mathbf{0} $ case is similar.
    \end{definition}
    This is the \textit{vector area} of the parallelogram generated by $ \mathbf{a}, \mathbf{b} $. Note that $ \mathbf{a} \wedge \mathbf{b} = \mathbf{a} \wedge \mathbf{b}_{\perp } $.

    Properties:
    \begin{itemize}
        \item $ \mathbf{a} \wedge \mathbf{b}=\mathbf{b}\wedge \mathbf{a} $.
        \item $ (\lambda \mathbf{a})\wedge \mathbf{b}=\lambda(\mathbf{a}\wedge \mathbf{b})=\mathbf{a} \wedge (\lambda\mathbf{b}) $.
        \item $ \mathbf{a}\wedge (\mathbf{b}+\mathbf{c})=\mathbf{a}\wedge \mathbf{b}+\mathbf{a}\wedge \mathbf{c} $.
        \item $ \mathbf{a} \wedge \mathbf{b}=\mathbf{0} $ if and only if $ \mathbf{a} \parallel \mathbf{b} $.
        \item $ \mathbf{a}\wedge \mathbf{b} \perp \mathbf{a} \land \perp \mathbf{b} $.
    \end{itemize}
    %Lecture 4
    \section{Orthonormal Bases and Components}\marginnote{Lecture 4.}
    Choose $ \mathbf{e}_1, \mathbf{e}_2, \mathbf{e}_3 $ that are \textit{orthonormal}. That is, they are of unit lengths and $ \mathbf{e}_i\cdot \mathbf{e}_j =0, i\neq j\in \{1,2,3\}$, which is equivalent to choose cartesian axes along the directions. Then $ \left\{ \mathbf{e}_i\right\} $ is a basis and $ \forall \mathbf{a}\in \mathbb{R}^3 $, 
    \[
        \mathbf{a}=\sum_{i=1}^{3}a_i \mathbf{e}_i \marginnote{Each component $ a_i $ is uniquely determined by $ a_i = \mathbf{e}_i \cdot \mathbf{a} $.}
    .\]
    By this spirite, we can write 
    \[
        \mathbf{a} = (a_1,a_2,a_3) = \begin{pmatrix}
            a_1\\ 
            a_2\\ 
            a_3\\
            \end{pmatrix}
    .\]
    Scalar product in this form can be written as 
    \[
        \begin{pmatrix}
            a_1\\ 
            a_2\\ 
            a_3\\
            \end{pmatrix} \cdot \begin{pmatrix}
                b_1\\ 
                b_2\\ 
                b_3\\
                \end{pmatrix}
                =a_1b_1+a_2b_2+a_3b_3,\quad \left| \mathbf{a} \right| = a_1^2+a_2^2+a_3^2.
    .\]
    For vector products, choose this basis that it is also \textit{right-handed}:
    \[
        \mathbf{e}_i \times \mathbf{e}_{i+1}=\mathbf{e}_{i+2}
    .\]
    Then
    \[
        \begin{aligned}
            \mathbf{a}\times \mathbf{b} &= (a_1 \mathbf{e_1}+a_2 \mathbf{e}_2+a_3 \mathbf{e}_3)(b_1 \mathbf{e_1}+b_2 \mathbf{e}_2+b_3 \mathbf{e}_3)\\
            &= (a_3b_2-a_2b_3)\mathbf{e}_1+(a_3b_1-a_1b_3)\mathbf{e}_2+(a_1b_2-a_2b_1)\mathbf{e}_3.
        \end{aligned}
    \]
    \section{Triple products}
    \subsection{Scalar triple product}
    \begin{definition}
        Define scalar triple product by
        \[
            [\mathbf{a},\mathbf{b},\mathbf{c}]=\mathbf{a}\cdot (\mathbf{b} \times \mathbf{c})=\mathbf{b}\cdot (\mathbf{c} \times \mathbf{a})=\mathbf{c}\cdot (\mathbf{a}\times \mathbf{b})
        .\]
        This is the volumn of the parallelepiped with bases $ \mathbf{b}, \mathbf{c} $ and side $ \mathbf{a} .$
    \end{definition}
    \begin{remark}
        $ \mathbf{c}\cdot (\mathbf{a}\times \mathbf{b}) $ is a "signed" volumn. If $ \mathbf{c}\cdot (\mathbf{a}\times \mathbf{b})>0 $ then $ \left\{ \mathbf{a},\mathbf{b},\mathbf{c}\right\} $ is called a \textit{right-handed set}. $ \mathbf{c}\cdot (\mathbf{a}\times \mathbf{b}) $ if and only if $ \mathbf{a},\mathbf{b},\mathbf{c} $ are coplanar, e.g., $ \mathbf{c}=\alpha \mathbf{a}+\beta \mathbf{b} \in \spn\left\{ \mathbf{a},\mathbf{b}\right\}$.
    \end{remark}
    In components, 
    \[
        \begin{aligned}
            \mathbf{a}\cdot (\mathbf{b}\times \mathbf{c})&=a_1b_2c_3-a_1b_3c_2 \\
            &+a_2b_3c_1-a_2b_1c_3\\
            &+a_3b_1c_2-a_3b_2c_1\\
        \end{aligned} =\begin{vmatrix}
            a_{1} & a_{2} & a_{3} \\
            b_{1} & b_{2} & b_{3} \\
            c_{1} & c_{2} & c_{3}
        \end{vmatrix}.
    \]
    \subsection{Vector triple product}
    \begin{definition}
        Define the vector triple product by $ \mathbf{a}\times (\mathbf{b}\times \mathbf{c}) $. Note that $ \mathbf{a}\times (\mathbf{b}\times \mathbf{c}) $ does not necessarily give the same result as $ (\mathbf{a}\times \mathbf{b})\times \mathbf{c} $.
        \begin{equation}\label{eq:vector triple prod identity}
            \mathbf{a}\times (\mathbf{b}\times \mathbf{c})=(\mathbf{a}\cdot \mathbf{c})\mathbf{b}-(\mathbf{a}\cdot \mathbf{b})\mathbf{c}.
        \end{equation}
    \end{definition}
    We have the following identities:
    \begin{proposition}\label{prop:triple}
        \[
            \begin{array}{l}
                \mathbf{a} \times(\mathbf{b} \times \mathbf{c})+\mathbf{b} \times(\mathbf{c} \times \mathbf{a})+\mathbf{c} \times(\mathbf{a} \times \mathbf{b})=\mathbf{0} \\
                (\mathbf{a} \times \mathbf{b}) \times(\mathbf{c} \times \mathbf{d})=[\mathbf{a}, \mathbf{b}, \mathbf{d}] \mathbf{c}-[\mathbf{a}, \mathbf{b}, \mathbf{c}] \mathbf{d} \\
                (\mathbf{a} \times \mathbf{b}) \cdot((\mathbf{c} \times \mathbf{d}) \times(\mathbf{e} \times \mathbf{f}))=[\mathbf{a}, \mathbf{b}, \mathbf{d}][\mathbf{c}, \mathbf{e}, \mathbf{f}]-[\mathbf{a}, \mathbf{b}, \mathbf{c}][\mathbf{d}, \mathbf{e}, \mathbf{f}] \\
                (\mathbf{b} \times \mathbf{c}) \cdot(\mathbf{a} \times \mathbf{d})+(\mathbf{c} \times \mathbf{a}) \cdot(\mathbf{b} \times \mathbf{d})+(\mathbf{a} \times \mathbf{b}) \cdot(\mathbf{c} \times \mathbf{d})=0
                \end{array}
        \]
    \end{proposition}
    \section{Lines, Planes, and Vector equations}
    Vectors are defined as position vectors from $O$. But the definition of addition enables us to use them to describe displacements between points.
    \subsection{Lines}
    General point on a line through $ \mathbf{a} $ through $ \mathbf{u} $:
    \[
        \begin{aligned}
            &\mathbf{r} = \mathbf{a}+\lambda\mathbf{u}, &\lambda\in \mathbb{R}\quad &\text{The parametric form.}\\
            &\mathbf{u} \times \mathbf{r} = \mathbf{u} \times \mathbf{a},&  &\text{Cross form.}
        \end{aligned}
    \]
    \begin{proposition}\label{prop:line_equation_vec}
        Any vector equation of the form $ \mathbf{u}\times \mathbf{r}=\mathbf{c} $ represents a line.
    \end{proposition}
    \begin{proof}
        $ \mathbf{u}\times \mathbf{r}=\mathbf{c} \Rightarrow \mathbf{u}\cdot (\mathbf{u}\times \mathbf{r})=\mathbf{u}\cdot \mathbf{c} \Leftrightarrow \mathbf{u} \cdot \mathbf{c}=0 $. If $ \mathbf{u} \cdot \mathbf{c}\neq 0 $ then the equation is inconsistent. If $ \mathbf{u}\cdot \mathbf{c} =0$, then note that 
        \[
            \mathbf{u} \times (\mathbf{u} \times \mathbf{c})=(\mathbf{u}\cdot \mathbf{c})\mathbf{u}-(\mathbf{u}\cdot \mathbf{u})\mathbf{c} = -\left| \mathbf{u} \right|^2 \mathbf{c}
        .\]
        Hence $ \mathbf{a} = -(\mathbf{u} \times \mathbf{c})/|\mathbf{u}|^2 $ is a solution, and thus it represents a line.
    \end{proof}
    \subsection{Planes}
    General point on a plane through $ \mathbf{a} $ with directions $ \mathbf{u},\mathbf{v} $ in the plane($ \mathbf{u}\nparallel \mathbf{v} $):
    \[
        \begin{aligned}
             &\mathbf{r} = \mathbf{a}+\lambda \mathbf{u}+\mu \mathbf{v}, &\lambda, \mu\in \mathbb{R} &\quad \text{Parametric form,}\\
             & \mathbf{n} \cdot \mathbf{r} = k = \mathbf{n} \cdot \mathbf{a}, & \mathbf{n} = \mathbf{u}\times \mathbf{v} &\quad \text{Dot form.}
        \end{aligned}
    \]
    The component of $\mathbf{r}$ along $ \mathbf{n} $ is 
    \[
        \frac{\mathbf{n}\cdot \mathbf{r}}{|\mathbf{n}|}=\frac{k}{|\mathbf{n}|}
    .\]
    \subsection{Other vector equations}
    \begin{enumerate}[(1)]
        \item $ |\mathbf{r}|^2+\mathbf{r}\cdot \mathbf{a}=k \Leftrightarrow \left| \mathbf{r}+\frac{1}{2}\mathbf{a} \right|^2=k+\frac{1}{4}|\mathbf{a}|^2  $, a sphere with centre $ -\frac{1}{2}\mathbf{a} $ and radius $ \sqrt{k+\frac{1}{4}|\mathbf{a}|^2} $, provided $ k>-\frac{1}{4}|\mathbf{a}|^2 $.
        \item $ \mathbf{r}+\mathbf{a} \times (\mathbf{b}\times \mathbf{r}) =\mathbf{c} \Leftrightarrow \mathbf{r}+(\mathbf{a}\cdot \mathbf{r})\mathbf{b}-(\mathbf{a}\cdot \mathbf{b})\mathbf{r}=\mathbf{c}$. Dot with $\mathbf{a}$:
        \[
            \mathbf{a}\cdot \mathbf{r}= \mathbf{a}\cdot \mathbf{c} \Longrightarrow (1-\mathbf{a}\cdot \mathbf{b})\mathbf{r}=\mathbf{c}-(\mathbf{a}\cdot \mathbf{c})\mathbf{b}
        .\]
        If $ \mathbf{a}\cdot \mathbf{b}\neq 1 $, then there is a unique solution
        \[
            \mathbf{r} = \frac{1}{1-\mathbf{a}\cdot \mathbf{b}}(\mathbf{c}-(\mathbf{a}\cdot \mathbf{c})\mathbf{b})
        ,\]
        which is a point.

        If $ \mathbf{a}\cdot \mathbf{b}=1 $ and RHS$\neq 0$, then it is inconsistent.

        If $ \mathbf{a}\cdot \mathbf{b} $ and $\mathbf{c}-(\mathbf{a}\cdot \mathbf{c})\mathbf{b}=\mathbf{0}$, then 
        \[
            (\mathbf{a}\cdot \mathbf{r}-\mathbf{a}\cdot \mathbf{c})\mathbf{b}=\mathbf{0}
        .\] 
        Hence it is a plane.
    \end{enumerate}
    \section{Index notation and the summation convention}
    \subsection{Components, $\delta\& \epsilon$}
    Write vectors $ \mathbf{a},\mathbf{b},\dots $ in terms of components $ a_i,b_i,\dots $ wrt an orthonormal right-handed basis $ \left\{ \mathbf{e}_i\right\} $. Indices $i,j,\dots$ take values $1,2,3$.

    For example, if $ \mathbf{c}=\alpha \mathbf{a}+\beta \mathbf{b} $, then $ c_i=[\alpha \mathbf{a}+\beta \mathbf{b}]_i=\alpha a_i+\beta b_i $, for $i=1,2,3$. $i$ is called a \textit{free index}.

    Hence
    \begin{itemize}
        \item $\displaystyle \mathbf{a}\cdot \mathbf{b} = \sum_{i=1}^{3}a_ib_i$.
        \item $\displaystyle \mathbf{x}=\mathbf{a}+(\mathbf{b}\cdot \mathbf{c})\mathbf{d} \Leftrightarrow x_j=a_j+\left( \sum_{k=1}^{3} b_kc_k \right)d_j $. 
    \end{itemize}
    \begin{definition}[Kronecker delta]
        \[
            \delta_{ij} = \begin{cases}
            1 &\text{ if }i=j\\
            0 &\text{ if }i\neq j\\
            \end{cases}
        \] 
    \end{definition}
    We see that $ \delta_{ij}=\delta_{ji} $ and also 
    \[
        \left(\begin{array}{lll}
            \delta_{11} & \delta_{12} & \delta_{13} \\
            \delta_{21} & \delta_{22} & \delta_{23} \\
            \delta_{31} & \delta_{32} & \delta_{33}
            \end{array}\right)=\left(\begin{array}{lll}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
            \end{array}\right)
    \]
    Using this, $ \mathbf{e}_i\cdot \mathbf{e}_j=\delta_{ij} $
    \begin{definition}[Levi-Civita epsilon]
        \[
            \epsilon_{ijk}=\begin{cases}
            1 &\text{ if $ (i,j,k) $ is an even permutation of $ (1,2,3) $,}\\
            -1 &\text{ if $ (i,j,k) $ is an odd permutation of $ (1,2,3) $,}\\
            0 &\text{ else.}\\
            \end{cases} 
        \]
    \end{definition}
    We have $ \epsilon_{123}=\epsilon_{231}=\epsilon_{312}=1, \epsilon_{321}=\epsilon_{213}=\epsilon_{132}=-1 $.
    $ \epsilon_{ijk} $ is totally anti-symmetric: exchanging any pair of indices produces a change in sign.

    Then 
    \[
        \mathbf{e}_i \times \mathbf{e}_j = \sum_{k}\epsilon_{ijk}\mathbf{e}_k
    \]
    and 
    \[
        \begin{aligned}
            \mathbf{a} \times \mathbf{b}&= \left( \sum_{i} a_i \mathbf{e}_i \right)\times \left( \sum_{j} b_j \mathbf{e}_j \right)\\
            &= \sum_{ij}a_ib_j \mathbf{e}_i \times \mathbf{e}_j\\
            &= \sum_{ij}a_ib_j\sum_{k}\epsilon_{ijk}\mathbf{e}_k=\sum_{ijk}a_ib_j\epsilon_{ijk}\mathbf{e}_k
        \end{aligned}
    \]
    so 
    \[
        \boxed{(\mathbf{a}\times \mathbf{b})_k=\sum_{ij}\epsilon_{ijk}a_ib_j}
    .\]
    \subsection{Summation convention}
    With components and index notation, indices that appear twice in a given term are usually summed over. In the summation convention, we omit the sum signs for repeated indices. i.e., the sum is understood.
    \begin{example}
        \begin{enumerate}[(i)]
            \item In $ a_i \delta_{ij} = a_1\delta_{1j}+a_2\delta_{2j}+a_3 \delta_{3j}=a_j $, since $ \Sigma_{i} $ is understood.
            \item $ \mathbf{a}\cdot \mathbf{b}=\delta_{ij}a_{i}b_{j}=a_ib_i $. $ \Sigma_{ij}, \Sigma_{i} $ are understood.
            \item $ (\mathbf{a}\times \mathbf{b})_i=\epsilon_{ijk}a_jb_k $, $ \Sigma_{jk} $ is understood.
            \item $ [\mathbf{a},\mathbf{b},\mathbf{c}]=\epsilon_{ijk}a_ib_jc_k $, $ \Sigma_{ijk} $ is understood.
            \item $ \delta_{ii}=\delta_{11}+\delta_{22}+\delta_{33}=3 $.
            \item 
            \[
                \begin{aligned}
                    [(\mathbf{a}\cdot \mathbf{c})\mathbf{b}-(\mathbf{a}\cdot \mathbf{b})\mathbf{c}]_i&=(\mathbf{a}\cdot \mathbf{c})b_i-(\mathbf{a}\cdot \mathbf{b})c_i\\
                    &= a_jc_jb_i-a_jb_jc_i.
                \end{aligned}
            \]
            $ \Sigma_j $ is understood.
        \end{enumerate}
    \end{example}
    Here are the rules of summation convention.
    \begin{enumerate}[(1)]
        \item An index occuring exactly once in any given term must appear once in every term in an equation, and it can take any value in $ 1,2,3 $, a \textit{free} index.
        \item An index occuring exactly twice in a given term is summed over. A \textit{repeated}, \textit{contracted}, or \textit{dummy} index.
        \item No index can occur more than twice in any given term.
    \end{enumerate}
    \subsection{Applications}
    We can use this to prove the vector triple product identity.

    \begin{proof}
        Write the huge sum in summation convention:
        \begin{IEEEeqnarray*}{rCl}
            [\mathbf{a}\times (\mathbf{b}\times \mathbf{c})]_i & = & \epsilon_{ijk}a_j(\mathbf{b}\times \mathbf{c})_k
        \\
            & = & \epsilon_{ijk}a_j\epsilon_{kpq}b_pc_q
        \\
            & = & (\epsilon_{ijk}\epsilon_{kpq})a_jb_pc_q.
        \end{IEEEeqnarray*}
        Notice that
        \begin{equation}
            \epsilon_{ijk}\epsilon_{kpq}=\delta_{ip}\delta_{jq}-\delta_{iq}\delta_{jp} \tag{*}
        \end{equation}
        see next subsection. So 
        \[
            [\mathbf{a}\times (\mathbf{b}\times \mathbf{c})]_i=\delta_{ip}\delta_{jq}a_jb_pc_q-\delta_{iq}\delta_{jp}a_jb_pc_q
        .\]
        Notice also that $ a_i \delta_{ij}=a_j $, so
        \[
            [\mathbf{a}\times (\mathbf{b}\times \mathbf{c})]_i = a_qb_ic_q-a_jb_jc_i = (\mathbf{a}\cdot \mathbf{c})b_i-(\mathbf{a}\cdot \mathbf{b})c_i
        .\]
        Hence the equation \ref{eq:vector triple prod identity} is proved.
    \end{proof}
    \subsection{$\epsilon\, \epsilon$ identity}
    \begin{proposition}\label{prop:eeidentity1}
        $ \epsilon_{ijk}\epsilon_{pqk}= \delta_{ip}\delta_{jq}-\delta_{iq}\delta_{jp}=\epsilon_{kij}\epsilon_{kpq}$.
    \end{proposition}
    \begin{proof}
        Notice that LHS and RHS are both anti-symmetric, so both vanish when $ i,j $ or $ p,q $ take the same value. Inspection shows that\mn{Think carefully here.} it suffices to show the cases $ i=p=1 \land  j=q=2 $ or $ i=q=1, j=p=2 $ and all other index changings that give non-zero results.
    \end{proof}

    \begin{proposition}\label{prop:eeidentity2}
        $ \epsilon_{ijk}\epsilon_{pjk}=2\delta_{ip}. $
    \end{proposition}
    \begin{proof}
        Take $q=j$ in the above equation:
        \[
            \epsilon_{ijk}\epsilon_{pjk}=\delta_{ip}\delta_{jj}-\delta_{ij}\delta_{jp}=3\delta_{ip}-\delta_{ip}=2\delta_{ip}
        .\]
    \end{proof}
    \begin{proposition}\label{prop:eeidentity3}
        $  \epsilon_{ijk}\epsilon_{ijk}=k $.
    \end{proposition}
    \begin{proposition}\label{prop:eeidentity4}
        \[
            \begin{aligned}
                \epsilon_{ijk}\epsilon_{pqr}&=\delta_{ip}\delta_{jq}\delta_{kr}-\delta_{jp}\delta_{iq}\delta_{kr}\\
                &+\delta_{jp}\delta_{kq}\delta_{ir}-\delta_{kp}\delta_{jq}\delta_{ir}\\
                &+\delta_{kp}\delta_{iq}\delta_{jr}-\delta_{ip}\delta_{kq}\delta_{jr}.
            \end{aligned}
        \]
    \end{proposition}
    \begin{proof}
        Total anti-symmetry\mn{This simplifies most of the process and leaves only one case to check.} in $i,j,k$ and independently in $p,q,r$ implies LHR, RHS agree up to an overall factor. To check the factor is 1, consider $i=p=1, j=q=2, k=r=3$.
    \end{proof}
    \part{Vectors in General}
    \section{Vectors in $ \mathbb{R}^n $}
    \subsection{Definition and basic properties}\marginnote{Lecture 6}
    \begin{definition}
        Regard vectors as sets of components, and let 
        \[
            \mathbb{R}^n = \left\{ \mathbf{x}=(x_1,\dots, x_n): x_i\in \mathbb{R} \right\}
        .\]
        Define:
        \begin{itemize}
            \item Addition: $ \mathbf{x}+\mathbf{y}=(x_1+y_1,\dots,x_n+y_n) $,
            \item Scalar multiplication: $ \lambda \mathbf{x}=(\lambda x_1,\dots,\lambda x_n) $.
            \item Linear combinations: $ \lambda \mathbf{x}+\mu \mathbf{y} $,
            \item Parallel: $ \mathbf{x} \parallel \mathbf{y} \Leftrightarrow \mathbf{x}=\lambda \mathbf{y} \lor \mathbf{y}=\lambda \mathbf{x} $.
            \item Inner Product(Scalar product): $ \mathbf{x}\cdot \mathbf{y}= \sum_{i=1}^{n} x_iy_i $.
        \end{itemize}
    \end{definition}
    Properites of inner product:
    \begin{enumerate}[(1).]
        \item Symmetric: $ \mathbf{x}\cdot \mathbf{y}=\mathbf{y}\cdot \mathbf{x} $.
        \item Bilinear: 
        \[
            \begin{aligned}
                (\lambda \mathbf{x}+\lambda' \mathbf{x}')\cdot \mathbf{y}&=\lambda \mathbf{x}\cdot \mathbf{y}+\lambda' \mathbf{x}'\cdot \mathbf{y},\\
                \mathbf{x}\cdot (\mu\mathbf{y}+\mu' \mathbf{y}')&=\mu \mathbf{x}\cdot \mathbf{y}+\mu' \mathbf{x}' \cdot \mathbf{y}.
            \end{aligned}
        \]
        \item Positive definite: $ \mathbf{x}\cdot \mathbf{x}\ge 0 $, with $=$ holds if and only if $ \mathbf{x}=\mathbf{0} $.
    \end{enumerate}
    \subsection{Norm of a vector}
    \begin{definition}
        The \textit{norm} of a vector $ \mathbf{x} $ is denoted as $ |\mathbf{x}| $ with $ |\mathbf{x}|^2=\mathbf{x}\cdot \mathbf{x} $.

        $ \mathbf{x},\mathbf{y} $ are called \textit{orthogonal} if $ \mathbf{x}\cdot \mathbf{y}=0 $, denote as $ \mathbf{x} \perp \mathbf{y} $.
    \end{definition}
    The \textit{standard basis} of $ \mathbb{R}^n $ is
    \[
        e_i = (0,\dots,1,\dots,0)
    \]
    with 1 on the $i$th position. So that 
    \[
        \mathbf{x}=\sum_{i=1}^{n}x_i \mathbf{e}_i
    \]
    and $ \mathbf{e}_i \cdot \mathbf{e}_j = \delta_{ij} $. i.e., standard basis is orthogonal.
    \subsection{Cauchy-Schwarz and Triangle inequalities}
    \begin{proposition}[Cauchy-Schwarz]\label{prop:cauchy-schwarz}
        \[
            \forall \mathbf{x}, \mathbf{y}\in \mathbb{R}^n,\quad |\mathbf{x}\cdot \mathbf{y}|\le |\mathbf{x}| | \mathbf{y}|
        \]
        with equality if and only if $ \mathbf{x}\parallel \mathbf{y} $.
    \end{proposition}
    General deductions:
    \begin{enumerate}[(i).]
        \item Setting $ \mathbf{x}\cdot \mathbf{y}=|\mathbf{x}||\mathbf{y}|\cos \theta $, we can define angle $ \theta $ between $ \mathbf{x},\mathbf{y}\in \mathbb{R}^n $.
        \item We have the \textit{triangle inequality}:
        \[
            |\mathbf{x}+\mathbf{y}|\le|\mathbf{x}|+|\mathbf{y}|
        .\]
    \end{enumerate}
    \begin{proof}
        If $ \mathbf{y}=\mathbf{0} $, then the result is immediate. If not, consider
        \[
            \begin{aligned}
                 \left| \mathbf{x}-\lambda \mathbf{y} \right|&= (\mathbf{x}-\lambda \mathbf{y})(\mathbf{x}-\lambda \mathbf{y})\\
                 &= \left| \mathbf{x} \right|^2-2\lambda \mathbf{x}\cdot \mathbf{y}+\lambda^2 |\mathbf{y}|^2\ge 0.
            \end{aligned}
        \]
        This is a real equation of $\lambda$ with at most one root, so 
        \[
            (-2 \mathbf{x}\cdot \mathbf{y})^2-4|\mathbf{x}|^2|\mathbf{y}|^2\le 0 \Longleftrightarrow |\mathbf{x}\cdot \mathbf{y}|\le |\mathbf{x}| | \mathbf{y}|
        .\]
        Equality holds if and only if $ \mathbf{x}=\lambda \mathbf{y} $.

        Note also that for triangle inequality:
        \[
            \begin{aligned}
                 |\mathbf{x}+\mathbf{y}|^2&= |\mathbf{x}|^2+2 \mathbf{x}\cdot \mathbf{y}+|\mathbf{y}|^2\\
                 &\le |\mathbf{x}|^2+2 |\mathbf{x}| | \mathbf{y}|+|\mathbf{y}|^2\\
                 &=(|\mathbf{x}|+|\mathbf{y}|)^2,
            \end{aligned}
        \]
        as required.
    \end{proof}
    \subsection{Inner Products and Cross products}
    Inner product in $ \mathbb{R}^n $ can be written as
    \[
        \mathbf{a}\cdot \mathbf{b}=\delta_{ij}a_{i}b_{j},\quad \text{by summation convention}
    .\]
    For $n=3,$ it matches geometrical definition.

    We can also define cross product in component definition. In 3d we have
    \[
        (\mathbf{a}\times \mathbf{b})_i=\epsilon_{ijk}a_jb_k
    ,\]
    and in $n$ dimensions we have $\epsilon_{ij\cdots l}$ which is totally anti-symmetric. But there are only two $a_ib_j$ so we cannot use this to define vector product in general.

    However, in $ \mathbb{R}^2 $ we have $ \epsilon_{ij} $ with $ \epsilon_{12}=-\epsilon_{21}=1 $, so can use this to define a new scalar product 
    \[
        [\mathbf{a},\mathbf{b}]=\epsilon_{ij}a_ib_j=a_1b_2-a_2b_1
    .\]
    Geometrically, this the (signed) area of parallelogram formed by $ \mathbf{a},\mathbf{b} $ and 
    \[
        |[\mathbf{a},\mathbf{b}]|=|\mathbf{a}||\mathbf{b}|\sin \theta
    .\]
    Compare with $ [\mathbf{a},\mathbf{b},\mathbf{c}]=\epsilon_{ijk}a_ib_jc_k $.
    \section{Vector Spaces}
    \subsection{Axioms, span, and subspaces}
    \begin{definition}
        Let $V$ be a set of objects called \textit{vectors} with operation
        \[
            \begin{aligned}
                 \mathbf{v}+\mathbf{w}\in V&\quad \forall \mathbf{v},\mathbf{w}\in V\\
                 \lambda \mathbf{v}\in V&\quad \forall \mathbf{v}\in V, \lambda\in \mathbb{R}.
            \end{aligned}
        \]
        Then $V$ is called a \textit{real vector space} if 
        \begin{enumerate}[(i).]
            \item $V$ with $+$ is an abelian group.
            \item $ \lambda(\mathbf{v}+\mathbf{w})=\lambda \mathbf{v}+\lambda \mathbf{w} $
            \item $ (\lambda+\mu)\mathbf{v}=\lambda \mathbf{v}+\mu \mathbf{v} $
            \item $ \lambda(\mu \mathbf{v})=(\lambda \mu) \mathbf{v} $
            \item $ 1 \mathbf{v}=\mathbf{v} $.
        \end{enumerate}
    \end{definition}
    \begin{example}
        Let $ V=\left\{ f:[0,1] \to \mathbb{R} : f \land f(0)=f(1)=0\right\} $. By smooth we mean $f$ is differentiable infinitely many times. Then $V$ is a real vector space with $+$ defined as $ (f+g)(x)=f(x)+g(x) $ and $ (\lambda f)(x)=\lambda(f(x)) $. Then all axioms apply.
    \end{example}
    \begin{definition}\marginnote{Lecture 7}
        A \textit{subspace} of a real vector space $V$ is a subset $U \subseteq V$ that is also a vector space. 
    \end{definition}
    \begin{remark}
        A non-empty subset is a subspace if and only if $ \forall v,w\in U, \lambda v+ \mu u\in U $.
    \end{remark}
    For any vectors $ v_1,\dots,v_r\in V $, their \textit{span} $ \spn\left\{ v_1,\dots,v_r\right\}=\left\{ \lambda_1 v_1+\cdots+\lambda_r v_r:v_i\in \mathbb{R} \right\} $ is a subspace. $V$ and $ \left\{ 0\right\} $ are subspaces of $V$.
    \begin{example}
        A line or plane through $O$ is a subspace in $ \mathbb{R}^3 $, but a line or plane that does not contain $ \mathbf{0} $ is not a subspace.
    \end{example}
    \subsection{Linear dependence and independence}
    For $ v_1,\dots,v_r\in V $, a real vector space, consider a linear relation
    \[
        \lambda_1 v_1+\cdots+\lambda_r v_r=0\tag{*}
    .\]
    If $ (*)\Rightarrow \lambda_i=0 $, then the vectors form a \textit{linearly independent set}. They obey only the trivial linear relation.

    If $ (*) $ holds with at least $ \lambda_k\neq 0 $, then the vectors form a \textit{linearly dependent} set. They obey a non-trivial linear relation.
    \begin{example}
        In $ \mathbb{R}^2 $, $ \left\{ (1,0),(0,1),(0,2)\right\} $ is linearly dependent.

        \bluecomment{We cannot express $(1,0)$ in terms of the others.}
    \end{example}
    Several facts:
    \begin{itemize}
        \item Any set containing $0$ is linearly dependent.
        \item In $ \mathbb{R}^3 $, $ \left\{ \mathbf{a}\right\} $ is linearly independent if and only if $ \mathbf{a}\neq \mathbf{0} $.
        \item $ \left\{ \mathbf{a},\mathbf{b}\mathbf{c}\right\} $ is linearly independent if $ [\mathbf{a},\mathbf{b},\mathbf{c}]\neq 0 $. Since if 
        \[
            \alpha \mathbf{a}+\beta \mathbf{b}+\gamma \mathbf{c}=0
        ,\]
        then dotting with $ \mathbf{b}\times \mathbf{c} $ we get $ \alpha[\mathbf{a},\mathbf{b},\mathbf{c}]=0 \Rightarrow \alpha=0 $. Similarly $ \beta=0, \gamma=0 $.
    \end{itemize}
    \subsection{Inner products}
    This is an additional structure on a real vector space $V$, that can also be characterised by axioms or key properties.

    For $v,w\in V$, denote inner product by 
    \[
        v\cdot w \text{  or  } (v,w)\in \mathbb{R} 
    .\]
    Require this satisfies 1. it's symmetric, 2. it's bilinear, 3. it is positive definite.

    Definition of length or norm and deductions such as Cauchy-Schwarz inequality depend just on these properties.

    \begin{example}
        Consider space of functions 
        $$ V=\left\{ f:[0,1]\to \mathbb{R} : f \text{ smooth}\land f(0)=f(1)=0\right\} .$$
        Define an inner product by 
        \[
            (f,g)=\int_{0}^{1} f(x)g(x) \,\mathrm{d}x
        .\]
        which has properties 123. Cauchy-Schwarz holds:
        \[
            |(f,g)|\le \left\| f \right\| \left\| g \right\| 
        \]
        with $ \left\| f \right\|^2 =(f,f) $. i.e.
        \[
            \left| \int_{0}^{1} f(x)g(x) \,\mathrm{d}x \right| \le \left( \int_{0}^{1} f(x) \,\mathrm{d}x \right)^{1/2}\left( \int_{0}^{1} g(x) \,\mathrm{d}x \right)^{1/2}
        .\]
    \end{example}
    \begin{lemma}\label{lma:innerprod}
        In any real vector space $V$ with an inner product, if $v_1,v_2,\dots,v_r$ are non-zero and orthogonal vectors, then they are linearly independent.
    \end{lemma}
    \begin{proof}
        If 
        \[
            \sum_{i}\alpha_i v_i=0
        ,\]
        then
        \[
            (v_j,\sum_{i}\alpha_i v_i)=0 \Longleftrightarrow \alpha_j=0
        .\]
    \end{proof}
    \section{Bases and dimension}
    \begin{definition}
        For a vector space $V$, a \textit{basis} is a set 
        \[
            \mathfrak{B}= \left\{ e_1,\dots, e_n\right\}
        \]
        such that 
        \begin{enumerate}[(i)]
            \item $ \mathfrak{B} $ spans $V$. i.e., $ \forall v\in V $, 
            \[
                v = \sum_{i=1}^{n}v_i e_i
            .\]
            \item $ \mathfrak{B} $ is linearly independent.
        \end{enumerate}
        Given (ii), the coefficients $v_i$ in (i) are unique, since 
        \[
            \sum_{i}v_ie_i=\sum_{i}v_i'e_i \Longleftrightarrow v_i-v_i'=0 \Longleftrightarrow v_i=v_i'
        .\]
    \end{definition}
    \begin{example}
        Standard basis for $ \mathbb{R}^n $ consists of 
        \[
            e_1=\begin{pmatrix}
                1\\
                0\\
                \vdots\\
                0
            \end{pmatrix},
            e_2 = \begin{pmatrix}
                0\\
                1\\
                \vdots\\
                0
            \end{pmatrix},
            \dots,
            e_n=\begin{pmatrix}
                0\\
                0\\
                \vdots\\
                1
            \end{pmatrix}
        .\]
        Many other bases can be chosen.
    \end{example}
    \begin{theorem}\label{thm:dimension}
        If $ \left\{ e_1,\dots,e_n\right\} $, $ \left\{ f_1,\dots,f_m\right\} $ are bases for a real vector space $V$, then $m=n$.
    \end{theorem}
    \begin{proof}
        We have 
        \[
            \begin{aligned}
                 f_a&= \sum_{i}A_{ai}e_i,\\ 
                 e_i&= \sum_{a}B_{ia}f_a
            \end{aligned}
        \]
        for $ A_{ai},B_{ia}\in \mathbb{R} $.
        Hence
        \[
            \begin{aligned}
                f_a&=\sum_{i}A_{ai}\sum_{b}B_{ib}f_{b}\\
                &= \sum_{b}\sum_{i}A_{ai}B_{ib}f_b
            \end{aligned}
        \]
        But the coefficients are unique, so 
        \[
            \sum_{i}A_{ai}B_{ib}=\delta_{ab}
        .\]
        Similarly,
        \[
            \sum_{a}B_{ia}A_{aj}=\delta_{ij}
        .\]
        Now, 
        \[
            \sum_{i,a}A_{ai}B_{ia}=\sum_{a} \delta_{aa}=m = \sum_{i}\delta_{ii}=n,\\
        .\]
    \end{proof}
    \begin{definition}
        The number of vectors in any basis is the \textit{dimension} of the vector space.
    \end{definition}

\end{document}